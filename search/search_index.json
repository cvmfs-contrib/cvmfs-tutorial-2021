{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the CernVM-FS tutorial! \u00b6 Scope \u00b6 This is an introductory tutorial to the CernVM File System (CernVM-FS). In this tutorial you will learn what CernVM-FS is and how to use it, both from an administrator and end user point of view, through guided examples and hands-on exercises. We focus on the core concepts and basic usage of CernVM-FS, and provide pointers to more information regarding more advanced aspects. Intended audience \u00b6 This tutorial is intended for people who are new to CernVM-FS: no specific prior knowledge or experience with it is required. We expect it to be most valuable to people who are interested in either setting up and managing a CernVM-FS repository themselves, or using one or more existing CernVM-FS repositories. Prerequisites \u00b6 Being familiar with a Linux shell environment and having a basic notion of Linux filesystems and system administration is recommended but not strictly required; we hope that the guided examples are sufficient to follow along comfortably. You should have access to a couple of Linux instances on which you have administrative rights ( sudo access). The required resources are minimal: 1 or 2 cores, a couple of GBs of RAM, and about 10GB of disk space per instance is more than sufficient. We recommend using Linux virtual machines that were created specifically for this tutorial, on which only a base Linux distribution was installed as operating system, and which you are comfortable to discard afterwards. This tutorial was prepared on CentOS 7 ( x86_64 ), but it should be relatively straightforward to translate the instructions to other Linux distributions like Ubuntu, or another CPU architecture like Arm 64-bit ( aarch64 ). Practical information \u00b6 This tutorial is being organised during the 4th EasyBuild User Meeting (Jan 25-29 2021) . More information about the CernVM-FS tutorial sessions is available at https://easybuild.io/eum/#cvmfs-tutorial . Note that registration for this tutorial is required! . Dedicated cloud resources in Microsoft Azure (sponsored by Microsoft) will be available only to registered tutorial attendees for working on the hands-on exercises during the the week of Jan 25-29 2021. Note that nothing in this tutorial, other than the section of using the provided Azure cloud resources , is specific to Azure; you can use your own resources if you prefer doing so. Tutorial contents \u00b6 Azure cloud resources Introduction to CernVM-FS Stratum 0 + client (*) Stratum 1 + proxies (*) Publishing (*) Advanced topics (sections indicated with (*) involve hands-on exercises) Contributors \u00b6 Jakob Blomer (CERN, Switzerland) Bob Dr\u00f6ge (University of Groningen, The Netherlands) Kenneth Hoste (HPC-UGent, Belgium) Additional resources \u00b6 CernVM-FS website: https://cernvm.cern.ch/fs CernVM-FS documentation: https://cvmfs.readthedocs.io CernVM-FS @ GitHub: https://github.com/cvmfs CernVM-FS tutorial @ GitHub: https://github.com/cvmfs-contrib/cvmfs-tutorial-2021","title":"Home"},{"location":"#welcome-to-the-cernvm-fs-tutorial","text":"","title":"Welcome to the CernVM-FS tutorial!"},{"location":"#scope","text":"This is an introductory tutorial to the CernVM File System (CernVM-FS). In this tutorial you will learn what CernVM-FS is and how to use it, both from an administrator and end user point of view, through guided examples and hands-on exercises. We focus on the core concepts and basic usage of CernVM-FS, and provide pointers to more information regarding more advanced aspects.","title":"Scope"},{"location":"#intended-audience","text":"This tutorial is intended for people who are new to CernVM-FS: no specific prior knowledge or experience with it is required. We expect it to be most valuable to people who are interested in either setting up and managing a CernVM-FS repository themselves, or using one or more existing CernVM-FS repositories.","title":"Intended audience"},{"location":"#prerequisites","text":"Being familiar with a Linux shell environment and having a basic notion of Linux filesystems and system administration is recommended but not strictly required; we hope that the guided examples are sufficient to follow along comfortably. You should have access to a couple of Linux instances on which you have administrative rights ( sudo access). The required resources are minimal: 1 or 2 cores, a couple of GBs of RAM, and about 10GB of disk space per instance is more than sufficient. We recommend using Linux virtual machines that were created specifically for this tutorial, on which only a base Linux distribution was installed as operating system, and which you are comfortable to discard afterwards. This tutorial was prepared on CentOS 7 ( x86_64 ), but it should be relatively straightforward to translate the instructions to other Linux distributions like Ubuntu, or another CPU architecture like Arm 64-bit ( aarch64 ).","title":"Prerequisites"},{"location":"#practical-information","text":"This tutorial is being organised during the 4th EasyBuild User Meeting (Jan 25-29 2021) . More information about the CernVM-FS tutorial sessions is available at https://easybuild.io/eum/#cvmfs-tutorial . Note that registration for this tutorial is required! . Dedicated cloud resources in Microsoft Azure (sponsored by Microsoft) will be available only to registered tutorial attendees for working on the hands-on exercises during the the week of Jan 25-29 2021. Note that nothing in this tutorial, other than the section of using the provided Azure cloud resources , is specific to Azure; you can use your own resources if you prefer doing so.","title":"Practical information"},{"location":"#tutorial-contents","text":"Azure cloud resources Introduction to CernVM-FS Stratum 0 + client (*) Stratum 1 + proxies (*) Publishing (*) Advanced topics (sections indicated with (*) involve hands-on exercises)","title":"Tutorial contents"},{"location":"#contributors","text":"Jakob Blomer (CERN, Switzerland) Bob Dr\u00f6ge (University of Groningen, The Netherlands) Kenneth Hoste (HPC-UGent, Belgium)","title":"Contributors"},{"location":"#additional-resources","text":"CernVM-FS website: https://cernvm.cern.ch/fs CernVM-FS documentation: https://cvmfs.readthedocs.io CernVM-FS @ GitHub: https://github.com/cvmfs CernVM-FS tutorial @ GitHub: https://github.com/cvmfs-contrib/cvmfs-tutorial-2021","title":"Additional resources"},{"location":"00_azure_cloud_resources/","text":"Practical info \u00b6 Azure/AWS VMs Q&A: 30min in afternoon (4pm-4.30pm) or book a time slot (via Google Calendar) test","title":"Azure cloud resources"},{"location":"00_azure_cloud_resources/#practical-info","text":"Azure/AWS VMs Q&A: 30min in afternoon (4pm-4.30pm) or book a time slot (via Google Calendar) test","title":"Practical info"},{"location":"01_introduction/","text":"Introduction to CernVM-FS \u00b6 What is CernVM-FS? \u00b6 Let's get started with explaining in detail what CernVM-FS is... CernVM-FS in a nutshell The CernVM File System (CernVM-FS) provides a scalable and reliable software distribution service, which is implemented as a read-only POSIX filesystem in user space (a FUSE module). Files and directories are hosted on standard web servers and mounted in the universal namespace /cvmfs . That's a mouthful, so let's break it down a bit... Read-only filesystem over HTTP \u00b6 CernVM-FS is a network filesystem , which you can mount in Linux or macOS via FUSE (Filesystem in Userspace) . In some ways it is similar to other network filesystems like NFS or AFS , but there are various aspects to it that are quite different. The files and directories that are made available via CernVM-FS are always located in a subdirectory of /cvmfs , and are provisioned via a network of servers that can basically be viewed as web servers since only outgoing HTTP connections are used. This makes it easy to use CernVM-FS in environments that are protected by a strict firewall. CernVM-FS is a read-only filesystem for those who access it; only those who administer it are able to add or change its contents. Internally, CernVM-FS uses content-addressable storage and Merkle trees in order to maintain file data and meta-data, but the filesystem it exposes is a standard POSIX filesystem . Software distribution system \u00b6 The primary use case of CernVM-FS is to easily distribute software around the world, which is reflected in various ways in the features implemented by CernVM-FS. It's worth highlighting that with software we actually mean software installations , that is the files that collectively form a usable instance of an application, tool, or library. This is in contrast with software packages (for example, RPMs), which are essentially bundles of files wrapped together for easy distribution, and which need to be installed in order to provide a working instance of the provided software. Software installations have specific characteristics, such as often involving lots of small files which are being opened and read as a whole regularly, frequent searching for files in multiple directories, hierarchical structuring, etc. CernVM-FS is heavily tuned to cater to this use case, with aggressive caching and reduction of latency, for example via automatic file de-duplication and compression. Scalable and reliable \u00b6 CernVM-FS was designed to be scalable and reliable , with known deployments involving hundreds of millions of files and many thousands of clients. It was originally created to fulfill the software distribution needs of the Large Hadron Collider (LHC) project at CERN . The network of (web) servers that make a CernVM-FS instance accessible is constructed such that it is robust against problems like network disconnects and hardware failures, and so it can be extended and tweaked on demand for optimal performance. More details are available in the CernVM-FS documentation . Terminology \u00b6 Before we get our hands dirty, let's cover some of the terminology used by the CernVM-FS project. The figure included below shows the different components of the CernVM-FS network: the central Stratum 0 server which hosts the filesystem; the Stratum 1 replica servers, and the associated proxies ; the client accessing the filesystem provided via CernVM-FS. Clients \u00b6 A client in the context of CernVM-FS is any system that mounts the filesystem. This includes laptops or personal workstations who need access to the provided software installations, but also High-Performance Computing (HPC) clusters, virtual machines running in a cloud environment, etc. Clients only have read-only access to the files included in a CernVM-FS repository, and are automatically notified when the contents of the filesystem has changed. The filesystem that is mounted on a client (under /cvmfs ) is a virtual filesystem, in the sense that data is only (down)loaded when it is actually accessed (and cached aggressively to ensure good performance). Mounting a CernVM-FS repository on a client will be covered in the first hands-on part of this tutorial . Extensive documentation on configuring a client is available in the CernVM-FS documentation . Stratum 0 + repository \u00b6 A CernVM-FS repository is the single source of (new) data for a filesystem. This single source is also called the Stratum 0 , which can be viewed as the central server the CernVM-FS network. Multiple repositories can be hosted on a single Stratum 0 server. A repository is a form of content-addressable storage, which is maintained by a dedicated release manager machine or publisher . All data stored into CernVM-FS has to be converted into a repository during the process of publishing , which involves creating the file catalog(s), compressing files, calculating content hashes, etc. A read-writable copy of a CernVM-FS repository is (only) available on a publisher system, which can be the same system as the Stratum 0 server. Providing write access is done by means of a union filesystem , which involves overlaying a read-only mount of the CernVM-FS filesystem with a writable scratch area. Publishing is an atomic operation: adding or changing files in a repository is done by ingesting files and creating a transaction that records the changes. In the first hands-on part of this tutorial we will guide you through the process of creating a CernVM-FS repository, which is also covered in detail in the CernVM-FS documentation . The 3rd hands-on part of this tutorial will focus on the publishing procedure to update the contents of a CernVM-FS repository. Stratum 1 replica servers \u00b6 A Stratum 1 replica server is a standard web server that provides a read-only mirror of a CernVM-FS repository served by a Stratum 0. The main purpose of a Stratum 1 is to improve reliability of the CernVM-FS network, by reducing the load on the Stratum 0. There usually are multiple Stratum 1 servers in a CernVM-FS network, which are typically distributed globally. Although clients can access a CernVM-FS repository directly via the Stratum 0, this is better done via the Stratum 1 replica servers. Stratum 1 servers enable clients to determine which Stratum 1 is geographically closest to connect to, via the Geo API which uses a GeoIP database that translates IP addresses of clients to longitude and latitude. Setting up a Stratum 1 replica server will be covered in the second hands-on part of this tutorial , and is also covered in detail in the CernVM-FS documentation . Squid proxies \u00b6 To reduce load on Stratum 1 servers and to help reduce latency on clients, it is recommended to set up a Squid forward proxy servers . A Squid proxy caches content that has been accessed recently, and helps to reduce bandwidth and improve response times. This is particularly important on large systems like HPC clusters where many workernodes are accessing the CernVM-FS repository, where it's recommended to set up multiple Squid proxies. The second hands-on part of this tutorial will also cover setting up a Squid proxy. More details are available in the CernVM-FS documentation .","title":"Introduction to CernVM-FS"},{"location":"01_introduction/#introduction-to-cernvm-fs","text":"","title":"Introduction to CernVM-FS"},{"location":"01_introduction/#what-is-cernvm-fs","text":"Let's get started with explaining in detail what CernVM-FS is... CernVM-FS in a nutshell The CernVM File System (CernVM-FS) provides a scalable and reliable software distribution service, which is implemented as a read-only POSIX filesystem in user space (a FUSE module). Files and directories are hosted on standard web servers and mounted in the universal namespace /cvmfs . That's a mouthful, so let's break it down a bit...","title":"What is CernVM-FS?"},{"location":"01_introduction/#read-only-filesystem-over-http","text":"CernVM-FS is a network filesystem , which you can mount in Linux or macOS via FUSE (Filesystem in Userspace) . In some ways it is similar to other network filesystems like NFS or AFS , but there are various aspects to it that are quite different. The files and directories that are made available via CernVM-FS are always located in a subdirectory of /cvmfs , and are provisioned via a network of servers that can basically be viewed as web servers since only outgoing HTTP connections are used. This makes it easy to use CernVM-FS in environments that are protected by a strict firewall. CernVM-FS is a read-only filesystem for those who access it; only those who administer it are able to add or change its contents. Internally, CernVM-FS uses content-addressable storage and Merkle trees in order to maintain file data and meta-data, but the filesystem it exposes is a standard POSIX filesystem .","title":"Read-only filesystem over HTTP"},{"location":"01_introduction/#software-distribution-system","text":"The primary use case of CernVM-FS is to easily distribute software around the world, which is reflected in various ways in the features implemented by CernVM-FS. It's worth highlighting that with software we actually mean software installations , that is the files that collectively form a usable instance of an application, tool, or library. This is in contrast with software packages (for example, RPMs), which are essentially bundles of files wrapped together for easy distribution, and which need to be installed in order to provide a working instance of the provided software. Software installations have specific characteristics, such as often involving lots of small files which are being opened and read as a whole regularly, frequent searching for files in multiple directories, hierarchical structuring, etc. CernVM-FS is heavily tuned to cater to this use case, with aggressive caching and reduction of latency, for example via automatic file de-duplication and compression.","title":"Software distribution system"},{"location":"01_introduction/#scalable-and-reliable","text":"CernVM-FS was designed to be scalable and reliable , with known deployments involving hundreds of millions of files and many thousands of clients. It was originally created to fulfill the software distribution needs of the Large Hadron Collider (LHC) project at CERN . The network of (web) servers that make a CernVM-FS instance accessible is constructed such that it is robust against problems like network disconnects and hardware failures, and so it can be extended and tweaked on demand for optimal performance. More details are available in the CernVM-FS documentation .","title":"Scalable and reliable"},{"location":"01_introduction/#terminology","text":"Before we get our hands dirty, let's cover some of the terminology used by the CernVM-FS project. The figure included below shows the different components of the CernVM-FS network: the central Stratum 0 server which hosts the filesystem; the Stratum 1 replica servers, and the associated proxies ; the client accessing the filesystem provided via CernVM-FS.","title":"Terminology"},{"location":"01_introduction/#clients","text":"A client in the context of CernVM-FS is any system that mounts the filesystem. This includes laptops or personal workstations who need access to the provided software installations, but also High-Performance Computing (HPC) clusters, virtual machines running in a cloud environment, etc. Clients only have read-only access to the files included in a CernVM-FS repository, and are automatically notified when the contents of the filesystem has changed. The filesystem that is mounted on a client (under /cvmfs ) is a virtual filesystem, in the sense that data is only (down)loaded when it is actually accessed (and cached aggressively to ensure good performance). Mounting a CernVM-FS repository on a client will be covered in the first hands-on part of this tutorial . Extensive documentation on configuring a client is available in the CernVM-FS documentation .","title":"Clients"},{"location":"01_introduction/#stratum-0-repository","text":"A CernVM-FS repository is the single source of (new) data for a filesystem. This single source is also called the Stratum 0 , which can be viewed as the central server the CernVM-FS network. Multiple repositories can be hosted on a single Stratum 0 server. A repository is a form of content-addressable storage, which is maintained by a dedicated release manager machine or publisher . All data stored into CernVM-FS has to be converted into a repository during the process of publishing , which involves creating the file catalog(s), compressing files, calculating content hashes, etc. A read-writable copy of a CernVM-FS repository is (only) available on a publisher system, which can be the same system as the Stratum 0 server. Providing write access is done by means of a union filesystem , which involves overlaying a read-only mount of the CernVM-FS filesystem with a writable scratch area. Publishing is an atomic operation: adding or changing files in a repository is done by ingesting files and creating a transaction that records the changes. In the first hands-on part of this tutorial we will guide you through the process of creating a CernVM-FS repository, which is also covered in detail in the CernVM-FS documentation . The 3rd hands-on part of this tutorial will focus on the publishing procedure to update the contents of a CernVM-FS repository.","title":"Stratum 0 + repository"},{"location":"01_introduction/#stratum-1-replica-servers","text":"A Stratum 1 replica server is a standard web server that provides a read-only mirror of a CernVM-FS repository served by a Stratum 0. The main purpose of a Stratum 1 is to improve reliability of the CernVM-FS network, by reducing the load on the Stratum 0. There usually are multiple Stratum 1 servers in a CernVM-FS network, which are typically distributed globally. Although clients can access a CernVM-FS repository directly via the Stratum 0, this is better done via the Stratum 1 replica servers. Stratum 1 servers enable clients to determine which Stratum 1 is geographically closest to connect to, via the Geo API which uses a GeoIP database that translates IP addresses of clients to longitude and latitude. Setting up a Stratum 1 replica server will be covered in the second hands-on part of this tutorial , and is also covered in detail in the CernVM-FS documentation .","title":"Stratum 1 replica servers"},{"location":"01_introduction/#squid-proxies","text":"To reduce load on Stratum 1 servers and to help reduce latency on clients, it is recommended to set up a Squid forward proxy servers . A Squid proxy caches content that has been accessed recently, and helps to reduce bandwidth and improve response times. This is particularly important on large systems like HPC clusters where many workernodes are accessing the CernVM-FS repository, where it's recommended to set up multiple Squid proxies. The second hands-on part of this tutorial will also cover setting up a Squid proxy. More details are available in the CernVM-FS documentation .","title":"Squid proxies"},{"location":"02_stratum0_client/","text":"Stratum 0 + client \u00b6 In order to get started with CernVM-FS, the first thing you need is a Stratum 0 server. This is the central server that hosts your repositories and makes it available to other machines. There can be only one Stratum 0 server for each repository, and from a security perspective it is usually recommended to restrict the access to this machine. We will look more into that later; for now, we are going to set up a Stratum 0, make a repository, and connect from a client machine directly to the Stratum 0. Set up the Stratum 0 \u00b6 Requirements \u00b6 Due to the scalable design of CernVM-FS, the host of your Stratum 0 server does not need a lot of resources in terms of CPU cores and memory; just a few cores and a few gigabytes of memory should suffice. Besides this, you need plenty of space to store the contents of your repository. By default, CernVM-FS uses /var/spool as scratch space while adding new files to the repository, and /srv/cvmfs as central repository storage location, but these locations can be modified in the CernVM-FS server settings later on. Furthermore, several (popular) Linux distributions are supported, see these requirements for a full list. We will only focus on CentOS in this tutorial. CernVM-FS also offers support for hosting the repository contents in S3 compatible storage, but for this tutorial we will focus on storing the files locally. For this we need an Apache server on the host, and port 80 should be open. Installation \u00b6 The installation of CernVM-FS is simple and only requires some packages to be installed. You can easily do this by adding the CernVM-FS repository and install the packages through your package manager: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y epel-release sudo yum install -y cvmfs cvmfs-server Alternatively, you can download the packages from the CernVM-FS downloads page and install them package manually; note that you need both the client and server package on your Stratum 0. Start Apache \u00b6 Since the Stratum 0 is serving contents via HTTP, Apache needs to be running before we can make a repository. The Apache ( httpd ) package should have been installed already, as it is a dependency of the cvmfs-server package, so it can now be enabled (so that it always starts after a reboot) and started using: sudo systemctl enable httpd sudo systemctl start httpd Create a repository \u00b6 Now that all required packages have been installed, it is time to create a repository. In the simplest way, this can be done by running the following command, which will make $USER the owner of the repository: sudo cvmfs_server mkfs -o $USER repo.organization.tld The full repository name, here repo.organization.tld , resembles a DNS name, but the organization.tld domain does not necessarily have to exist. It is recommended to give all the repositories belonging to the same project or organization the same .organization.tld domain here. This makes the client configuration much easier, also in case new repositories will be added later on. Repository keys \u00b6 For each repository that you create, a set of keys will be generated in /etc/cvmfs/keys : - repo.organization.tld.crt - the repository\u2019s public key (encoded as X509 certificate); - repo.organization.tld.key - the repository's private key; - repo.organization.tld.masterkey - the repository's private master key; - repo.organization.tld.pub - repository\u2019s public key (RSA). The public key is the one that is needed by clients in order to access the repository; we will need this later on. The master key is used to sign a whitelist of known publisher certificates; this whitelist is, by default, valid for 30 days, so the signing has to be done regularly. For now we are going to use one master key per repository, but in practice it is recommended to use the same master key for all repositories under a single domain, so that clients only need a single public key to access all repositories under this domain. We will explain this in more detail in the advanced section. TODO: DO WE WANT TO EXPLAIN THIS? Add some files to the repository \u00b6 A new repository automatically gets a file new_repository in its root ( /cvmfs/repo.organization.tld ). You can add more files by starting and publishing a transaction, which will be explained in more detail in a later section. For now it is enough to just run the following commands as root: MY_REPO_NAME = repo.organization.tld sudo cvmfs_server transaction ${ MY_REPO_NAME } # Now make some changes in /cvmfs/${MY_REPO_NAME}, # e.g. by adding files or directories. # If you made $USER the owner of the repository, you can do this without sudo. sudo cvmfs_server publish ${ MY_REPO_NAME } Cronjob for resigning the whitelist \u00b6 Each CernVM-FS repository has a whitelist containing fingerprints of certificates that are allowed to sign the repository. This whitelist has an expiration time of, by default, 30 days. This means that you regularly have to resign the whitelist. There are several ways to do this, see for instance the page about master keys in the documentation. If you just keep the master key on our Stratum 0 sever, you can set up a simple cronjob for resigning the whitelist. For instance, make a file /etc/cron.d/cvmfs_resign with the following content to do this every Monday at 11:00: 0 11 * * 1 root /usr/bin/cvmfs_server resign repo.organization.tld Remove a repository \u00b6 An existing repository can be removed by running: sudo cvmfs_server rmfs repo.organization.tld Set up a client \u00b6 Accessing CernVM-FS repositories on a client machine involves three steps: installing the CernVM-FS client package, adding some configuration files for the repository you want to connect to, and finally run a CernVM-FS setup procedure that will mount the repository. Since the client is going to pull in files over an HTTP connection, you need sufficient space for storing a local cache on the client machine. You can define the maximum size of your cache in the settings; the larger your cache is, the less often you have to pull in files again, and the faster your applications will start. Note that you can add more cache layers by adding a proxy nearby your client; this will be covered in a later section. Installation \u00b6 The installation is the same as for the Stratum 0, except that you only need the cvmfs package: sudo yum install https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs Configuration \u00b6 Many popular/large organizations hosting CernVM-FS repositories offer a client package that you can install to do most of the configuration. For our repository, we are going to do this manually. All required configuration files will have to be stored somewhere under /etc/cvmfs . We will discuss them one by one, where we use repo.organization.tld as repository name, and hence organization.tld as domain. /etc/cvmfs/keys/organization.tld/repo.organization.tld.pub \u00b6 This file contains the public key of the repository you want to access. You can copy this file from your Stratum 0 server, where it should be stored at the same location. /etc/cvmfs/config.d/repo.organization.tld.conf \u00b6 This file contains the main configuration for the repository you want to access, which should minimally contain the URL(s) of the Stratum 1 servers and the location of the public key. Because we do not have a Stratum 1 server yet, we are going to (mis)use our Stratum 0 as a Stratum 1. You should not do this in production! A typical, minimal configuration should look as follows: CVMFS_SERVER_URL=\"http://your-stratum0/cvmfs/@fqrn@\" CVMFS_PUBLIC_KEY=\"/etc/cvmfs/keys/organization.tld/repo.organization.tld.pub\" Note that the CVMFS_SERVER_URL should have the /cvmfs/@fqrn ; the last part will automatically be replaced by the full name of your repository. /etc/cvmfs/default.local \u00b6 This file can be used for setting or overriding settings that are specific to your client machine. One required parameter is CVMFS_HTTP_PROXY , which should point to your local proxy that serves as a cache between your client(s) and the Stratum 1 server(s). Since we do not have a proxy yet, we are setting this to DIRECT , meaning that we connect directly to the Stratum 1: CVMFS_HTTP_PROXY=DIRECT You can also use this file to set a maximum size (in megabytes) for the cache, for instance: CVMFS_QUOTA_LIMIT=50000 Mount the repositories \u00b6 When your configuration is complete, you can run the following command as root to mount the repository: sudo cvmfs_config setup This should not return any error message. If you do run into an issue, check out the debugging section on the Advanced topics page . Browse the repository \u00b6 Finally, we can try to access our repository on the client machine. Note that CernVM-FS uses autofs , which means that you may not see the repository when you do ls /cvmfs . Your repository will only be actually mounted when you access it, and may be unmounted after not using it for a while. So, the following should work and show the contents of your repository: ls /cvmfs/repo.organization.tld Exercise \u00b6 Set up your own Stratum 0 on a virtual machine. Create a repository with a suitable name ( name.domain.tld ). Add a simple bash script to the repository, which, for instance, prints Hello world! . Install and configure the CernVM-FS client on another virtual machine, using the Stratum 0 as your Stratum 1 for now. Try to access your repository and run your bash script on the client.","title":"Stratum 0 + client"},{"location":"02_stratum0_client/#stratum-0-client","text":"In order to get started with CernVM-FS, the first thing you need is a Stratum 0 server. This is the central server that hosts your repositories and makes it available to other machines. There can be only one Stratum 0 server for each repository, and from a security perspective it is usually recommended to restrict the access to this machine. We will look more into that later; for now, we are going to set up a Stratum 0, make a repository, and connect from a client machine directly to the Stratum 0.","title":"Stratum 0 + client"},{"location":"02_stratum0_client/#set-up-the-stratum-0","text":"","title":"Set up the Stratum 0"},{"location":"02_stratum0_client/#requirements","text":"Due to the scalable design of CernVM-FS, the host of your Stratum 0 server does not need a lot of resources in terms of CPU cores and memory; just a few cores and a few gigabytes of memory should suffice. Besides this, you need plenty of space to store the contents of your repository. By default, CernVM-FS uses /var/spool as scratch space while adding new files to the repository, and /srv/cvmfs as central repository storage location, but these locations can be modified in the CernVM-FS server settings later on. Furthermore, several (popular) Linux distributions are supported, see these requirements for a full list. We will only focus on CentOS in this tutorial. CernVM-FS also offers support for hosting the repository contents in S3 compatible storage, but for this tutorial we will focus on storing the files locally. For this we need an Apache server on the host, and port 80 should be open.","title":"Requirements"},{"location":"02_stratum0_client/#installation","text":"The installation of CernVM-FS is simple and only requires some packages to be installed. You can easily do this by adding the CernVM-FS repository and install the packages through your package manager: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y epel-release sudo yum install -y cvmfs cvmfs-server Alternatively, you can download the packages from the CernVM-FS downloads page and install them package manually; note that you need both the client and server package on your Stratum 0.","title":"Installation"},{"location":"02_stratum0_client/#start-apache","text":"Since the Stratum 0 is serving contents via HTTP, Apache needs to be running before we can make a repository. The Apache ( httpd ) package should have been installed already, as it is a dependency of the cvmfs-server package, so it can now be enabled (so that it always starts after a reboot) and started using: sudo systemctl enable httpd sudo systemctl start httpd","title":"Start Apache"},{"location":"02_stratum0_client/#create-a-repository","text":"Now that all required packages have been installed, it is time to create a repository. In the simplest way, this can be done by running the following command, which will make $USER the owner of the repository: sudo cvmfs_server mkfs -o $USER repo.organization.tld The full repository name, here repo.organization.tld , resembles a DNS name, but the organization.tld domain does not necessarily have to exist. It is recommended to give all the repositories belonging to the same project or organization the same .organization.tld domain here. This makes the client configuration much easier, also in case new repositories will be added later on.","title":"Create a repository"},{"location":"02_stratum0_client/#repository-keys","text":"For each repository that you create, a set of keys will be generated in /etc/cvmfs/keys : - repo.organization.tld.crt - the repository\u2019s public key (encoded as X509 certificate); - repo.organization.tld.key - the repository's private key; - repo.organization.tld.masterkey - the repository's private master key; - repo.organization.tld.pub - repository\u2019s public key (RSA). The public key is the one that is needed by clients in order to access the repository; we will need this later on. The master key is used to sign a whitelist of known publisher certificates; this whitelist is, by default, valid for 30 days, so the signing has to be done regularly. For now we are going to use one master key per repository, but in practice it is recommended to use the same master key for all repositories under a single domain, so that clients only need a single public key to access all repositories under this domain. We will explain this in more detail in the advanced section. TODO: DO WE WANT TO EXPLAIN THIS?","title":"Repository keys"},{"location":"02_stratum0_client/#add-some-files-to-the-repository","text":"A new repository automatically gets a file new_repository in its root ( /cvmfs/repo.organization.tld ). You can add more files by starting and publishing a transaction, which will be explained in more detail in a later section. For now it is enough to just run the following commands as root: MY_REPO_NAME = repo.organization.tld sudo cvmfs_server transaction ${ MY_REPO_NAME } # Now make some changes in /cvmfs/${MY_REPO_NAME}, # e.g. by adding files or directories. # If you made $USER the owner of the repository, you can do this without sudo. sudo cvmfs_server publish ${ MY_REPO_NAME }","title":"Add some files to the repository"},{"location":"02_stratum0_client/#cronjob-for-resigning-the-whitelist","text":"Each CernVM-FS repository has a whitelist containing fingerprints of certificates that are allowed to sign the repository. This whitelist has an expiration time of, by default, 30 days. This means that you regularly have to resign the whitelist. There are several ways to do this, see for instance the page about master keys in the documentation. If you just keep the master key on our Stratum 0 sever, you can set up a simple cronjob for resigning the whitelist. For instance, make a file /etc/cron.d/cvmfs_resign with the following content to do this every Monday at 11:00: 0 11 * * 1 root /usr/bin/cvmfs_server resign repo.organization.tld","title":"Cronjob for resigning the whitelist"},{"location":"02_stratum0_client/#remove-a-repository","text":"An existing repository can be removed by running: sudo cvmfs_server rmfs repo.organization.tld","title":"Remove a repository"},{"location":"02_stratum0_client/#set-up-a-client","text":"Accessing CernVM-FS repositories on a client machine involves three steps: installing the CernVM-FS client package, adding some configuration files for the repository you want to connect to, and finally run a CernVM-FS setup procedure that will mount the repository. Since the client is going to pull in files over an HTTP connection, you need sufficient space for storing a local cache on the client machine. You can define the maximum size of your cache in the settings; the larger your cache is, the less often you have to pull in files again, and the faster your applications will start. Note that you can add more cache layers by adding a proxy nearby your client; this will be covered in a later section.","title":"Set up a client"},{"location":"02_stratum0_client/#installation_1","text":"The installation is the same as for the Stratum 0, except that you only need the cvmfs package: sudo yum install https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs","title":"Installation"},{"location":"02_stratum0_client/#configuration","text":"Many popular/large organizations hosting CernVM-FS repositories offer a client package that you can install to do most of the configuration. For our repository, we are going to do this manually. All required configuration files will have to be stored somewhere under /etc/cvmfs . We will discuss them one by one, where we use repo.organization.tld as repository name, and hence organization.tld as domain.","title":"Configuration"},{"location":"02_stratum0_client/#etccvmfskeysorganizationtldrepoorganizationtldpub","text":"This file contains the public key of the repository you want to access. You can copy this file from your Stratum 0 server, where it should be stored at the same location.","title":"/etc/cvmfs/keys/organization.tld/repo.organization.tld.pub"},{"location":"02_stratum0_client/#etccvmfsconfigdrepoorganizationtldconf","text":"This file contains the main configuration for the repository you want to access, which should minimally contain the URL(s) of the Stratum 1 servers and the location of the public key. Because we do not have a Stratum 1 server yet, we are going to (mis)use our Stratum 0 as a Stratum 1. You should not do this in production! A typical, minimal configuration should look as follows: CVMFS_SERVER_URL=\"http://your-stratum0/cvmfs/@fqrn@\" CVMFS_PUBLIC_KEY=\"/etc/cvmfs/keys/organization.tld/repo.organization.tld.pub\" Note that the CVMFS_SERVER_URL should have the /cvmfs/@fqrn ; the last part will automatically be replaced by the full name of your repository.","title":"/etc/cvmfs/config.d/repo.organization.tld.conf"},{"location":"02_stratum0_client/#etccvmfsdefaultlocal","text":"This file can be used for setting or overriding settings that are specific to your client machine. One required parameter is CVMFS_HTTP_PROXY , which should point to your local proxy that serves as a cache between your client(s) and the Stratum 1 server(s). Since we do not have a proxy yet, we are setting this to DIRECT , meaning that we connect directly to the Stratum 1: CVMFS_HTTP_PROXY=DIRECT You can also use this file to set a maximum size (in megabytes) for the cache, for instance: CVMFS_QUOTA_LIMIT=50000","title":"/etc/cvmfs/default.local"},{"location":"02_stratum0_client/#mount-the-repositories","text":"When your configuration is complete, you can run the following command as root to mount the repository: sudo cvmfs_config setup This should not return any error message. If you do run into an issue, check out the debugging section on the Advanced topics page .","title":"Mount the repositories"},{"location":"02_stratum0_client/#browse-the-repository","text":"Finally, we can try to access our repository on the client machine. Note that CernVM-FS uses autofs , which means that you may not see the repository when you do ls /cvmfs . Your repository will only be actually mounted when you access it, and may be unmounted after not using it for a while. So, the following should work and show the contents of your repository: ls /cvmfs/repo.organization.tld","title":"Browse the repository"},{"location":"02_stratum0_client/#exercise","text":"Set up your own Stratum 0 on a virtual machine. Create a repository with a suitable name ( name.domain.tld ). Add a simple bash script to the repository, which, for instance, prints Hello world! . Install and configure the CernVM-FS client on another virtual machine, using the Stratum 0 as your Stratum 1 for now. Try to access your repository and run your bash script on the client.","title":"Exercise"},{"location":"03_stratum1_proxies/","text":"Stratum 1 and proxies \u00b6 In the previous section we have set up a Stratum 0 server and a client that directly connects to the Stratum 0. Although this worked fine, this setup is neither scalable, nor very reliable, nor secure: it is a single point of failure, too open in terms of connectivity, and it will have to serve all possible clients on its own. Therefore, this section will show how all these points can be addressed by adding one or more Stratum 1 servers and caching proxy servers. A Stratum 1 is a replica server that keeps mirrors of the repositories served by a Stratum 0. It is basically a web server that periodically synchronizes the contents of the repositories, and, in contrast to a Stratum 0 server, you can have multiple Stratum 1 servers. It is recommended to have several ones that are geographically distributed, so that clients always have a nearby Stratum 1 server. How many you need mostly depends on the distribution and number of clients, but often a few is already sufficient. More scalability can be added with proxies, which we will discuss later in this section. INSERT IMAGE OF CVMFS INFRA HERE Set up a Stratum 1 server \u00b6 Requirements \u00b6 A Stratum 1 servers has similar requirements as a Stratum 1 in terms of resources. Regarding the storage, it could do with less, because the Stratum 1 stores a deduplicated and compressed version of the repositories. In addition to port 80, also port 8000 has to be accessible for a Stratum 1. Furthermore, you need a (free) license key for Maxmind's Geo API , which you can obtain by signing up for an account . Installation \u00b6 For the Stratum 1 you need to install the following packages: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y epel-release sudo yum install -y cvmfs-server sudo yum install -y mod_wsgi sudo yum install -y squid Apache and Squid configuration \u00b6 We will be running Apache with a Squid frontend (reverse proxy); Apache will be listening internally on port 8080, while Squid needs to listen (externally) on port 80 and 8000, which are the default Stratum 1 ports. For this we first edit /etc/httpd/conf/httpd.conf and change the default: Listen 80 to: Listen 127.0.0.1:8080 Next, we replace the default contents of /etc/squid/squid.conf: with the following: http_port 80 accel http_port 8000 accel http_access allow all cache_peer 127.0.0.1 parent 8080 0 no-query originserver acl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/ cache deny !CVMFSAPI cache_mem 128 MB Finally, we start and enable Apache and Squid: sudo systemctl start httpd sudo systemctl start squid sudo systemctl enable httpd sudo systemctl enable squid DNS cache \u00b6 As a Stratum 1 server does a lot of DNS lookups, it is recommended to have a local DNS caching server on that same machine. We will not discuss this topic any further here, but you can use dnsmasq , bind , or systemd-resolved . See for instance (this tutorial)[https://geekflare.com/linux-server-local-dns-caching/] for setting up systemd-resolved . Create the Stratum 1 replica \u00b6 With all the required components in place, we can now really set up our Stratum 1 replica server. We first add our Geo API key to the CernVM-FS server settings: echo 'CVMFS_GEO_LICENSE_KEY=YOUR_KEY' | sudo tee -a /etc/cvmfs/server.local sudo chmod 600 /etc/cvmfs/server.local We also need to have the public keys of all repositories we want to mirror to be available on our Stratum 1. This can be done by copying all the corresponding .pub files from /etc/cvmfs/keys on your Stratum 0 server to /etc/cvmfs/keys/organization.tld/ (note the extra level!) on your Stratum 1 server. Now we make the replica by giving the URL to the repository on the Stratum 0 server (which is always like http://host:port/cvmfs/repository ) and the path to the corresponding public key: sudo cvmfs_server add-replica -o $USER http://YOUR_STRATUM0/cvmfs/repo.organization.tld /etc/cvmfs/keys/organization.tld/repo.organization.tld Manually synchronize the Stratum 1 \u00b6 The Stratum 1 has been registered, so now we should try to do a first synchronization. You can do this by running the following command: sudo cvmfs_server snapshot repo.organization.tld As there is not much in the repository yet, this should complete within a few seconds. Make cronjobs \u00b6 Whenever you make changes to the repository, the changes have to be synchronized to all Stratum 1 servers. Furthermore, the Geo database has to be updated regularly. Both tasks can be automated by setting up cronjobs that periodically run cvmfs_server update-geodb and cvmfs_server snapshot -a , where -a does the synchronization for all active repositories. This option will give an error if no log rotation has been configured for CernVM-FS, so we first have to create a file /etc/logrotate.d/cvmfs with the following contents: /var/log/cvmfs/*.log { weekly missingok notifempty } Now we can make a cronjob /etc/cron.d/cvmfs_stratum1_snapshot for the snapshots: */5 * * * * root output=$(/usr/bin/cvmfs_server snapshot -a -i 2>&1) || echo \"$output\" And another cronjob /etc/cron.d/cvmfs_geoip_db_update for updating the Geo database: 4 2 2 * * root /usr/bin/cvmfs_server update-geodb Set up a proxy \u00b6 If you have a lot of local machines, e.g. a cluster, that need to access your repositories, you also want another cache layer close to these machines. This can be done by adding Squid proxies between your local machine(s) and the layer of Stratum 1 servers. Usually it is recommended to have at least two of them for reliability and load-balancing reasons. Requirements \u00b6 Just as with the other components, the squid proxy server does not need a lot of resources. Just a few cores and few gigabytes of memory should be enough. The more disk space you allocate for this machine, the larger the cache can be, and the better the performance will be. Do note that this machine will only store a part of the (deduplicated and compressed) repository, so it does not need as much space as your Stratum 1. Installation \u00b6 The proxy server only requires Squid to be installed: sudo yum install -y squid Configuration \u00b6 The configuration of a standalone Squid is slightly different from the one that we used for our Stratum 1. You can use the following template to set up your own configuration: # List of local IP addresses (separate IPs and/or CIDR notation) allowed to access your local proxy acl local_nodes src YOUR_CLIENT_IPS # Destination domains that are allowed #acl stratum_ones dstdomain .YOURDOMAIN.ORG #acl stratum_ones dstdom_regex YOUR_REGEX # Squid port (default: 3128) # http_port 3128 # Deny access to anything which is not part of our stratum_ones ACL. http_access deny !stratum_ones # Only allow access from our local machines http_access allow local_nodes http_access allow localhost # Finally, deny all other access to this proxy http_access deny all minimum_expiry_time 0 maximum_object_size 1024 MB cache_mem 128 MB maximum_object_size_in_memory 128 KB # 50 GB disk cache cache_dir ufs /var/spool/squid 50000 16 256 You should use the local_nodes ACL here to specify which clients are allowed to use this proxy; you can use CIDR notation . Furthermore, you probably also want to have an ACL that specifies that your Squid should only cache the Stratum 1 servers. The template uses a stratum_ones ACL for this, and you can make use of either dstdomain (in case you have a single domain for all your Stratum 1 servers) or dstdom_regex for more complex situations. More information about Squid ACLs can be found in the (Squid documentation)[http://www.squid-cache.org/Doc/config/acl/]. Finally, there are some settings regarding the size of your cache. Make sure that you have enough disk space for the value that you provide. Starting Squid \u00b6 Before you actually start Squid, you can verify the correctness of your configuration with sudo squid -k parse . When you are sure things are okay, start and enable Squid: sudo systemctl start squid sudo systemctl enable squid Client configuration \u00b6 Now that we have a Stratum 0, one ore more Stratum 1 servers, and one or more local Squid proxies, all the infrastructure for a production-ready CernVM-FS setup is in place. This means that we can now configure our client properly, and start using the repository. In the previous section we connected our client directly to the Stratum 0. We are going to reuse that configuration, and only need to change two things. Connect to the Stratum 1 \u00b6 We used the URL of the Stratum 0 in the file /etc/cvmfs/config.d/repo.organization.tld.conf . We should now change this URL, and point to the Stratum 1 instead: CVMFS_SERVER_URL=\"http://your-stratum1/cvmfs/@fqrn@\" When you have more Stratum 1 servers inside the organization, you can make it a semicolon-separated list of servers. The Geo API will make sure that your client always connects to the geographically closest Stratum 1 server. Use the Squid proxy \u00b6 In order to use the local cache layer of our proxy, we have to instruct the client to send all requests through the proxy. This needs one small change in /etc/cvmfs/default.local , where you will have to set: CVMFS_HTTP_PROXY=\"http://your-proxy:3128\" More proxies can be added to that list by separating them with a pipe symbol. See for more (complex) examples this documentation page . Homework \u00b6 Set up a Stratum 1 server. Make sure that it includes: a proper Geo API license key; cronjobs for automatically synchronizing the database and updating the Geo database; properly configured Apache and Squid services; Set up a separate Squid proxy. Though it is recommended to at least have two in production, one is enough for now. TODO: reuse or set up a new client?? Add firewall rules to the Stratum 0? \u00b6 Reconfigure the client that you set up in the previous section and make sure that it uses your Stratum 1 and Squid proxy.","title":"Stratum 1 + proxies"},{"location":"03_stratum1_proxies/#stratum-1-and-proxies","text":"In the previous section we have set up a Stratum 0 server and a client that directly connects to the Stratum 0. Although this worked fine, this setup is neither scalable, nor very reliable, nor secure: it is a single point of failure, too open in terms of connectivity, and it will have to serve all possible clients on its own. Therefore, this section will show how all these points can be addressed by adding one or more Stratum 1 servers and caching proxy servers. A Stratum 1 is a replica server that keeps mirrors of the repositories served by a Stratum 0. It is basically a web server that periodically synchronizes the contents of the repositories, and, in contrast to a Stratum 0 server, you can have multiple Stratum 1 servers. It is recommended to have several ones that are geographically distributed, so that clients always have a nearby Stratum 1 server. How many you need mostly depends on the distribution and number of clients, but often a few is already sufficient. More scalability can be added with proxies, which we will discuss later in this section. INSERT IMAGE OF CVMFS INFRA HERE","title":"Stratum 1 and proxies"},{"location":"03_stratum1_proxies/#set-up-a-stratum-1-server","text":"","title":"Set up a Stratum 1 server"},{"location":"03_stratum1_proxies/#requirements","text":"A Stratum 1 servers has similar requirements as a Stratum 1 in terms of resources. Regarding the storage, it could do with less, because the Stratum 1 stores a deduplicated and compressed version of the repositories. In addition to port 80, also port 8000 has to be accessible for a Stratum 1. Furthermore, you need a (free) license key for Maxmind's Geo API , which you can obtain by signing up for an account .","title":"Requirements"},{"location":"03_stratum1_proxies/#installation","text":"For the Stratum 1 you need to install the following packages: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y epel-release sudo yum install -y cvmfs-server sudo yum install -y mod_wsgi sudo yum install -y squid","title":"Installation"},{"location":"03_stratum1_proxies/#apache-and-squid-configuration","text":"We will be running Apache with a Squid frontend (reverse proxy); Apache will be listening internally on port 8080, while Squid needs to listen (externally) on port 80 and 8000, which are the default Stratum 1 ports. For this we first edit /etc/httpd/conf/httpd.conf and change the default: Listen 80 to: Listen 127.0.0.1:8080 Next, we replace the default contents of /etc/squid/squid.conf: with the following: http_port 80 accel http_port 8000 accel http_access allow all cache_peer 127.0.0.1 parent 8080 0 no-query originserver acl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/ cache deny !CVMFSAPI cache_mem 128 MB Finally, we start and enable Apache and Squid: sudo systemctl start httpd sudo systemctl start squid sudo systemctl enable httpd sudo systemctl enable squid","title":"Apache and Squid configuration"},{"location":"03_stratum1_proxies/#dns-cache","text":"As a Stratum 1 server does a lot of DNS lookups, it is recommended to have a local DNS caching server on that same machine. We will not discuss this topic any further here, but you can use dnsmasq , bind , or systemd-resolved . See for instance (this tutorial)[https://geekflare.com/linux-server-local-dns-caching/] for setting up systemd-resolved .","title":"DNS cache"},{"location":"03_stratum1_proxies/#create-the-stratum-1-replica","text":"With all the required components in place, we can now really set up our Stratum 1 replica server. We first add our Geo API key to the CernVM-FS server settings: echo 'CVMFS_GEO_LICENSE_KEY=YOUR_KEY' | sudo tee -a /etc/cvmfs/server.local sudo chmod 600 /etc/cvmfs/server.local We also need to have the public keys of all repositories we want to mirror to be available on our Stratum 1. This can be done by copying all the corresponding .pub files from /etc/cvmfs/keys on your Stratum 0 server to /etc/cvmfs/keys/organization.tld/ (note the extra level!) on your Stratum 1 server. Now we make the replica by giving the URL to the repository on the Stratum 0 server (which is always like http://host:port/cvmfs/repository ) and the path to the corresponding public key: sudo cvmfs_server add-replica -o $USER http://YOUR_STRATUM0/cvmfs/repo.organization.tld /etc/cvmfs/keys/organization.tld/repo.organization.tld","title":"Create the Stratum 1 replica"},{"location":"03_stratum1_proxies/#manually-synchronize-the-stratum-1","text":"The Stratum 1 has been registered, so now we should try to do a first synchronization. You can do this by running the following command: sudo cvmfs_server snapshot repo.organization.tld As there is not much in the repository yet, this should complete within a few seconds.","title":"Manually synchronize the Stratum 1"},{"location":"03_stratum1_proxies/#make-cronjobs","text":"Whenever you make changes to the repository, the changes have to be synchronized to all Stratum 1 servers. Furthermore, the Geo database has to be updated regularly. Both tasks can be automated by setting up cronjobs that periodically run cvmfs_server update-geodb and cvmfs_server snapshot -a , where -a does the synchronization for all active repositories. This option will give an error if no log rotation has been configured for CernVM-FS, so we first have to create a file /etc/logrotate.d/cvmfs with the following contents: /var/log/cvmfs/*.log { weekly missingok notifempty } Now we can make a cronjob /etc/cron.d/cvmfs_stratum1_snapshot for the snapshots: */5 * * * * root output=$(/usr/bin/cvmfs_server snapshot -a -i 2>&1) || echo \"$output\" And another cronjob /etc/cron.d/cvmfs_geoip_db_update for updating the Geo database: 4 2 2 * * root /usr/bin/cvmfs_server update-geodb","title":"Make cronjobs"},{"location":"03_stratum1_proxies/#set-up-a-proxy","text":"If you have a lot of local machines, e.g. a cluster, that need to access your repositories, you also want another cache layer close to these machines. This can be done by adding Squid proxies between your local machine(s) and the layer of Stratum 1 servers. Usually it is recommended to have at least two of them for reliability and load-balancing reasons.","title":"Set up a proxy"},{"location":"03_stratum1_proxies/#requirements_1","text":"Just as with the other components, the squid proxy server does not need a lot of resources. Just a few cores and few gigabytes of memory should be enough. The more disk space you allocate for this machine, the larger the cache can be, and the better the performance will be. Do note that this machine will only store a part of the (deduplicated and compressed) repository, so it does not need as much space as your Stratum 1.","title":"Requirements"},{"location":"03_stratum1_proxies/#installation_1","text":"The proxy server only requires Squid to be installed: sudo yum install -y squid","title":"Installation"},{"location":"03_stratum1_proxies/#configuration","text":"The configuration of a standalone Squid is slightly different from the one that we used for our Stratum 1. You can use the following template to set up your own configuration: # List of local IP addresses (separate IPs and/or CIDR notation) allowed to access your local proxy acl local_nodes src YOUR_CLIENT_IPS # Destination domains that are allowed #acl stratum_ones dstdomain .YOURDOMAIN.ORG #acl stratum_ones dstdom_regex YOUR_REGEX # Squid port (default: 3128) # http_port 3128 # Deny access to anything which is not part of our stratum_ones ACL. http_access deny !stratum_ones # Only allow access from our local machines http_access allow local_nodes http_access allow localhost # Finally, deny all other access to this proxy http_access deny all minimum_expiry_time 0 maximum_object_size 1024 MB cache_mem 128 MB maximum_object_size_in_memory 128 KB # 50 GB disk cache cache_dir ufs /var/spool/squid 50000 16 256 You should use the local_nodes ACL here to specify which clients are allowed to use this proxy; you can use CIDR notation . Furthermore, you probably also want to have an ACL that specifies that your Squid should only cache the Stratum 1 servers. The template uses a stratum_ones ACL for this, and you can make use of either dstdomain (in case you have a single domain for all your Stratum 1 servers) or dstdom_regex for more complex situations. More information about Squid ACLs can be found in the (Squid documentation)[http://www.squid-cache.org/Doc/config/acl/]. Finally, there are some settings regarding the size of your cache. Make sure that you have enough disk space for the value that you provide.","title":"Configuration"},{"location":"03_stratum1_proxies/#starting-squid","text":"Before you actually start Squid, you can verify the correctness of your configuration with sudo squid -k parse . When you are sure things are okay, start and enable Squid: sudo systemctl start squid sudo systemctl enable squid","title":"Starting Squid"},{"location":"03_stratum1_proxies/#client-configuration","text":"Now that we have a Stratum 0, one ore more Stratum 1 servers, and one or more local Squid proxies, all the infrastructure for a production-ready CernVM-FS setup is in place. This means that we can now configure our client properly, and start using the repository. In the previous section we connected our client directly to the Stratum 0. We are going to reuse that configuration, and only need to change two things.","title":"Client configuration"},{"location":"03_stratum1_proxies/#connect-to-the-stratum-1","text":"We used the URL of the Stratum 0 in the file /etc/cvmfs/config.d/repo.organization.tld.conf . We should now change this URL, and point to the Stratum 1 instead: CVMFS_SERVER_URL=\"http://your-stratum1/cvmfs/@fqrn@\" When you have more Stratum 1 servers inside the organization, you can make it a semicolon-separated list of servers. The Geo API will make sure that your client always connects to the geographically closest Stratum 1 server.","title":"Connect to the Stratum 1"},{"location":"03_stratum1_proxies/#use-the-squid-proxy","text":"In order to use the local cache layer of our proxy, we have to instruct the client to send all requests through the proxy. This needs one small change in /etc/cvmfs/default.local , where you will have to set: CVMFS_HTTP_PROXY=\"http://your-proxy:3128\" More proxies can be added to that list by separating them with a pipe symbol. See for more (complex) examples this documentation page .","title":"Use the Squid proxy"},{"location":"03_stratum1_proxies/#homework","text":"Set up a Stratum 1 server. Make sure that it includes: a proper Geo API license key; cronjobs for automatically synchronizing the database and updating the Geo database; properly configured Apache and Squid services; Set up a separate Squid proxy. Though it is recommended to at least have two in production, one is enough for now.","title":"Homework"},{"location":"03_stratum1_proxies/#todo-reuse-or-set-up-a-new-client-add-firewall-rules-to-the-stratum-0","text":"Reconfigure the client that you set up in the previous section and make sure that it uses your Stratum 1 and Squid proxy.","title":"TODO: reuse or set up a new client?? Add firewall rules to the Stratum 0?"},{"location":"04_publishing/","text":"Publishing \u00b6 The previous sections were mostly about setting up the infrastructure. Now that all the components for hosting and accessing your own CernVM-FS repositories are in place, it is time to really start using it. In this section we will give some more details about publishing files. Transactions \u00b6 As we already showed in a previous section, the easiest way to add files to your repository is by opening and publishing a transaction on your Stratum 0 server. By default, your repository directory under /cvmfs is read-only, but by a transaction makes the directory writable for the user that is owner of the repository. sudo cvmfs_server transaction repo.organization.tld Once you are done with making changes, be sure to change your working directory to somewhere outside of the repository (otherwise you will get an error), and publish your changes using: sudo cvmfs_server publish repo.organization.tld And you can always abort a transaction, which will undo all the non-published modifications: sudo cvmfs_server abort repo.organization.tld Ingesting tarballs \u00b6 When you need to compile software that you want to add to your repository, you may want to do the actual compilation on a different machine than your Stratum 0 and copy the resulting installation as a tarball to your Stratum 0. Instead of manually extracting the tarball and doing a transaction, the cvmfs_server command offers a more efficient method for directly publishing the contents of a tarball: sudo cvmfs_server ingest -b /some/path repo.organization.tld -t mytarball.tar The -b expects the relative location in your repository where the contents of the tarball, specified with -t , will be extracted. So, in this case, the tarball gets extracted to /cvmfs/repo.organization.tld/some/path . Note that passing / to -b does not work at the moment, but will be supported in a future release of CernVM-FS. In case you have a compressed tarball, you can use an appropriate decompression tool and write the output to stdout . This output can then be piped to cvmfs_server by setting -t - , e.g. for a .tar.gz file: gunzip -c mytarball.tar.gz | sudo cvmfs_server ingest -b /some/path -t - Tags \u00b6 By default, a newly published version of the repository will automatically get a tag with a timestamp in its name. This allows you to revert back to earlier versions. You can also set your own tag name and/or description upon publication: sudo cvmfs_server publish [-a tag name] [-m tag description] repo.organization.tld The tag subcommand for cvmfs_server allows you to create ( -a ), remove ( -r ), inspect ( -i ), or list ( -l ) tags of your repository, e.g.: sudo cvmfs_server tag -a \"v1.0\" repo.organization.tld sudo cvmfs_server tag -l repo.organization.tld With the rollback subcommand you can revert back to an earlier version. By default, this will be the previous version, but with -t you can specify a specific tag to revert to: sudo cvmfs_server rollback -t \"v0.5\" repo.organization.tld Catalogs \u00b6 All metadata about files in your repository is stored in a file catalog, which is a SQLite database. When a client accesses the repository for the first time, it first needs to retrieve this catalog. Only then it can start fetching the files it actually needs. Clients also need to regularly check for new versions of the repository, and redownload it if it has changed. As this catalog can quickly become quite large when you start adding more and more files, just having a single one would cause significant overhead. In order to keep them small, you can make use of nested catalogs by having several catalogs for different subtrees of your repository. All metadata for that part of the subtree will not be part of the main catalog anymore, and clients will only download the catalogs for the subtree(s) they are trying to access. The general recommendation is to have more than 1000 and fewer than 200,000 files/directories per (nested) catalog, and to bundle the files/directories that are often accessed together. For instance, it may make sense to make a catalog per installation directory of a specific version of some software in your repository. Making nested catalogs manually can be done in two ways, which we will describe in more detail. .cvmfscatalog files \u00b6 By adding an (empty) file named .cvmfscatalog into a directory of your repository, each following publish operation will automatically generate a nested catalog for the entire subtree below that directory. You can put these files at as many levels as you like, but do keep the aforementioned recommendations in mind. .cvmfsdirtab \u00b6 Instead of creating the .cvmfscatalog files manually, you can also add a file named .cvmfsdirtab to the root of your repository. In this file you can specify a list of relative directory paths (they all start from the root of your repository) that should get a nested catalog. You can also use wildcards to specify patterns and automatically include future contents, and use exclamation marks to exclude paths from a nested catalog. As an example, assume you have a typical HPC software module tree in your repository with the following structure: / \u251c\u2500 /software \u2502 \u251c\u2500 /software/app1 \u2502 \u2502 \u251c\u2500 /software/app1/1.0 \u2502 \u2502 \u251c\u2500 /software/app1/2.0 \u2502 \u251c\u2500 /software/app2 \u2502 \u2502 \u251c\u2500 /software/app2/20201201 \u2502 \u2502 \u251c\u2500 /software/app2/20210125 \u251c\u2500 /modules \u2502 \u251c\u2500 /modules/all \u2502 \u2502 \u251c\u2500 /modules/all/app1 \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app1/1.0.lua \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app1/2.0.lua \u2502 \u2502 \u251c\u2500 /modules/all/app2 \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app2/20201201.lua \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app2/20210125.lua For this repository the .cvmfsdirtab file may look like: # Nested catalog for each version of each application /software/*/* # One nested catalog for all software directories /software # Nested catalog containing for all modulefiles /modules After you have added this file to your repository, you should see automatically generated .cvmfscatalog files in all the specified directories (note that you can still place additional ones manually as well). You can also run cvmfs_server list-catalogs to get a full list of all the nested catalogs. One final note: if you use a .cvmfsdirtab file, a tarball ingestion using the cvmfs_server ingest command will not automatically create the nested catalogs. You will need to do another (empty) transaction right after the ingestion to trigger the creation of the nested catalogs. Homework \u00b6 We prepared a tarball that contains a tree with dummy software installations. You can find the tarball at: TODO: INSERT DETAILS Insert this tarball to a directory named software in your repository using the ingest subcommand (i.e. without actually extracting the tarball); Note that you get some warnings about the catalog containing too many entries; Fix the catalog issue by adding a .cvmfsdirtab file to the root of your repo, which automatically makes a catalog for each software installation directory; Make sure that the warning is gone when you publish this .cvmfsdirtab file. Instead, you may see a message about the catalog being defragmented (because lots of entries were cleaned up).","title":"Publishing"},{"location":"04_publishing/#publishing","text":"The previous sections were mostly about setting up the infrastructure. Now that all the components for hosting and accessing your own CernVM-FS repositories are in place, it is time to really start using it. In this section we will give some more details about publishing files.","title":"Publishing"},{"location":"04_publishing/#transactions","text":"As we already showed in a previous section, the easiest way to add files to your repository is by opening and publishing a transaction on your Stratum 0 server. By default, your repository directory under /cvmfs is read-only, but by a transaction makes the directory writable for the user that is owner of the repository. sudo cvmfs_server transaction repo.organization.tld Once you are done with making changes, be sure to change your working directory to somewhere outside of the repository (otherwise you will get an error), and publish your changes using: sudo cvmfs_server publish repo.organization.tld And you can always abort a transaction, which will undo all the non-published modifications: sudo cvmfs_server abort repo.organization.tld","title":"Transactions"},{"location":"04_publishing/#ingesting-tarballs","text":"When you need to compile software that you want to add to your repository, you may want to do the actual compilation on a different machine than your Stratum 0 and copy the resulting installation as a tarball to your Stratum 0. Instead of manually extracting the tarball and doing a transaction, the cvmfs_server command offers a more efficient method for directly publishing the contents of a tarball: sudo cvmfs_server ingest -b /some/path repo.organization.tld -t mytarball.tar The -b expects the relative location in your repository where the contents of the tarball, specified with -t , will be extracted. So, in this case, the tarball gets extracted to /cvmfs/repo.organization.tld/some/path . Note that passing / to -b does not work at the moment, but will be supported in a future release of CernVM-FS. In case you have a compressed tarball, you can use an appropriate decompression tool and write the output to stdout . This output can then be piped to cvmfs_server by setting -t - , e.g. for a .tar.gz file: gunzip -c mytarball.tar.gz | sudo cvmfs_server ingest -b /some/path -t -","title":"Ingesting tarballs"},{"location":"04_publishing/#tags","text":"By default, a newly published version of the repository will automatically get a tag with a timestamp in its name. This allows you to revert back to earlier versions. You can also set your own tag name and/or description upon publication: sudo cvmfs_server publish [-a tag name] [-m tag description] repo.organization.tld The tag subcommand for cvmfs_server allows you to create ( -a ), remove ( -r ), inspect ( -i ), or list ( -l ) tags of your repository, e.g.: sudo cvmfs_server tag -a \"v1.0\" repo.organization.tld sudo cvmfs_server tag -l repo.organization.tld With the rollback subcommand you can revert back to an earlier version. By default, this will be the previous version, but with -t you can specify a specific tag to revert to: sudo cvmfs_server rollback -t \"v0.5\" repo.organization.tld","title":"Tags"},{"location":"04_publishing/#catalogs","text":"All metadata about files in your repository is stored in a file catalog, which is a SQLite database. When a client accesses the repository for the first time, it first needs to retrieve this catalog. Only then it can start fetching the files it actually needs. Clients also need to regularly check for new versions of the repository, and redownload it if it has changed. As this catalog can quickly become quite large when you start adding more and more files, just having a single one would cause significant overhead. In order to keep them small, you can make use of nested catalogs by having several catalogs for different subtrees of your repository. All metadata for that part of the subtree will not be part of the main catalog anymore, and clients will only download the catalogs for the subtree(s) they are trying to access. The general recommendation is to have more than 1000 and fewer than 200,000 files/directories per (nested) catalog, and to bundle the files/directories that are often accessed together. For instance, it may make sense to make a catalog per installation directory of a specific version of some software in your repository. Making nested catalogs manually can be done in two ways, which we will describe in more detail.","title":"Catalogs"},{"location":"04_publishing/#cvmfscatalog-files","text":"By adding an (empty) file named .cvmfscatalog into a directory of your repository, each following publish operation will automatically generate a nested catalog for the entire subtree below that directory. You can put these files at as many levels as you like, but do keep the aforementioned recommendations in mind.","title":".cvmfscatalog files"},{"location":"04_publishing/#cvmfsdirtab","text":"Instead of creating the .cvmfscatalog files manually, you can also add a file named .cvmfsdirtab to the root of your repository. In this file you can specify a list of relative directory paths (they all start from the root of your repository) that should get a nested catalog. You can also use wildcards to specify patterns and automatically include future contents, and use exclamation marks to exclude paths from a nested catalog. As an example, assume you have a typical HPC software module tree in your repository with the following structure: / \u251c\u2500 /software \u2502 \u251c\u2500 /software/app1 \u2502 \u2502 \u251c\u2500 /software/app1/1.0 \u2502 \u2502 \u251c\u2500 /software/app1/2.0 \u2502 \u251c\u2500 /software/app2 \u2502 \u2502 \u251c\u2500 /software/app2/20201201 \u2502 \u2502 \u251c\u2500 /software/app2/20210125 \u251c\u2500 /modules \u2502 \u251c\u2500 /modules/all \u2502 \u2502 \u251c\u2500 /modules/all/app1 \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app1/1.0.lua \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app1/2.0.lua \u2502 \u2502 \u251c\u2500 /modules/all/app2 \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app2/20201201.lua \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app2/20210125.lua For this repository the .cvmfsdirtab file may look like: # Nested catalog for each version of each application /software/*/* # One nested catalog for all software directories /software # Nested catalog containing for all modulefiles /modules After you have added this file to your repository, you should see automatically generated .cvmfscatalog files in all the specified directories (note that you can still place additional ones manually as well). You can also run cvmfs_server list-catalogs to get a full list of all the nested catalogs. One final note: if you use a .cvmfsdirtab file, a tarball ingestion using the cvmfs_server ingest command will not automatically create the nested catalogs. You will need to do another (empty) transaction right after the ingestion to trigger the creation of the nested catalogs.","title":".cvmfsdirtab"},{"location":"04_publishing/#homework","text":"We prepared a tarball that contains a tree with dummy software installations. You can find the tarball at: TODO: INSERT DETAILS Insert this tarball to a directory named software in your repository using the ingest subcommand (i.e. without actually extracting the tarball); Note that you get some warnings about the catalog containing too many entries; Fix the catalog issue by adding a .cvmfsdirtab file to the root of your repo, which automatically makes a catalog for each software installation directory; Make sure that the warning is gone when you publish this .cvmfsdirtab file. Instead, you may see a message about the catalog being defragmented (because lots of entries were cleaned up).","title":"Homework"},{"location":"05_advanced/","text":"Advanced topics \u00b6 gateway-publisher (with warnings) installing these takes time prepared: 1 gateway + 2 publishers demo only (no hands-on) troubleshooting exploding containers + use via Singularity demo Debugging issues \u00b6 If you are experiencing issues with your CernVM-FS setup, there are various ways to start debugging. Most issues are caused by wrongly configured clients (either a configuration issue, or a wrong public key) and connection or firewall issues. In order to find the cause of the issue, we should first find out where the issue is being caused. You can start by checking the client configuration for syntax errors: sudo cvmfs_config chksetup This should return OK . To make sure that your configuration is really picked up and set correctly (because of the hierarchical structure of the configuration, it is easily possible that some parameter gets overwritten by another configuration file), you can dump the effective configuration for your repository using: sudo cvmfs_config showconfig repo.organization.tld Make sure that at least CVMFS_HTTP_PROXY and CVMFS_SERVER_URL are set correctly, and that the directory pointed to by CVMFS_KEYS_DIR really contains the (correct) public key file. The probe subcommand can be used for (re)trying to mount the repository, and should normally return OK : sudo cvmfs_config probe repo.organization.tld Probing /cvmfs/repo.organization.tld... OK But, since you were debugging an issue, it probably returns an error. So, let's enable some debugging output by adding the following line to your /etc/cvmfs/default.local : CVMFS_DEBUGLOG=/path/to/cvmfs.log Now we unmount the repository, re-run the setup step, and try to probe it again: sudo cvmfs_config umount sudo cvmfs_config setup sudo cvmfs_config probe repo.organization.tld You can now check your debug log file, and look for any error messages near the bottom of the file; they may reveal more details about the issue. If it turns out to be some kind of connection issue, you can trace it down further by manually checking the connections to your proxy and/or Stratum 1 server. First, let's rule out that it is some kind of firewall issue by verifying that you can actually connect to the appropriate ports on those servers: telnet url-to-your-proxy 3128 telnet url-to-your-stratum1 80 If this does work, probably something is wrong with the services running on these machines. Every CernVM-FS repository has a file named .cvmfspublished , and you can try to fetch it manually using curl , both directly from the Stratum 1 and via your proxy: # Without your own proxy, i.e. directly go to the Stratum 1: curl --head http://url-to-your-stratum1/cvmfs/repo.organization.tld/.cvmfspublished # With your caching proxy between the client and Stratum 1: curl --proxy http://url-to-your-proxy:3128 --head http://url-to-your-stratum1/cvmfs/repo.organization.tld/.cvmfspublished These commands should return HTTP/1.1 200 OK . If the first command returns something else, you should inspect your CernVM-FS, Apache, and Squid configuration (and log) files on the Stratum 1 server. If the first curl command does work, but the second does not, there is something wrong with your Squid proxy; make sure that it is running, configured, and able to access your Stratum 1 server. Gateway and Publishers \u00b6 Only being able to modify your repository on the Stratum 0 server can be a bit of a limitation, especially when multiple people have to maintain the repository. A quite new feature in CernVM-FS allows you to set up so-called publisher machines, which are separate machines that are allowed to modify the repository. It also allows for setting up simple ACLs to give certain machines only access to subtrees of the repository. In order to use this feature you also need a gateway machine that has the repository storage mounted; the easiest way to set it up is by having a single machine that serves as both the Stratum 0 and the gateway. This is the setup that we will explain here. Do note that this is a fairly new feature and is not used a lot by production sites yet. Therefore, use it at your own risk! Gateway \u00b6 Requirements \u00b6 This machine has the same requirements as a standard Stratum 0 server, except that it also needs an additional port for the gateway service. This port is configurable, but by default port 4929 is used. Installation \u00b6 Perform the installation steps for the Stratum 0, which can be found in an earlier section. Additionally, install the cvmfs-gateway package: sudo yum install -y cvmfs-gateway Now make your repository using: sudo cvmfs_server mkfs -o $USER repo.organization.tld Configuration \u00b6 The gateway requires you to set up a configuration file /etc/cvmfs/gateway/repo.json . This is a JSON file containing the name of the repository, the keys that can be used by publishers to get access to the repository, and the (sub)path that these publishers are allowed to publish to. The cvmfs-gateway package will make an example file for you, which you can edit or overwrite. It should look like this: { \"version\": 2, \"repos\" : [ { \"domain\" : \"repo.organization.tld\", \"keys\" : [ { \"id\": \"keyid1\", \"path\": \"/\" }, { \"id\": \"keyid2\", \"path\": \"/restricted/to/subdir\" } ] } ], \"keys\" : [ { \"type\" : \"plain_text\", \"id\" : \"keyid1\", \"secret\" : \"SOME_SECRET\" }, { \"type\" : \"plain_text\", \"id\" : \"keyid2\", \"secret\" : \"SOME_OTHER_SECRET\" }, ] } You can choose the key IDs and secrets yourself; the secret has to be given to the owner of the corresponding publisher machine. Finally, there is a second configuration file /etc/cvmfs/gateway/user.json . This is where you can, for instance, change the port of the gateway service and the maximum length of an acquired lease. Assuming you do not have to change the port, you can leave it as it is for now. Starting the service \u00b6 We can now start the gateway service using: systemctl start cvmfs-gateway Do note that, once this service is running, you should not open transactions on this machine anymore, or you may corrupt the repository. If you do want to open a transaction, stop the gateway service first. Publisher \u00b6 Requirements \u00b6 This machine has no special requirements. Installation \u00b6 The publisher only needs the cvmfs-server package to be installed: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs-server Configuration \u00b6 The publisher machine only needs three files with keys: the repository's public key: repo.organization.tld.pub ; the repository's public key encoded as X509 certificate: repo.organization.tld.crt ; the gateway API key stored in a file named repo.organization.tld.gw . The first two files can be taken from /etc/cvmfs/keys on your Stratum 0 server. The latter can be created manually and should just contain the secret that you used in the gateway configuration. Place all these files in some (temporary) directory on your publisher machine. Make the repository \u00b6 We can now make the repository available for writing on our publisher machine by running: sudo cvmfs_server mkfs -w http://YOUR_STRATUM0_GATEWAY/cvmfs/repo.organization.tld \\ -u gw,/srv/cvmfs/repo.organization.tld/data/txn,http://YOUR_STRATUM0_GATEWAY:4929/api/v1 \\ -k /path/to/keys/dir -o `whoami` repo.organization.tld Replace both occurrences of YOUR_STRATUM0_GATEWAY by the IP address or hostname of your gateway / Stratum 0 server (and change 4929 in case you changed the gateway port), and /path/to/keys/dir by the path where you stored the keys in the previous step. Start publishing! \u00b6 You should now be able to make changes to the repository by opening a transaction. Note that you can do it as the regular user: cvmfs_server transaction repo.organization.tld # MAKE CHANGES TO /cvmfs/repo.organization.tld cvmfs_server publish repo.organization.tld","title":"Advanced topics"},{"location":"05_advanced/#advanced-topics","text":"gateway-publisher (with warnings) installing these takes time prepared: 1 gateway + 2 publishers demo only (no hands-on) troubleshooting exploding containers + use via Singularity demo","title":"Advanced topics"},{"location":"05_advanced/#debugging-issues","text":"If you are experiencing issues with your CernVM-FS setup, there are various ways to start debugging. Most issues are caused by wrongly configured clients (either a configuration issue, or a wrong public key) and connection or firewall issues. In order to find the cause of the issue, we should first find out where the issue is being caused. You can start by checking the client configuration for syntax errors: sudo cvmfs_config chksetup This should return OK . To make sure that your configuration is really picked up and set correctly (because of the hierarchical structure of the configuration, it is easily possible that some parameter gets overwritten by another configuration file), you can dump the effective configuration for your repository using: sudo cvmfs_config showconfig repo.organization.tld Make sure that at least CVMFS_HTTP_PROXY and CVMFS_SERVER_URL are set correctly, and that the directory pointed to by CVMFS_KEYS_DIR really contains the (correct) public key file. The probe subcommand can be used for (re)trying to mount the repository, and should normally return OK : sudo cvmfs_config probe repo.organization.tld Probing /cvmfs/repo.organization.tld... OK But, since you were debugging an issue, it probably returns an error. So, let's enable some debugging output by adding the following line to your /etc/cvmfs/default.local : CVMFS_DEBUGLOG=/path/to/cvmfs.log Now we unmount the repository, re-run the setup step, and try to probe it again: sudo cvmfs_config umount sudo cvmfs_config setup sudo cvmfs_config probe repo.organization.tld You can now check your debug log file, and look for any error messages near the bottom of the file; they may reveal more details about the issue. If it turns out to be some kind of connection issue, you can trace it down further by manually checking the connections to your proxy and/or Stratum 1 server. First, let's rule out that it is some kind of firewall issue by verifying that you can actually connect to the appropriate ports on those servers: telnet url-to-your-proxy 3128 telnet url-to-your-stratum1 80 If this does work, probably something is wrong with the services running on these machines. Every CernVM-FS repository has a file named .cvmfspublished , and you can try to fetch it manually using curl , both directly from the Stratum 1 and via your proxy: # Without your own proxy, i.e. directly go to the Stratum 1: curl --head http://url-to-your-stratum1/cvmfs/repo.organization.tld/.cvmfspublished # With your caching proxy between the client and Stratum 1: curl --proxy http://url-to-your-proxy:3128 --head http://url-to-your-stratum1/cvmfs/repo.organization.tld/.cvmfspublished These commands should return HTTP/1.1 200 OK . If the first command returns something else, you should inspect your CernVM-FS, Apache, and Squid configuration (and log) files on the Stratum 1 server. If the first curl command does work, but the second does not, there is something wrong with your Squid proxy; make sure that it is running, configured, and able to access your Stratum 1 server.","title":"Debugging issues"},{"location":"05_advanced/#gateway-and-publishers","text":"Only being able to modify your repository on the Stratum 0 server can be a bit of a limitation, especially when multiple people have to maintain the repository. A quite new feature in CernVM-FS allows you to set up so-called publisher machines, which are separate machines that are allowed to modify the repository. It also allows for setting up simple ACLs to give certain machines only access to subtrees of the repository. In order to use this feature you also need a gateway machine that has the repository storage mounted; the easiest way to set it up is by having a single machine that serves as both the Stratum 0 and the gateway. This is the setup that we will explain here. Do note that this is a fairly new feature and is not used a lot by production sites yet. Therefore, use it at your own risk!","title":"Gateway and Publishers"},{"location":"05_advanced/#gateway","text":"","title":"Gateway"},{"location":"05_advanced/#requirements","text":"This machine has the same requirements as a standard Stratum 0 server, except that it also needs an additional port for the gateway service. This port is configurable, but by default port 4929 is used.","title":"Requirements"},{"location":"05_advanced/#installation","text":"Perform the installation steps for the Stratum 0, which can be found in an earlier section. Additionally, install the cvmfs-gateway package: sudo yum install -y cvmfs-gateway Now make your repository using: sudo cvmfs_server mkfs -o $USER repo.organization.tld","title":"Installation"},{"location":"05_advanced/#configuration","text":"The gateway requires you to set up a configuration file /etc/cvmfs/gateway/repo.json . This is a JSON file containing the name of the repository, the keys that can be used by publishers to get access to the repository, and the (sub)path that these publishers are allowed to publish to. The cvmfs-gateway package will make an example file for you, which you can edit or overwrite. It should look like this: { \"version\": 2, \"repos\" : [ { \"domain\" : \"repo.organization.tld\", \"keys\" : [ { \"id\": \"keyid1\", \"path\": \"/\" }, { \"id\": \"keyid2\", \"path\": \"/restricted/to/subdir\" } ] } ], \"keys\" : [ { \"type\" : \"plain_text\", \"id\" : \"keyid1\", \"secret\" : \"SOME_SECRET\" }, { \"type\" : \"plain_text\", \"id\" : \"keyid2\", \"secret\" : \"SOME_OTHER_SECRET\" }, ] } You can choose the key IDs and secrets yourself; the secret has to be given to the owner of the corresponding publisher machine. Finally, there is a second configuration file /etc/cvmfs/gateway/user.json . This is where you can, for instance, change the port of the gateway service and the maximum length of an acquired lease. Assuming you do not have to change the port, you can leave it as it is for now.","title":"Configuration"},{"location":"05_advanced/#starting-the-service","text":"We can now start the gateway service using: systemctl start cvmfs-gateway Do note that, once this service is running, you should not open transactions on this machine anymore, or you may corrupt the repository. If you do want to open a transaction, stop the gateway service first.","title":"Starting the service"},{"location":"05_advanced/#publisher","text":"","title":"Publisher"},{"location":"05_advanced/#requirements_1","text":"This machine has no special requirements.","title":"Requirements"},{"location":"05_advanced/#installation_1","text":"The publisher only needs the cvmfs-server package to be installed: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs-server","title":"Installation"},{"location":"05_advanced/#configuration_1","text":"The publisher machine only needs three files with keys: the repository's public key: repo.organization.tld.pub ; the repository's public key encoded as X509 certificate: repo.organization.tld.crt ; the gateway API key stored in a file named repo.organization.tld.gw . The first two files can be taken from /etc/cvmfs/keys on your Stratum 0 server. The latter can be created manually and should just contain the secret that you used in the gateway configuration. Place all these files in some (temporary) directory on your publisher machine.","title":"Configuration"},{"location":"05_advanced/#make-the-repository","text":"We can now make the repository available for writing on our publisher machine by running: sudo cvmfs_server mkfs -w http://YOUR_STRATUM0_GATEWAY/cvmfs/repo.organization.tld \\ -u gw,/srv/cvmfs/repo.organization.tld/data/txn,http://YOUR_STRATUM0_GATEWAY:4929/api/v1 \\ -k /path/to/keys/dir -o `whoami` repo.organization.tld Replace both occurrences of YOUR_STRATUM0_GATEWAY by the IP address or hostname of your gateway / Stratum 0 server (and change 4929 in case you changed the gateway port), and /path/to/keys/dir by the path where you stored the keys in the previous step.","title":"Make the repository"},{"location":"05_advanced/#start-publishing","text":"You should now be able to make changes to the repository by opening a transaction. Note that you can do it as the regular user: cvmfs_server transaction repo.organization.tld # MAKE CHANGES TO /cvmfs/repo.organization.tld cvmfs_server publish repo.organization.tld","title":"Start publishing!"}]}