{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the CernVM-FS tutorial! \u00b6 Scope \u00b6 This is an introductory tutorial to the CernVM File System (CernVM-FS). In this tutorial you will learn what CernVM-FS is and how to use it, both from an administrator and end user point of view, through guided examples and hands-on exercises. We focus on the core concepts and basic usage of CernVM-FS, and provide pointers to more information regarding more advanced aspects. Intended audience \u00b6 This tutorial is intended for people who are new to CernVM-FS: no specific prior knowledge or experience with it is required. We expect it to be most valuable to people who are interested in either setting up and managing a CernVM-FS repository themselves, or using one or more existing CernVM-FS repositories. Prerequisites \u00b6 Being familiar with a Linux shell environment and having a basic notion of Linux filesystems and system administration is recommended but not strictly required; we hope that the guided examples are sufficient to follow along comfortably. You should have access to a couple of Linux instances on which you have administrative rights ( sudo access). The required resources are minimal: 1 or 2 cores, a couple of GBs of RAM, and about 10GB of disk space per instance is more than sufficient. We recommend using Linux virtual machines that were created specifically for this tutorial, on which only a base Linux distribution was installed as operating system, and which you are comfortable to discard afterwards. This tutorial was prepared on CentOS 7 ( x86_64 ), but it should be relatively straightforward to translate the instructions to other Linux distributions like Ubuntu, or another CPU architecture like Arm 64-bit ( aarch64 ). Practical information \u00b6 This tutorial is being organised during the 6th EasyBuild User Meeting (Jan 25-29 2021) . More information about the CernVM-FS tutorial sessions is available at https://easybuild.io/eum/#cvmfs-tutorial . Registration for this tutorial is required in order to get access to the provided cloud resources! Dedicated cloud resources in Microsoft Azure (sponsored by Microsoft) will be available only to registered tutorial attendees for working on the hands-on exercises during the the week of Jan 25-29 2021. Nothing in this tutorial, other than the section of using the provided Azure cloud resources , is specific to Azure. You can use your own resources if you prefer doing so. Tutorial contents \u00b6 0. Azure cloud resources 1. Introduction to CernVM-FS 2. Stratum 0 and client (*) 3. Stratum 1 and proxies (*) 4. Publishing (*) 5. Advanced topics (sections indicated with (*) involve hands-on exercises) Slides \u00b6 The (minimal) slides used to guide the tutorial are available here . Note: these slides are updated daily through the week of Jan 25-29, as the different parts of the tutorial are covered. Contributors \u00b6 Jakob Blomer (CERN, Switzerland) Bob Dr\u00f6ge (University of Groningen, The Netherlands) Kenneth Hoste (HPC-UGent, Belgium) Ryan Taylor (ComputeCanada) Additional resources \u00b6 CernVM-FS website: https://cernvm.cern.ch/fs CernVM-FS documentation: https://cvmfs.readthedocs.io CernVM-FS @ GitHub: https://github.com/cvmfs CernVM-FS tutorial @ GitHub: https://github.com/cvmfs-contrib/cvmfs-tutorial-2021","title":"Home"},{"location":"#welcome-to-the-cernvm-fs-tutorial","text":"","title":"Welcome to the CernVM-FS tutorial!"},{"location":"#scope","text":"This is an introductory tutorial to the CernVM File System (CernVM-FS). In this tutorial you will learn what CernVM-FS is and how to use it, both from an administrator and end user point of view, through guided examples and hands-on exercises. We focus on the core concepts and basic usage of CernVM-FS, and provide pointers to more information regarding more advanced aspects.","title":"Scope"},{"location":"#intended-audience","text":"This tutorial is intended for people who are new to CernVM-FS: no specific prior knowledge or experience with it is required. We expect it to be most valuable to people who are interested in either setting up and managing a CernVM-FS repository themselves, or using one or more existing CernVM-FS repositories.","title":"Intended audience"},{"location":"#prerequisites","text":"Being familiar with a Linux shell environment and having a basic notion of Linux filesystems and system administration is recommended but not strictly required; we hope that the guided examples are sufficient to follow along comfortably. You should have access to a couple of Linux instances on which you have administrative rights ( sudo access). The required resources are minimal: 1 or 2 cores, a couple of GBs of RAM, and about 10GB of disk space per instance is more than sufficient. We recommend using Linux virtual machines that were created specifically for this tutorial, on which only a base Linux distribution was installed as operating system, and which you are comfortable to discard afterwards. This tutorial was prepared on CentOS 7 ( x86_64 ), but it should be relatively straightforward to translate the instructions to other Linux distributions like Ubuntu, or another CPU architecture like Arm 64-bit ( aarch64 ).","title":"Prerequisites"},{"location":"#practical-information","text":"This tutorial is being organised during the 6th EasyBuild User Meeting (Jan 25-29 2021) . More information about the CernVM-FS tutorial sessions is available at https://easybuild.io/eum/#cvmfs-tutorial . Registration for this tutorial is required in order to get access to the provided cloud resources! Dedicated cloud resources in Microsoft Azure (sponsored by Microsoft) will be available only to registered tutorial attendees for working on the hands-on exercises during the the week of Jan 25-29 2021. Nothing in this tutorial, other than the section of using the provided Azure cloud resources , is specific to Azure. You can use your own resources if you prefer doing so.","title":"Practical information"},{"location":"#tutorial-contents","text":"0. Azure cloud resources 1. Introduction to CernVM-FS 2. Stratum 0 and client (*) 3. Stratum 1 and proxies (*) 4. Publishing (*) 5. Advanced topics (sections indicated with (*) involve hands-on exercises)","title":"Tutorial contents"},{"location":"#slides","text":"The (minimal) slides used to guide the tutorial are available here . Note: these slides are updated daily through the week of Jan 25-29, as the different parts of the tutorial are covered.","title":"Slides"},{"location":"#contributors","text":"Jakob Blomer (CERN, Switzerland) Bob Dr\u00f6ge (University of Groningen, The Netherlands) Kenneth Hoste (HPC-UGent, Belgium) Ryan Taylor (ComputeCanada)","title":"Contributors"},{"location":"#additional-resources","text":"CernVM-FS website: https://cernvm.cern.ch/fs CernVM-FS documentation: https://cvmfs.readthedocs.io CernVM-FS @ GitHub: https://github.com/cvmfs CernVM-FS tutorial @ GitHub: https://github.com/cvmfs-contrib/cvmfs-tutorial-2021","title":"Additional resources"},{"location":"00_azure_cloud_resources/","text":"0. Azure cloud resources \u00b6 For registered attendees of the CernVM-FS tutorial at the 6th EasyBuild User Meeting (Jan 25-29 2021) , a small cluster of virtual machines is available in Microsoft Azure to use for the hands-on parts of the tutorial. These resources are sponsored by Microsoft. Step 0: Log in to CycleCloud \u00b6 To get started, you should log into the Azure CycleCloud web portal, where you can manage the virtual machine cluster that was prepared for you. Visit https://cvmfstutorial.westeurope.cloudapp.azure.com , and log in with the user name ( cvmfs0XY ) and the password that you received via email (don't try using cvmfs001 as shown in the screenshot below, that will not work). You will need to create and confirm a new password in order to login in. Make sure you use a password you can remember! . If you need a password reset, either contact eum@lists.ugent.be , or ask in the #eum21-cvmfs-tutorial channel in the EasyBuild Slack. Step 1: Add SSH public key \u00b6 Before you do anything with the nodes in the cluster, make sure you add your SSH public key into your CycleCloud account. Note: this must be an RSA SSH public key ( ed25519 will not work)! Use My Profile which you can access by clicking your name in the top right corner: Then click Edit Profile : Add your SSH public key ( must be RSA! ), and click Save . Then use the Back to cluster link at the top left corner to return to the previous screen. Step 2: Start cluster \u00b6 Once you have added your SSH public key, you can start the cluster using the Start \"play\" button: Shortly after, you should see that all nodes were started (green status bar): Step 3: SSH into a node \u00b6 Once a node is started, you can SSH into it using your cvmfs0XY user name and the public IP address of the node. To determine the IP address of a node, select it in the Nodes overview: the IP address will be shown in the Host/IP column in the detailed view at the bottom: To log into the node, use the ssh command from a Linux or macOS system: # change accordingly for your user name and IP address! $ ssh cvmfs001@51.145.228.229 The authenticity of host '51.145.228.229 (51.145.228.229)' can 't be established. ECDSA key fingerprint is SHA256:Lv19NNnDJlMEcJskqS9tU7SlyLibD3u1vKSc7HJqlAM. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added ' 51 .145.228.229 ' ( ECDSA ) to the list of known hosts. [ cvmfs001@ip-0A000009 ~ ] $ (or use PuTTy on Windows). You will need to confirm the host key fingerprint the first time you log in (enter yes as shown above). Once you're logged in, you are ready to get started with the hands-on parts of the tutorial! Don't suspend unused nodes! \u00b6 The virtual machines do not have a fixed IP address. This means that their IP address will change if you suspend the machines while not using them and restart them later, and you will have to reconfigure your CernVM-FS services. Therefore we recommend to leave all your nodes running for the duration of the workshop. Other actions \u00b6 Resetting a single node \u00b6 If you have made some mistakes, and would like to reset a node to start from a blank slate, you can do so as follows: Select the node and use Actions -> Shut Down : Use the Terminate ... option (the default). Do keep in mind that this will erase all data and configuration for that node! Click Shut down , and wait until the node is fully off (gray status bar): Finally, start the node again to complete the reset cycle: Keep in mind that this implies that this node will have a different IP address afterwards! Resetting all nodes (entire cluster) \u00b6 If you want to restart from scratch on all nodes, you can do so to. Keep in mind that all data and configuration on all nodes will be lost when doing this. Terminate the entire cluster using the Terminate button just below the cluster name (same as your user name): Once the cluster is fully shut down (all node status bars are gray), you can start it again: Keep in mind that this implies that all nodes will have a different IP address afterwards!","title":"0. Azure cloud resources"},{"location":"00_azure_cloud_resources/#0-azure-cloud-resources","text":"For registered attendees of the CernVM-FS tutorial at the 6th EasyBuild User Meeting (Jan 25-29 2021) , a small cluster of virtual machines is available in Microsoft Azure to use for the hands-on parts of the tutorial. These resources are sponsored by Microsoft.","title":"0. Azure cloud resources"},{"location":"00_azure_cloud_resources/#step-0-log-in-to-cyclecloud","text":"To get started, you should log into the Azure CycleCloud web portal, where you can manage the virtual machine cluster that was prepared for you. Visit https://cvmfstutorial.westeurope.cloudapp.azure.com , and log in with the user name ( cvmfs0XY ) and the password that you received via email (don't try using cvmfs001 as shown in the screenshot below, that will not work). You will need to create and confirm a new password in order to login in. Make sure you use a password you can remember! . If you need a password reset, either contact eum@lists.ugent.be , or ask in the #eum21-cvmfs-tutorial channel in the EasyBuild Slack.","title":"Step 0: Log in to CycleCloud"},{"location":"00_azure_cloud_resources/#step-1-add-ssh-public-key","text":"Before you do anything with the nodes in the cluster, make sure you add your SSH public key into your CycleCloud account. Note: this must be an RSA SSH public key ( ed25519 will not work)! Use My Profile which you can access by clicking your name in the top right corner: Then click Edit Profile : Add your SSH public key ( must be RSA! ), and click Save . Then use the Back to cluster link at the top left corner to return to the previous screen.","title":"Step 1: Add SSH public key"},{"location":"00_azure_cloud_resources/#step-2-start-cluster","text":"Once you have added your SSH public key, you can start the cluster using the Start \"play\" button: Shortly after, you should see that all nodes were started (green status bar):","title":"Step 2: Start cluster"},{"location":"00_azure_cloud_resources/#step-3-ssh-into-a-node","text":"Once a node is started, you can SSH into it using your cvmfs0XY user name and the public IP address of the node. To determine the IP address of a node, select it in the Nodes overview: the IP address will be shown in the Host/IP column in the detailed view at the bottom: To log into the node, use the ssh command from a Linux or macOS system: # change accordingly for your user name and IP address! $ ssh cvmfs001@51.145.228.229 The authenticity of host '51.145.228.229 (51.145.228.229)' can 't be established. ECDSA key fingerprint is SHA256:Lv19NNnDJlMEcJskqS9tU7SlyLibD3u1vKSc7HJqlAM. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added ' 51 .145.228.229 ' ( ECDSA ) to the list of known hosts. [ cvmfs001@ip-0A000009 ~ ] $ (or use PuTTy on Windows). You will need to confirm the host key fingerprint the first time you log in (enter yes as shown above). Once you're logged in, you are ready to get started with the hands-on parts of the tutorial!","title":"Step 3: SSH into a node"},{"location":"00_azure_cloud_resources/#dont-suspend-unused-nodes","text":"The virtual machines do not have a fixed IP address. This means that their IP address will change if you suspend the machines while not using them and restart them later, and you will have to reconfigure your CernVM-FS services. Therefore we recommend to leave all your nodes running for the duration of the workshop.","title":"Don't suspend unused nodes!"},{"location":"00_azure_cloud_resources/#other-actions","text":"","title":"Other actions"},{"location":"00_azure_cloud_resources/#resetting-a-single-node","text":"If you have made some mistakes, and would like to reset a node to start from a blank slate, you can do so as follows: Select the node and use Actions -> Shut Down : Use the Terminate ... option (the default). Do keep in mind that this will erase all data and configuration for that node! Click Shut down , and wait until the node is fully off (gray status bar): Finally, start the node again to complete the reset cycle: Keep in mind that this implies that this node will have a different IP address afterwards!","title":"Resetting a single node"},{"location":"00_azure_cloud_resources/#resetting-all-nodes-entire-cluster","text":"If you want to restart from scratch on all nodes, you can do so to. Keep in mind that all data and configuration on all nodes will be lost when doing this. Terminate the entire cluster using the Terminate button just below the cluster name (same as your user name): Once the cluster is fully shut down (all node status bars are gray), you can start it again: Keep in mind that this implies that all nodes will have a different IP address afterwards!","title":"Resetting all nodes (entire cluster)"},{"location":"01_introduction/","text":"1. Introduction to CernVM-FS \u00b6 1.0 Introductory talk by Jakob Blomer \u00b6 The best possible introduction to CernVM-FS is the talk by Jakob Blomer (CERN, lead developer) at the 6th EasyBuild User Meeting. The slides for this talk are available here . The recording of this presentation is available on YouTube: 1.1 What is CernVM-FS? \u00b6 Let's get started with explaining in detail what CernVM-FS is... CernVM-FS in a nutshell The CernVM File System (CernVM-FS) provides a scalable and reliable software distribution service, which is implemented as a read-only POSIX filesystem in user space (a FUSE module). Files and directories are hosted on standard web servers and mounted in the universal namespace /cvmfs . That's a mouthful, so let's break it down a bit... 1.1.1 Read-only filesystem over HTTP \u00b6 CernVM-FS is a network filesystem , which you can mount in Linux or macOS via FUSE (Filesystem in Userspace) and on Windows in a WSL2 virtualized Linux environment. In some ways it is similar to other network filesystems like NFS or AFS , but there are various aspects to it that are quite different. The files and directories that are made available via CernVM-FS are always located in a subdirectory of /cvmfs , and are provisioned via a network of servers that can basically be viewed as web servers since only outgoing HTTP connections are used. This makes it easy to use CernVM-FS in environments that are protected by a strict firewall. CernVM-FS is a read-only filesystem for those who access it; only those who administer it are able to add or change its contents. Internally, CernVM-FS uses content-addressable storage and Merkle trees in order to maintain file data and meta-data, but the filesystem it exposes is a standard POSIX filesystem . 1.1.2 Software distribution system \u00b6 The primary use case of CernVM-FS is to easily distribute software around the world, which is reflected in various ways in the features implemented by CernVM-FS. It's worth highlighting that with software we actually mean software installations , that is the files that collectively form a usable instance of an application, tool, or library. This is in contrast with software packages (for example, RPMs), which are essentially bundles of files wrapped together for easy distribution, and which need to be installed in order to provide a working instance of the provided software. Software installations have specific characteristics, such as often involving lots of small files which are being opened and read as a whole regularly, frequent searching for files in multiple directories, hierarchical structuring, etc. CernVM-FS is heavily tuned to cater to this use case, with aggressive caching and reduction of latency, for example via automatic file de-duplication and compression. 1.1.3 Scalable and reliable \u00b6 CernVM-FS was designed to be scalable and reliable , with known deployments involving hundreds of millions of files and many tens of thousands of clients. It was originally created to fulfill the software distribution needs of the experiments at the Large Hadron Collider (LHC) at CERN. The network of (web) servers that make a CernVM-FS instance accessible is constructed such that it is robust against problems like network disconnects and hardware failures, and so it can be extended and tweaked on demand for optimal performance. More details are available in the CernVM-FS documentation . 1.2 Terminology \u00b6 Before we get our hands dirty, let's cover some of the terminology used by the CernVM-FS project. The figure included below shows the different components of the CernVM-FS network: the central Stratum 0 server which hosts the filesystem; the Stratum 1 replica servers, and the associated proxies ; the client accessing the filesystem provided via CernVM-FS. 1.2.1 Clients \u00b6 A client in the context of CernVM-FS is any system that mounts the filesystem. This includes laptops or personal workstations who need access to the provided software installations, but also High-Performance Computing (HPC) clusters, virtual machines running in a cloud environment, etc. Clients only have read-only access to the files included in a CernVM-FS repository, and are automatically notified and updated when the contents of the filesystem have changed. The filesystem that is mounted on a client (under /cvmfs ) is a virtual filesystem, in the sense that data is only (down)loaded when it is actually accessed (and cached aggressively to ensure good performance). Mounting a CernVM-FS repository on a client will be covered in the first hands-on part of this tutorial . Extensive documentation on configuring a client is available in the CernVM-FS documentation . 1.2.2 Stratum 0 + repository \u00b6 A CernVM-FS repository is an instance of a CernVM-FS filesystem. A repository is hosted on one Stratum 0 server, which is the single authoritative source of content for the repository. Multiple repositories can be hosted on the same Stratum 0 server. The data in a repository is stored using a content-addressable storage (CAS) scheme. All files written to a CernVM-FS repository must be converted into data chunks in the CAS store during the process of publishing , which involves creating catalogs which represent directory structure and metadata, and splitting files into chunks, compressing them, calculating content hashes, etc. Publishing is done on a dedicated release manager machine or publisher system which interfaces with the Stratum 0 server. Read-write access to a CernVM-FS repository is only available on a Stratum 0 server or publisher (the publisher and Stratum 0 can be the same system). Write access is provided via a union filesystem , which overlays a writable scratch area and the read-only mount of the CernVM-FS repository. Publishing is an atomic operation: adding or changing files in a repository is done in a transaction that records and collectively commits a set of file system changes, preventing partial or incomplete updates of the repository and ensuring that changes are either applied completely, or not at all. In the first hands-on part of this tutorial we will guide you through the process of creating a CernVM-FS repository, which is also covered in detail in the CernVM-FS documentation . The 3rd hands-on part of this tutorial will focus on the publishing procedure to update the contents of a CernVM-FS repository. 1.2.3 Stratum 1 replica servers \u00b6 A Stratum 1 replica server is a standard web server that provides a read-only mirror of a CernVM-FS repository served by a Stratum 0. The main purpose of a Stratum 1 is to improve reliability and capacity of the CernVM-FS network, by distributing load across multiple servers and allowing clients to fail over if one is unavailable, and to relieve the Stratum 0 from serving client requests. Although clients can access a CernVM-FS repository via the Stratum 0, it is advisable to block external client access to the Stratum 0 with a firewall, and instead rely on the Stratum 1 replica servers to provide client access. There usually are multiple Stratum 1 servers in a CernVM-FS network, which are typically distributed across geographic regions. A repository may be replicated to arbitrarily many Stratum 1 servers, but for reasons related to caching efficiency of HTTP proxies, it is best to use only a modest number of Stratum 1 servers (in the 5-10 range), not an excessive amount. While it depends on the specific context and circumstances under consideration, a reasonable rule of thumb would be approximately one Stratum 1 per continent for a deployment that is global in scope, and one Stratum 1 per geographic region of a country for a deployment that is national in scope. Stratum 1 servers enable clients to determine which Stratum 1 is geographically closest to connect to, via the Geo API which uses a GeoIP database that translates IP addresses of clients to an estimated longitude and latitude. Setting up a Stratum 1 replica server will be covered in the second hands-on part of this tutorial , and is also covered in detail in the CernVM-FS documentation . 1.2.4 Squid proxies \u00b6 To further extend the scalability and hierarchical caching model of CernVM-FS, another layer is used between end clients and Stratum 1 servers: forward caching HTTP proxies which reduce load on Stratum 1 servers and help reduce latency for clients. Squid cache is commonly used for this. A Squid proxy caches content that has been accessed recently, and helps to reduce bandwidth and improve response times. It is particularly important to have caching proxies at large systems like HPC clusters where many worker nodes are accessing the CernVM-FS repository, and it is recommended to set up multiple Squid proxies for redundancy and capacity. The second hands-on part of this tutorial will also cover setting up a Squid proxy. More details are available in the CernVM-FS documentation .","title":"1. Introduction to CernVM-FS"},{"location":"01_introduction/#1-introduction-to-cernvm-fs","text":"","title":"1. Introduction to CernVM-FS"},{"location":"01_introduction/#10-introductory-talk-by-jakob-blomer","text":"The best possible introduction to CernVM-FS is the talk by Jakob Blomer (CERN, lead developer) at the 6th EasyBuild User Meeting. The slides for this talk are available here . The recording of this presentation is available on YouTube:","title":"1.0 Introductory talk by Jakob Blomer"},{"location":"01_introduction/#11-what-is-cernvm-fs","text":"Let's get started with explaining in detail what CernVM-FS is... CernVM-FS in a nutshell The CernVM File System (CernVM-FS) provides a scalable and reliable software distribution service, which is implemented as a read-only POSIX filesystem in user space (a FUSE module). Files and directories are hosted on standard web servers and mounted in the universal namespace /cvmfs . That's a mouthful, so let's break it down a bit...","title":"1.1 What is CernVM-FS?"},{"location":"01_introduction/#111-read-only-filesystem-over-http","text":"CernVM-FS is a network filesystem , which you can mount in Linux or macOS via FUSE (Filesystem in Userspace) and on Windows in a WSL2 virtualized Linux environment. In some ways it is similar to other network filesystems like NFS or AFS , but there are various aspects to it that are quite different. The files and directories that are made available via CernVM-FS are always located in a subdirectory of /cvmfs , and are provisioned via a network of servers that can basically be viewed as web servers since only outgoing HTTP connections are used. This makes it easy to use CernVM-FS in environments that are protected by a strict firewall. CernVM-FS is a read-only filesystem for those who access it; only those who administer it are able to add or change its contents. Internally, CernVM-FS uses content-addressable storage and Merkle trees in order to maintain file data and meta-data, but the filesystem it exposes is a standard POSIX filesystem .","title":"1.1.1 Read-only filesystem over HTTP"},{"location":"01_introduction/#112-software-distribution-system","text":"The primary use case of CernVM-FS is to easily distribute software around the world, which is reflected in various ways in the features implemented by CernVM-FS. It's worth highlighting that with software we actually mean software installations , that is the files that collectively form a usable instance of an application, tool, or library. This is in contrast with software packages (for example, RPMs), which are essentially bundles of files wrapped together for easy distribution, and which need to be installed in order to provide a working instance of the provided software. Software installations have specific characteristics, such as often involving lots of small files which are being opened and read as a whole regularly, frequent searching for files in multiple directories, hierarchical structuring, etc. CernVM-FS is heavily tuned to cater to this use case, with aggressive caching and reduction of latency, for example via automatic file de-duplication and compression.","title":"1.1.2 Software distribution system"},{"location":"01_introduction/#113-scalable-and-reliable","text":"CernVM-FS was designed to be scalable and reliable , with known deployments involving hundreds of millions of files and many tens of thousands of clients. It was originally created to fulfill the software distribution needs of the experiments at the Large Hadron Collider (LHC) at CERN. The network of (web) servers that make a CernVM-FS instance accessible is constructed such that it is robust against problems like network disconnects and hardware failures, and so it can be extended and tweaked on demand for optimal performance. More details are available in the CernVM-FS documentation .","title":"1.1.3 Scalable and reliable"},{"location":"01_introduction/#12-terminology","text":"Before we get our hands dirty, let's cover some of the terminology used by the CernVM-FS project. The figure included below shows the different components of the CernVM-FS network: the central Stratum 0 server which hosts the filesystem; the Stratum 1 replica servers, and the associated proxies ; the client accessing the filesystem provided via CernVM-FS.","title":"1.2 Terminology"},{"location":"01_introduction/#121-clients","text":"A client in the context of CernVM-FS is any system that mounts the filesystem. This includes laptops or personal workstations who need access to the provided software installations, but also High-Performance Computing (HPC) clusters, virtual machines running in a cloud environment, etc. Clients only have read-only access to the files included in a CernVM-FS repository, and are automatically notified and updated when the contents of the filesystem have changed. The filesystem that is mounted on a client (under /cvmfs ) is a virtual filesystem, in the sense that data is only (down)loaded when it is actually accessed (and cached aggressively to ensure good performance). Mounting a CernVM-FS repository on a client will be covered in the first hands-on part of this tutorial . Extensive documentation on configuring a client is available in the CernVM-FS documentation .","title":"1.2.1 Clients"},{"location":"01_introduction/#122-stratum-0-repository","text":"A CernVM-FS repository is an instance of a CernVM-FS filesystem. A repository is hosted on one Stratum 0 server, which is the single authoritative source of content for the repository. Multiple repositories can be hosted on the same Stratum 0 server. The data in a repository is stored using a content-addressable storage (CAS) scheme. All files written to a CernVM-FS repository must be converted into data chunks in the CAS store during the process of publishing , which involves creating catalogs which represent directory structure and metadata, and splitting files into chunks, compressing them, calculating content hashes, etc. Publishing is done on a dedicated release manager machine or publisher system which interfaces with the Stratum 0 server. Read-write access to a CernVM-FS repository is only available on a Stratum 0 server or publisher (the publisher and Stratum 0 can be the same system). Write access is provided via a union filesystem , which overlays a writable scratch area and the read-only mount of the CernVM-FS repository. Publishing is an atomic operation: adding or changing files in a repository is done in a transaction that records and collectively commits a set of file system changes, preventing partial or incomplete updates of the repository and ensuring that changes are either applied completely, or not at all. In the first hands-on part of this tutorial we will guide you through the process of creating a CernVM-FS repository, which is also covered in detail in the CernVM-FS documentation . The 3rd hands-on part of this tutorial will focus on the publishing procedure to update the contents of a CernVM-FS repository.","title":"1.2.2 Stratum 0 + repository"},{"location":"01_introduction/#123-stratum-1-replica-servers","text":"A Stratum 1 replica server is a standard web server that provides a read-only mirror of a CernVM-FS repository served by a Stratum 0. The main purpose of a Stratum 1 is to improve reliability and capacity of the CernVM-FS network, by distributing load across multiple servers and allowing clients to fail over if one is unavailable, and to relieve the Stratum 0 from serving client requests. Although clients can access a CernVM-FS repository via the Stratum 0, it is advisable to block external client access to the Stratum 0 with a firewall, and instead rely on the Stratum 1 replica servers to provide client access. There usually are multiple Stratum 1 servers in a CernVM-FS network, which are typically distributed across geographic regions. A repository may be replicated to arbitrarily many Stratum 1 servers, but for reasons related to caching efficiency of HTTP proxies, it is best to use only a modest number of Stratum 1 servers (in the 5-10 range), not an excessive amount. While it depends on the specific context and circumstances under consideration, a reasonable rule of thumb would be approximately one Stratum 1 per continent for a deployment that is global in scope, and one Stratum 1 per geographic region of a country for a deployment that is national in scope. Stratum 1 servers enable clients to determine which Stratum 1 is geographically closest to connect to, via the Geo API which uses a GeoIP database that translates IP addresses of clients to an estimated longitude and latitude. Setting up a Stratum 1 replica server will be covered in the second hands-on part of this tutorial , and is also covered in detail in the CernVM-FS documentation .","title":"1.2.3 Stratum 1 replica servers"},{"location":"01_introduction/#124-squid-proxies","text":"To further extend the scalability and hierarchical caching model of CernVM-FS, another layer is used between end clients and Stratum 1 servers: forward caching HTTP proxies which reduce load on Stratum 1 servers and help reduce latency for clients. Squid cache is commonly used for this. A Squid proxy caches content that has been accessed recently, and helps to reduce bandwidth and improve response times. It is particularly important to have caching proxies at large systems like HPC clusters where many worker nodes are accessing the CernVM-FS repository, and it is recommended to set up multiple Squid proxies for redundancy and capacity. The second hands-on part of this tutorial will also cover setting up a Squid proxy. More details are available in the CernVM-FS documentation .","title":"1.2.4 Squid proxies"},{"location":"02_stratum0_client/","text":"2. Stratum 0 and client \u00b6 In order to get started with CernVM-FS, the first thing you need is a Stratum 0 server. The Stratum 0 is the central server that hosts your repositories and makes it available to other systems. There can be only one Stratum 0 server for each CernVM-FS repository , and from a security perspective it is recommended to restrict the access to this system. We will look more into that later. For now, we are going to set up a Stratum 0, create a repository, and access it by connecting from a client machine directly to the Stratum 0 server. Warning Directly connecting to the Stratum 0 is not recommended , but it is a good way to get started. In the next hands-on part of this tutorial we will remedy this by setting up a Stratum 1 replica server and squid proxy, and accessing the repository that way instead. 2.1 Setting up the Stratum 0 \u00b6 2.1.1 Requirements \u00b6 Resources \u00b6 Due to the scalable design of CernVM-FS, the Stratum 0 server does not need a lot of resources in terms of CPU cores and memory; just a few cores and a couple gigabytes of memory is sufficient. Besides this, you do need sufficient storage space to store the contents of your repository. CernVM-FS uses /var/spool/cvmfs as scratch space while adding new files to the repository, and /srv/cvmfs as central repository storage location. To change these locations, you can create either of the paths as a symbolic link to a different directory. Operating system \u00b6 Several (popular) Linux distributions are supported by CernVM-FS, see the Getting Started page of the CernVM-FS documentation for a full list. In this tutorial we will use CentOS 7 on x86_64 (with some pointers for CentOS 8), but it should be relatively straightforward to use another OS or CPU architecture instead. CernVM-FS supports for hosting the repository contents in S3 compatible storage, but for this tutorial we will focus on storing the files locally on the Stratum 0 server. For this we need an Apache (web)server on the host, and port 80 must be open. 2.1.2 Installing CernVM-FS \u00b6 Installing CernVM-FS is simple and only requires some packages to be installed. You can easily do this by adding the CernVM-FS repository and install the packages through your package manager: sudo yum install -y epel-release # not needed on CentOS 8 sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs cvmfs-server Alternatively, you can download the packages from the CernVM-FS downloads page and install them package manually. Note that you need both the client and server package installed on the Stratum 0 server. 2.1.3 Starting Apache webserver \u00b6 Since the Stratum 0 is serving contents via HTTP, the Apache service needs to be running before we can make a repository. The Apache ( httpd ) package should have been installed already, as it is a dependency of the cvmfs-server package, so the service can now be enabled (so that it always starts after a reboot) and started using: sudo systemctl enable httpd sudo systemctl start httpd 2.1.4 Creating the repository \u00b6 Now that all required packages have been installed, we can create a CernVM-FS repository. In the simplest way, this can be done by running the following command, which will make the current user the owner of the repository: sudo cvmfs_server mkfs -o $USER repo.organization.tld The full repository name (here repo.organization.tld ) resembles a DNS name, but is not. The organization.tld domain does not actually need to exist in DNS. It is recommended to give all the repositories belonging to the same project or organization the same .organization.tld domain. This makes the client configuration much easier, also in case new repositories will be added later on. Warning Please come up with a proper name for your repository, including a domain you will remember throughout this tutorial. Do not use repo.organization.tld . For example, use repo.<your_first_name>.org , where you replace <your_first_name> with (you guessed it) your first name. Feel free to use something else than your first name, of course. 2.1.5 Repository keys \u00b6 For each repository that you create, a set of keys will be generated in /etc/cvmfs/keys : *.crt : the repository\u2019s public key (encoded as an X509 certificate); *.key : the repository's private key; *.masterkey : the repository's private master key; *.pub : repository\u2019s public master key (RSA). The public master key ( repo.organization.tld.pub ) is the one that is needed by clients in order to access the repository, so we will need this later on. The private master key ( repo.organization.tld.masterkey ) is used to sign a whitelist of known publisher certificates, and should not be shared with others. This whitelist is (by default) valid for 30 days, so the signing has to be done regularly (for example via a cron job). Although you can use a different master key per repository, it is recommended to use the same master key for all repositories under a single domain, so that clients only need a single public master key to access all repositories under this domain. For more information, see the CernVM-FS documentation: https://cvmfs.readthedocs.io/en/stable/cpt-repo.html#master-keys . 2.1.6 Adding files to the repository \u00b6 A new repository automatically gets a file named new_repository in its root ( /cvmfs/repo.organization.tld ). You can add more files by starting and publishing a transaction, which will be explained in more detail in the 3rd hands-on part of this tutorial . For now it is enough to just run the following commands to add a simple hello.sh script to your repository. First, set an environment variable for convenience: # Change this to your repository/domain name! MY_REPO_NAME = repo.organization.tld Then start the transaction: cvmfs_server transaction ${ MY_REPO_NAME } Next, add the file to the repository (in /cvmfs/${MY_REPO_NAME} ). If you made the current user the owner of the repository, you can do this without sudo since you have write permissions to your repository: echo '#!/bin/bash' > /cvmfs/ ${ MY_REPO_NAME } /hello.sh echo 'echo hello' >> /cvmfs/ ${ MY_REPO_NAME } /hello.sh chmod a+x /cvmfs/ ${ MY_REPO_NAME } /hello.sh Complete the transaction by publishing the changes using: cvmfs_server publish ${ MY_REPO_NAME } 2.1.7 Cron job for resigning the whitelist \u00b6 Each CernVM-FS repository has a whitelist containing fingerprints of certificates that are allowed to sign the repository. This whitelist has an expiration time of (by default) 30 days, so you regularly have to resign the whitelist. There are several ways to do this, see for instance the page about master keys in the CernVM-FS documentation. If you keep the master key on the Stratum 0 server, you can set up a simple cronjob for resigning the whitelist. For instance, make a file /etc/cron.d/cvmfs_resign with the following content to do this every Monday at 11:00: 0 11 * * 1 root /usr/bin/cvmfs_server resign repo.organization.tld For the sake of this tutorial this is not really necessary of course, but it's an important aspect to be aware of. 2.1.8 Removing a repository \u00b6 An existing repository can be removed easily by running: sudo cvmfs_server rmfs repo.organization.tld Obviously you should only do this when you actually want to get rid of the repository... 2.2 Setting up a client \u00b6 Accessing a CernVM-FS repository on a client system involves three steps: installing the CernVM-FS client package; adding some configuration files for the repository you want to connect to; running the CernVM-FS setup procedure that will mount the repository. The client is going to pull in files from the repository over an HTTP connection. CernVM-FS maintains a local cache on the client, so you need sufficient space for storing it. You can define the maximum size of your cache in the client configuration. The larger your cache is, the less often you have to pull in files again, and the faster your applications will start. Typical client cache sizes range from 4GB to 50GB. Note that you can add more cache layers by adding a proxy nearby your client, which will be covered in the 2nd hands-on part of this tutorial . Note Make sure you use a different system (or virtual machine) for the client! It doesn't make much sense to install both the Stratum 0 server and the CernVM-FS client configuration on the same system... 2.2.1 Installing the client package \u00b6 The installation is the same as for the Stratum 0, except that you only need the cvmfs package (we don't need to CernVM-FS server component on the client): sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs 2.2.2 Configuring the client \u00b6 Most organizations hosting CernVM-FS repositories offer a client package that you can install to do most of the configuration. For the sake of this tutorial, we are going to do this manually for our repository. All required configuration files will have to be stored under /etc/cvmfs . We will discuss them one by one, where we use repo.organization.tld as repository name, and hence organization.tld as domain. Don't forget to rename or change these files according to the repository name and domain you are using! Public key of repository \u00b6 /etc/cvmfs/keys/organization.tld/repo.organization.tld.pub This file contains the public key of the repository you want to access. You can copy this file from your Stratum 0 server, where it is stored under /etc/cvmfs/keys/ . Note that verification of the integrity of the repository content relies on securely distributing this public key to client nodes, so in production you should take care to e.g. confirm the fingerprint or checksum of this key. Main repository configuration \u00b6 /etc/cvmfs/config.d/repo.organization.tld.conf This file contains the main configuration for the repository you want to access, which should minimally contain the URL(s) of the Stratum 1 servers and the location of the keys. We do not have a Stratum 1 server yet (we will set that up in the next hands-on part of this tutorial ), so we are going to connect directly to our Stratum 0 server instead. You should not do this in production! A typical, minimal configuration should look as follows: CVMFS_SERVER_URL=\"http://<STRATUM0_IP>/cvmfs/@fqrn@\" CVMFS_KEYS_DIR=\"/etc/cvmfs/keys/organization.tld\" Replace the <STRATUM0_IP> part with the IP address of your Stratum 0 server! Note that the CVMFS_SERVER_URL should include the part /cvmfs/@fqrn@ exactly like that; the last part ( @fqrn@ ) substitutes in the fully-qualified repository name when a repository is mounted. Local client configuration \u00b6 /etc/cvmfs/default.local This file can be used for setting or overriding client configuration settings that are specific to your system. The CVMFS_HTTP_PROXY parameter is required : it should point to your local proxy that serves as a cache between your client(s) and the Stratum 1 server(s). Since we do not have a proxy yet, we are setting this to DIRECT , meaning that we connect directly to the Stratum 1 server (actually, Stratum 0 at this point): CVMFS_HTTP_PROXY=DIRECT You can also use this file to specify a maximum size (in megabytes) for the cache. For example, to use a local cache of maximum 5GB: CVMFS_QUOTA_LIMIT=5000 2.2.3 Mounting the repositories \u00b6 When your client configuration is complete, you can try to set up the client: sudo cvmfs_config setup This should not return any output or error messages. You can then run the following command to checks for common misconfigurations: sudo cvmfs_config chksetup This command should return OK . If you do run into a problem, check out the debugging section on the Advanced topics page . 2.2.4 Inspecting the repository \u00b6 Finally, we can try to access our repository on the client system. Note that CernVM-FS uses autofs , which means that you may not see the repository when you just run \" ls /cvmfs \". Your repository will only be actually mounted when you access it, and may be unmounted after not using it for a while. So, the following should work and show the contents of your repository: ls /cvmfs/repo.organization.tld Exercise \u00b6 Time to get your hands dirty! Try this yourself: Set up your own CernVM-FS repository on Stratum 0 server in a virtual machine. Create a repository with a suitable name (for example, exercise.<your_first_name>.org ). Add a simple bash script to the repository, which, for instance, prints Hello world! . Install and configure the CernVM-FS client on another virtual machine. Access your repository directly via Stratum 0 for now. Try to access your repository, and run your bash script on the client.","title":"2. Stratum 0 and client"},{"location":"02_stratum0_client/#2-stratum-0-and-client","text":"In order to get started with CernVM-FS, the first thing you need is a Stratum 0 server. The Stratum 0 is the central server that hosts your repositories and makes it available to other systems. There can be only one Stratum 0 server for each CernVM-FS repository , and from a security perspective it is recommended to restrict the access to this system. We will look more into that later. For now, we are going to set up a Stratum 0, create a repository, and access it by connecting from a client machine directly to the Stratum 0 server. Warning Directly connecting to the Stratum 0 is not recommended , but it is a good way to get started. In the next hands-on part of this tutorial we will remedy this by setting up a Stratum 1 replica server and squid proxy, and accessing the repository that way instead.","title":"2. Stratum 0 and client"},{"location":"02_stratum0_client/#21-setting-up-the-stratum-0","text":"","title":"2.1 Setting up the Stratum 0"},{"location":"02_stratum0_client/#211-requirements","text":"","title":"2.1.1 Requirements"},{"location":"02_stratum0_client/#resources","text":"Due to the scalable design of CernVM-FS, the Stratum 0 server does not need a lot of resources in terms of CPU cores and memory; just a few cores and a couple gigabytes of memory is sufficient. Besides this, you do need sufficient storage space to store the contents of your repository. CernVM-FS uses /var/spool/cvmfs as scratch space while adding new files to the repository, and /srv/cvmfs as central repository storage location. To change these locations, you can create either of the paths as a symbolic link to a different directory.","title":"Resources"},{"location":"02_stratum0_client/#operating-system","text":"Several (popular) Linux distributions are supported by CernVM-FS, see the Getting Started page of the CernVM-FS documentation for a full list. In this tutorial we will use CentOS 7 on x86_64 (with some pointers for CentOS 8), but it should be relatively straightforward to use another OS or CPU architecture instead. CernVM-FS supports for hosting the repository contents in S3 compatible storage, but for this tutorial we will focus on storing the files locally on the Stratum 0 server. For this we need an Apache (web)server on the host, and port 80 must be open.","title":"Operating system"},{"location":"02_stratum0_client/#212-installing-cernvm-fs","text":"Installing CernVM-FS is simple and only requires some packages to be installed. You can easily do this by adding the CernVM-FS repository and install the packages through your package manager: sudo yum install -y epel-release # not needed on CentOS 8 sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs cvmfs-server Alternatively, you can download the packages from the CernVM-FS downloads page and install them package manually. Note that you need both the client and server package installed on the Stratum 0 server.","title":"2.1.2 Installing CernVM-FS"},{"location":"02_stratum0_client/#213-starting-apache-webserver","text":"Since the Stratum 0 is serving contents via HTTP, the Apache service needs to be running before we can make a repository. The Apache ( httpd ) package should have been installed already, as it is a dependency of the cvmfs-server package, so the service can now be enabled (so that it always starts after a reboot) and started using: sudo systemctl enable httpd sudo systemctl start httpd","title":"2.1.3 Starting Apache webserver"},{"location":"02_stratum0_client/#214-creating-the-repository","text":"Now that all required packages have been installed, we can create a CernVM-FS repository. In the simplest way, this can be done by running the following command, which will make the current user the owner of the repository: sudo cvmfs_server mkfs -o $USER repo.organization.tld The full repository name (here repo.organization.tld ) resembles a DNS name, but is not. The organization.tld domain does not actually need to exist in DNS. It is recommended to give all the repositories belonging to the same project or organization the same .organization.tld domain. This makes the client configuration much easier, also in case new repositories will be added later on. Warning Please come up with a proper name for your repository, including a domain you will remember throughout this tutorial. Do not use repo.organization.tld . For example, use repo.<your_first_name>.org , where you replace <your_first_name> with (you guessed it) your first name. Feel free to use something else than your first name, of course.","title":"2.1.4 Creating the repository"},{"location":"02_stratum0_client/#215-repository-keys","text":"For each repository that you create, a set of keys will be generated in /etc/cvmfs/keys : *.crt : the repository\u2019s public key (encoded as an X509 certificate); *.key : the repository's private key; *.masterkey : the repository's private master key; *.pub : repository\u2019s public master key (RSA). The public master key ( repo.organization.tld.pub ) is the one that is needed by clients in order to access the repository, so we will need this later on. The private master key ( repo.organization.tld.masterkey ) is used to sign a whitelist of known publisher certificates, and should not be shared with others. This whitelist is (by default) valid for 30 days, so the signing has to be done regularly (for example via a cron job). Although you can use a different master key per repository, it is recommended to use the same master key for all repositories under a single domain, so that clients only need a single public master key to access all repositories under this domain. For more information, see the CernVM-FS documentation: https://cvmfs.readthedocs.io/en/stable/cpt-repo.html#master-keys .","title":"2.1.5 Repository keys"},{"location":"02_stratum0_client/#216-adding-files-to-the-repository","text":"A new repository automatically gets a file named new_repository in its root ( /cvmfs/repo.organization.tld ). You can add more files by starting and publishing a transaction, which will be explained in more detail in the 3rd hands-on part of this tutorial . For now it is enough to just run the following commands to add a simple hello.sh script to your repository. First, set an environment variable for convenience: # Change this to your repository/domain name! MY_REPO_NAME = repo.organization.tld Then start the transaction: cvmfs_server transaction ${ MY_REPO_NAME } Next, add the file to the repository (in /cvmfs/${MY_REPO_NAME} ). If you made the current user the owner of the repository, you can do this without sudo since you have write permissions to your repository: echo '#!/bin/bash' > /cvmfs/ ${ MY_REPO_NAME } /hello.sh echo 'echo hello' >> /cvmfs/ ${ MY_REPO_NAME } /hello.sh chmod a+x /cvmfs/ ${ MY_REPO_NAME } /hello.sh Complete the transaction by publishing the changes using: cvmfs_server publish ${ MY_REPO_NAME }","title":"2.1.6 Adding files to the repository"},{"location":"02_stratum0_client/#217-cron-job-for-resigning-the-whitelist","text":"Each CernVM-FS repository has a whitelist containing fingerprints of certificates that are allowed to sign the repository. This whitelist has an expiration time of (by default) 30 days, so you regularly have to resign the whitelist. There are several ways to do this, see for instance the page about master keys in the CernVM-FS documentation. If you keep the master key on the Stratum 0 server, you can set up a simple cronjob for resigning the whitelist. For instance, make a file /etc/cron.d/cvmfs_resign with the following content to do this every Monday at 11:00: 0 11 * * 1 root /usr/bin/cvmfs_server resign repo.organization.tld For the sake of this tutorial this is not really necessary of course, but it's an important aspect to be aware of.","title":"2.1.7 Cron job for resigning the whitelist"},{"location":"02_stratum0_client/#218-removing-a-repository","text":"An existing repository can be removed easily by running: sudo cvmfs_server rmfs repo.organization.tld Obviously you should only do this when you actually want to get rid of the repository...","title":"2.1.8 Removing a repository"},{"location":"02_stratum0_client/#22-setting-up-a-client","text":"Accessing a CernVM-FS repository on a client system involves three steps: installing the CernVM-FS client package; adding some configuration files for the repository you want to connect to; running the CernVM-FS setup procedure that will mount the repository. The client is going to pull in files from the repository over an HTTP connection. CernVM-FS maintains a local cache on the client, so you need sufficient space for storing it. You can define the maximum size of your cache in the client configuration. The larger your cache is, the less often you have to pull in files again, and the faster your applications will start. Typical client cache sizes range from 4GB to 50GB. Note that you can add more cache layers by adding a proxy nearby your client, which will be covered in the 2nd hands-on part of this tutorial . Note Make sure you use a different system (or virtual machine) for the client! It doesn't make much sense to install both the Stratum 0 server and the CernVM-FS client configuration on the same system...","title":"2.2 Setting up a client"},{"location":"02_stratum0_client/#221-installing-the-client-package","text":"The installation is the same as for the Stratum 0, except that you only need the cvmfs package (we don't need to CernVM-FS server component on the client): sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs","title":"2.2.1 Installing the client package"},{"location":"02_stratum0_client/#222-configuring-the-client","text":"Most organizations hosting CernVM-FS repositories offer a client package that you can install to do most of the configuration. For the sake of this tutorial, we are going to do this manually for our repository. All required configuration files will have to be stored under /etc/cvmfs . We will discuss them one by one, where we use repo.organization.tld as repository name, and hence organization.tld as domain. Don't forget to rename or change these files according to the repository name and domain you are using!","title":"2.2.2 Configuring the client"},{"location":"02_stratum0_client/#public-key-of-repository","text":"/etc/cvmfs/keys/organization.tld/repo.organization.tld.pub This file contains the public key of the repository you want to access. You can copy this file from your Stratum 0 server, where it is stored under /etc/cvmfs/keys/ . Note that verification of the integrity of the repository content relies on securely distributing this public key to client nodes, so in production you should take care to e.g. confirm the fingerprint or checksum of this key.","title":"Public key of repository"},{"location":"02_stratum0_client/#main-repository-configuration","text":"/etc/cvmfs/config.d/repo.organization.tld.conf This file contains the main configuration for the repository you want to access, which should minimally contain the URL(s) of the Stratum 1 servers and the location of the keys. We do not have a Stratum 1 server yet (we will set that up in the next hands-on part of this tutorial ), so we are going to connect directly to our Stratum 0 server instead. You should not do this in production! A typical, minimal configuration should look as follows: CVMFS_SERVER_URL=\"http://<STRATUM0_IP>/cvmfs/@fqrn@\" CVMFS_KEYS_DIR=\"/etc/cvmfs/keys/organization.tld\" Replace the <STRATUM0_IP> part with the IP address of your Stratum 0 server! Note that the CVMFS_SERVER_URL should include the part /cvmfs/@fqrn@ exactly like that; the last part ( @fqrn@ ) substitutes in the fully-qualified repository name when a repository is mounted.","title":"Main repository configuration"},{"location":"02_stratum0_client/#local-client-configuration","text":"/etc/cvmfs/default.local This file can be used for setting or overriding client configuration settings that are specific to your system. The CVMFS_HTTP_PROXY parameter is required : it should point to your local proxy that serves as a cache between your client(s) and the Stratum 1 server(s). Since we do not have a proxy yet, we are setting this to DIRECT , meaning that we connect directly to the Stratum 1 server (actually, Stratum 0 at this point): CVMFS_HTTP_PROXY=DIRECT You can also use this file to specify a maximum size (in megabytes) for the cache. For example, to use a local cache of maximum 5GB: CVMFS_QUOTA_LIMIT=5000","title":"Local client configuration"},{"location":"02_stratum0_client/#223-mounting-the-repositories","text":"When your client configuration is complete, you can try to set up the client: sudo cvmfs_config setup This should not return any output or error messages. You can then run the following command to checks for common misconfigurations: sudo cvmfs_config chksetup This command should return OK . If you do run into a problem, check out the debugging section on the Advanced topics page .","title":"2.2.3 Mounting the repositories"},{"location":"02_stratum0_client/#224-inspecting-the-repository","text":"Finally, we can try to access our repository on the client system. Note that CernVM-FS uses autofs , which means that you may not see the repository when you just run \" ls /cvmfs \". Your repository will only be actually mounted when you access it, and may be unmounted after not using it for a while. So, the following should work and show the contents of your repository: ls /cvmfs/repo.organization.tld","title":"2.2.4 Inspecting the repository"},{"location":"02_stratum0_client/#exercise","text":"Time to get your hands dirty! Try this yourself: Set up your own CernVM-FS repository on Stratum 0 server in a virtual machine. Create a repository with a suitable name (for example, exercise.<your_first_name>.org ). Add a simple bash script to the repository, which, for instance, prints Hello world! . Install and configure the CernVM-FS client on another virtual machine. Access your repository directly via Stratum 0 for now. Try to access your repository, and run your bash script on the client.","title":"Exercise"},{"location":"03_stratum1_proxies/","text":"3. Stratum 1 and proxies \u00b6 In the previous section we have set up a Stratum 0 server and a client that directly connects to the Stratum 0. Although this worked fine this is not recommended , since this setup is neither scalable, nor very reliable, nor secure: it is a single point of failure, too open in terms of connectivity, and it will have to serve all clients on its own. Therefore, we will show how all these points can be addressed by adding a Stratum 1 server and a caching proxy server . Quick reminder: a Stratum 1 is a replica server that keeps mirrors of the repositories served by a Stratum 0. It is a web server that periodically synchronizes the contents of the repositories. In contrast to the central Stratum 0 server you can have multiple Stratum 1 servers, and it is recommended to have them geographically distributed, so that clients always have a nearby Stratum 1. How many Stratum 1 servers you need mostly depends on the number of clients and how they are distributed geographically, but often a few is already sufficient. Scalability and performance can be improved with proxies, which we will discuss later in this section. 3.1 Setting up the Stratum 1 server \u00b6 3.1.1 Requirements \u00b6 A Stratum 1 server has similar requirements as a Stratum 0 in terms of resources . In addition to port 80 (for the Apache web server), port 8000 also has to be accessible for a Stratum 1 (for the Squid proxy frontend). Furthermore, you need a (free) license key for Maxmind's Geo API , which you can obtain by signing up for an account . This is used by CernVM-FS to allow clients to determine which Stratum 1 server is geographically located closest. 3.1.2 Installation \u00b6 For the Stratum 1 you need to install the following packages: sudo yum install -y epel-release # not needed on CentOS 8 sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs-server squid sudo yum install -y mod_wsgi # on CentOS 7 # sudo yum install -y python3-mod_wsgi # on CentOS 8 This is: cvmfs-server : the CermVM-FS server package (just like for Stratum 0); mod_wsgi : an Apache module that provides a WSGI compliant interface for hosting Python based web applications within Apache, which is required by CernVM-FS to query the Geo API (more on that later); squid : the Squid proxy package. Warning The Squid package in the official CentOS 7 repository is version 3.5.20, which is deprecated . It is not recommended to use this version in production systems. Squid 4 packages for CentOS 7 can be found in other repositories, see the Binary packages page on the Squid website . 3.1.3 Configuring Apache and Squid proxy \u00b6 On the Stratum 1, we will be running Apache with a Squid frontend (reverse proxy). Squid is mainly used to cache the calls to the Geo API, and it can also be helpful for setting up joint monitoring (for instance, see the monitoring page of the Worldwide LHC Computing Grid project ). We will be setting up a single Squid reverse proxy, but you could even have a load balancer that spreads the load over multiple instances. The Apache web server will be listening internally on port 8080, while the Squid proxy will be set up to listen (externally) on port 80 and 8000. Port 80 is the default HTTP port, and port 8000 is a somewhat de facto standard commonly used for CernVM-FS Stratum 1 servers. Apache configuration \u00b6 First, we modify the Apache webserver configuration, by editing /etc/httpd/conf/httpd.conf and change the default: Listen 80 to: Listen 127.0.0.1:8080 Squid configuration \u00b6 Next, we replace the default contents of /etc/squid/squid.conf with the following: http_port 80 accel http_port 8000 accel http_access allow all cache_peer 127.0.0.1 parent 8080 0 no-query originserver acl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/ cache deny !CVMFSAPI cache_mem 128 MB To clarify: http_port specifies on which ports Squid will listen for HTTP requests; http_access specifies access restrictions for HTTP traffic (none, in this case); cache_peer specifies that the Apache web server is listening on port 8080; acl specifies the access list for CernVM-FS: only paths under /cvmfs/*/api/ are relevant; cache specifies which paths should be cached by the Squid proxy (only paths that match the regular expression on the line above); cache_mem specifies the amount of memory that Squid is allowed to use; For more information, see the Squid documentation: http://www.squid-cache.org/Doc/config/ . Start & enable services \u00b6 Finally, we start and enable both the Apache and Squid services: sudo systemctl start httpd sudo systemctl start squid sudo systemctl enable httpd sudo systemctl enable squid To make sure both the Apache and Squid services are running, you can check with: sudo systemctl status httpd and sudo systemctl status squid Look for active (running) (in green) in the output. 3.1.4 DNS cache \u00b6 As the GeoAPI on a Stratum 1 server does a lot of DNS lookups, it is recommended to have a local DNS caching service on that same system. We will not discuss this topic any further here, but you can use dnsmasq , bind , or systemd-resolved . See for instance this tutorial for setting up systemd-resolved . 3.1.5 Creating the Stratum 1 replica \u00b6 With all the required components in place, we can now really set up our Stratum 1 replica server. Create and add Geo API key (optional) \u00b6 We first add our Geo API key to the CernVM-FS server settings, by creating it and then running these commands: echo 'CVMFS_GEO_LICENSE_KEY=YOUR_KEY' | sudo tee -a /etc/cvmfs/server.local sudo chmod 600 /etc/cvmfs/server.local Replace YOUR_KEY with your Geo API license key; see https://www.maxmind.com/en/accounts/YOUR_ACCOUNT_ID/license-key ! Note that this is not strictly required for the sake of this tutorial, but it's highly recommended. Add repository master public key \u00b6 We also need to have the public master key of each repository we want to mirror to be available on our Stratum 1. This can be done by copying the .pub file(s) from /etc/cvmfs/keys on the Stratum 0 server to /etc/cvmfs/keys/organization.tld/ (note the extra level!) on the Stratum 1 server, just like we did on the client. Create replica \u00b6 Now we make the replica by giving the URL to the repository on the Stratum 0 server (which is always like http://host:port/cvmfs/repository , with the :port part optional in this tutorial since the default HTTP port 80 is used) and the path to the corresponding public master key: sudo cvmfs_server add-replica -o $USER http://<STRATUM0_IP>/cvmfs/repo.organization.tld /etc/cvmfs/keys/organization.tld/ Replace the <STRATUM0_IP> part with the IP address of your Stratum 0 server, and adjust for the name and domain of your CernVM-FS repository! Executing the add-replica command should produce output reporting on the steps being performed, and only take a couple of moments to complete. If no output is produced and the command seems to be hanging, make sure that port 80 on your Stratum 0 server is accessible via the IP address you are using! Bypassing the Geo API license key \u00b6 If you prefer not to create a MaxMind account and Geo API license key for the sake of this tutorial, you can bypass the \" CVMFS_GEO_LICENSE_KEY not set \" error message produced by cvmfs_server add-replica by setting the server variable CVMFS_GEO_DB_FILE to NONE before running the command: # only do this if you do not want to provide a Geo API key (not recommended!) echo 'CVMFS_GEO_DB_FILE=NONE' | sudo tee -a /etc/cvmfs/server.local Remove the replica \u00b6 If you ever want to remove the repository replica again, you can use the rmfs subcommand in the same way as on Stratum 0: sudo cvmfs_server rmfs repo.organization.tld 3.1.6 Manually synchronize the Stratum 1 \u00b6 Now that the Stratum 1 has been registered, we should try to do a first synchronization. You can do this by running the following command: sudo cvmfs_server snapshot repo.organization.tld As there is not much in the repository yet, this should complete within a few seconds. The output should end with something like: Serving revision 2 Fetched 2 new chunks out of 3 processed chunks 3.1.7 Adding a synchronization cron job \u00b6 Whenever you make changes to the repository, the changes have to be synchronized to all Stratum 1 servers. This task can be automated by setting up a cron job that periodically runs cvmfs_server snapshot -a , where -a does the synchronization for all active repositories. This option will give an error if no log rotation has been configured for CernVM-FS, so we first have to create a file /etc/logrotate.d/cvmfs with the following contents: /var/log/cvmfs/*.log { weekly missingok notifempty } Now we can make a cron job /etc/cron.d/cvmfs_stratum1_snapshot for the snapshots: */5 * * * * root output=$(/usr/bin/cvmfs_server snapshot -a -i 2>&1) || echo \"$output\" 3.2 Setting up a proxy \u00b6 If you have a lot of local machines, e.g. an HPC cluster, that need to access your repositories, you also want another cache layer close to these machines. This can be done by adding one or more Squid proxies between your local machine(s) and the Stratum 1 server(s). It is recommended to have at least two proxies, for reliability and load-balancing reasons. 3.2.1 Requirements \u00b6 Just as with the other components, the Squid proxy server does not need a lot of resources. Just a few cores and few gigabytes of memory should be enough, although extra RAM can be used to cache more content. Similarly the more disk space you allocate for this machine, the larger the cache can be, and the better the performance will be. Note that this system will only store a part of the (deduplicated and compressed) repository, so it does not need as much storage space as the Stratum 0 or Stratum 1 server. 3.2.2 Installation \u00b6 On the proxy server only Squid needs to be installed: sudo yum install -y squid 3.2.3 Configuration \u00b6 The configuration of a standalone Squid proxy is slightly different from the one that we used for our Stratum 1. You can use the following template to set up your own Squid configuration for your proxy server (in /etc/squid/squid.conf ): # List of local IP addresses (separate IPs and/or CIDR notation) allowed to access your local proxy acl local_nodes src YOUR_CLIENT_IPS # Destination domains that are allowed #acl stratum_ones dstdomain .YOURDOMAIN.ORG #acl stratum_ones dstdom_regex YOUR_REGEX # Squid port http_port 3128 # Deny access to anything which is not part of our stratum_ones ACL. http_access deny !stratum_ones # Only allow access from our local machines http_access allow local_nodes http_access allow localhost # Finally, deny all other access to this proxy http_access deny all minimum_expiry_time 0 maximum_object_size 1024 MB cache_mem 128 MB maximum_object_size_in_memory 128 KB # 5 GB disk cache cache_dir ufs /var/spool/squid 5000 16 256 In this template, there are two things you must change in the Access Control List (ACL) settings: The line starting with acl local_nodes specifies which clients are allowed to use this proxy. You can use CIDR notation . For the sake of this tutorial, you can just replace YOUR_CLIENT_IPS with the IP of your client system. For example: acl local_nodes src 1.2.3.4 Add a line starting with acl stratum_ones to specify an ACL for the allowed destination domains. For the sake of this tutorial, we can just hardcode this to our Stratum 1 server via dst (destination): acl stratum_ones dst <STRATUM1_IP> (where you need to change <STRATUM1_IP> with the IP address of your Stratum 1 server) The stratum_ones ACL you defined is used to specify that the Squid should only cache the Stratum 1 server, via the first http_access deny line. (Although restricting the destination is not strictly required for functionality, this can be beneficial for security to prevent misuse of the proxy server.) More information about Squid ACLs can be found in the Squid documentation . Finally, there are some settings regarding the size of your cache. Make sure that you have enough disk space for the value that you provide. 3.2.4 Verifying Squid configuration \u00b6 To verify the correctness of your Squid configuration, you can run: sudo squid -k parse When things look OK (no errors or warnings are printed, exit code zero), you can start and enable Squid. 3.2.5 Starting and enabling Squid \u00b6 To start and enable Squid, run: sudo systemctl start squid sudo systemctl enable squid 3.3 Re-configuring the client \u00b6 Now that we have a Stratum 0 server, a Stratum 1 server, and a Squid proxy, we have the (minimal) infrastructure in place for a production-ready CernVM-FS setup. So we can now configure our client properly, and start using the repository. In the previous section we connected our client directly to the Stratum 0. We are going to reuse that configuration, and only need to change two things. 3.3.1 Connect to the Stratum 1 \u00b6 We used the URL of the Stratum 0 in the file /etc/cvmfs/config.d/repo.organization.tld.conf . We should now change this URL, and point to the Stratum 1 instead: CVMFS_SERVER_URL=\"http://<STRATUM1_IP>/cvmfs/@fqrn@\" Replace the <STRATUM1_IP> part with the IP address of your Stratum 1 server! When you have more Stratum 1 servers inside the organization, you can make it a semicolon-separated list of servers. The Geo API will make sure that your client always connects to the geographically closest Stratum 1 server (if CVMFS_USE_GEOAPI=yes ). 3.3.2 Use the Squid proxy \u00b6 In order to use the local cache layer of our proxy, we have to instruct the client to send all requests through the proxy. This needs one small change in /etc/cvmfs/default.local , where you will have to replace DIRECT with IP address of your Squid proxy service, plus the (default) port 3128 at which Squid is running: CVMFS_HTTP_PROXY=\"http://<PROXY_IP>:3128\" Replace the <PROXY_IP> part with the IP address of your Squid proxy server! After changing the default.local configuration file, if the repository is currently mounted, do sudo cvmfs_config reload repo.organization.tld to apply the new configuration. If it is not currently mounted, the updated configuration will be automatically used the next time the repository is mounted (you can do ls /cvmfs/repo.organization.tld to cause autofs to mount it). More proxies can be added to CVMFS_HTTP_PROXY by separating them with a pipe or semicolon symbol. See the CernVM-FS documentation for examples. 3.3.3 Test the new configuration \u00b6 Now you can test your new configuration by checking if you can still access the repository. Do sudo cvmfs_config chksetup to again test the configuration, including access to all configured Stratum 1 servers through all configured proxy servers. Furthermore, to confirm you are really using your Squid proxy (and to see which if there are more than one), you can do: cvmfs_config stat -v repo.organization.tld This should show a line that looks like: Connection: http://<STRATUM1_IP>/cvmfs/repo.organization.tld through proxy http://<PROXY_IP>:3128 (online) Make sure that it lists your proxy here (and not DIRECT ), and that it is marked as online . If you run into any issues, the Debugging section on the Advanced topics page may provide some useful information for finding the cause. Exercise \u00b6 1) Set up a Stratum 1 server. Make sure that it includes: a proper Geo API license key (if you do not want to request an account, you can use the described method to bypass this, but again: do not do this in production!); cron job for automatically synchronizing the repository (e.g. once every 10 minutes); properly configured Apache and Squid services; 2) Set up a separate Squid proxy. Though it is recommended to at least have two in production, one is enough for now. 3) Reconfigure the client that you set up in the previous section and make sure that it uses your Stratum 1 and Squid proxy.","title":"3. Stratum 1 and proxies"},{"location":"03_stratum1_proxies/#3-stratum-1-and-proxies","text":"In the previous section we have set up a Stratum 0 server and a client that directly connects to the Stratum 0. Although this worked fine this is not recommended , since this setup is neither scalable, nor very reliable, nor secure: it is a single point of failure, too open in terms of connectivity, and it will have to serve all clients on its own. Therefore, we will show how all these points can be addressed by adding a Stratum 1 server and a caching proxy server . Quick reminder: a Stratum 1 is a replica server that keeps mirrors of the repositories served by a Stratum 0. It is a web server that periodically synchronizes the contents of the repositories. In contrast to the central Stratum 0 server you can have multiple Stratum 1 servers, and it is recommended to have them geographically distributed, so that clients always have a nearby Stratum 1. How many Stratum 1 servers you need mostly depends on the number of clients and how they are distributed geographically, but often a few is already sufficient. Scalability and performance can be improved with proxies, which we will discuss later in this section.","title":"3. Stratum 1 and proxies"},{"location":"03_stratum1_proxies/#31-setting-up-the-stratum-1-server","text":"","title":"3.1 Setting up the Stratum 1 server"},{"location":"03_stratum1_proxies/#311-requirements","text":"A Stratum 1 server has similar requirements as a Stratum 0 in terms of resources . In addition to port 80 (for the Apache web server), port 8000 also has to be accessible for a Stratum 1 (for the Squid proxy frontend). Furthermore, you need a (free) license key for Maxmind's Geo API , which you can obtain by signing up for an account . This is used by CernVM-FS to allow clients to determine which Stratum 1 server is geographically located closest.","title":"3.1.1 Requirements"},{"location":"03_stratum1_proxies/#312-installation","text":"For the Stratum 1 you need to install the following packages: sudo yum install -y epel-release # not needed on CentOS 8 sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs-server squid sudo yum install -y mod_wsgi # on CentOS 7 # sudo yum install -y python3-mod_wsgi # on CentOS 8 This is: cvmfs-server : the CermVM-FS server package (just like for Stratum 0); mod_wsgi : an Apache module that provides a WSGI compliant interface for hosting Python based web applications within Apache, which is required by CernVM-FS to query the Geo API (more on that later); squid : the Squid proxy package. Warning The Squid package in the official CentOS 7 repository is version 3.5.20, which is deprecated . It is not recommended to use this version in production systems. Squid 4 packages for CentOS 7 can be found in other repositories, see the Binary packages page on the Squid website .","title":"3.1.2 Installation"},{"location":"03_stratum1_proxies/#313-configuring-apache-and-squid-proxy","text":"On the Stratum 1, we will be running Apache with a Squid frontend (reverse proxy). Squid is mainly used to cache the calls to the Geo API, and it can also be helpful for setting up joint monitoring (for instance, see the monitoring page of the Worldwide LHC Computing Grid project ). We will be setting up a single Squid reverse proxy, but you could even have a load balancer that spreads the load over multiple instances. The Apache web server will be listening internally on port 8080, while the Squid proxy will be set up to listen (externally) on port 80 and 8000. Port 80 is the default HTTP port, and port 8000 is a somewhat de facto standard commonly used for CernVM-FS Stratum 1 servers.","title":"3.1.3 Configuring Apache and Squid proxy"},{"location":"03_stratum1_proxies/#apache-configuration","text":"First, we modify the Apache webserver configuration, by editing /etc/httpd/conf/httpd.conf and change the default: Listen 80 to: Listen 127.0.0.1:8080","title":"Apache configuration"},{"location":"03_stratum1_proxies/#squid-configuration","text":"Next, we replace the default contents of /etc/squid/squid.conf with the following: http_port 80 accel http_port 8000 accel http_access allow all cache_peer 127.0.0.1 parent 8080 0 no-query originserver acl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/ cache deny !CVMFSAPI cache_mem 128 MB To clarify: http_port specifies on which ports Squid will listen for HTTP requests; http_access specifies access restrictions for HTTP traffic (none, in this case); cache_peer specifies that the Apache web server is listening on port 8080; acl specifies the access list for CernVM-FS: only paths under /cvmfs/*/api/ are relevant; cache specifies which paths should be cached by the Squid proxy (only paths that match the regular expression on the line above); cache_mem specifies the amount of memory that Squid is allowed to use; For more information, see the Squid documentation: http://www.squid-cache.org/Doc/config/ .","title":"Squid configuration"},{"location":"03_stratum1_proxies/#start-enable-services","text":"Finally, we start and enable both the Apache and Squid services: sudo systemctl start httpd sudo systemctl start squid sudo systemctl enable httpd sudo systemctl enable squid To make sure both the Apache and Squid services are running, you can check with: sudo systemctl status httpd and sudo systemctl status squid Look for active (running) (in green) in the output.","title":"Start &amp; enable services"},{"location":"03_stratum1_proxies/#314-dns-cache","text":"As the GeoAPI on a Stratum 1 server does a lot of DNS lookups, it is recommended to have a local DNS caching service on that same system. We will not discuss this topic any further here, but you can use dnsmasq , bind , or systemd-resolved . See for instance this tutorial for setting up systemd-resolved .","title":"3.1.4 DNS cache"},{"location":"03_stratum1_proxies/#315-creating-the-stratum-1-replica","text":"With all the required components in place, we can now really set up our Stratum 1 replica server.","title":"3.1.5 Creating the Stratum 1 replica"},{"location":"03_stratum1_proxies/#create-and-add-geo-api-key-optional","text":"We first add our Geo API key to the CernVM-FS server settings, by creating it and then running these commands: echo 'CVMFS_GEO_LICENSE_KEY=YOUR_KEY' | sudo tee -a /etc/cvmfs/server.local sudo chmod 600 /etc/cvmfs/server.local Replace YOUR_KEY with your Geo API license key; see https://www.maxmind.com/en/accounts/YOUR_ACCOUNT_ID/license-key ! Note that this is not strictly required for the sake of this tutorial, but it's highly recommended.","title":"Create and add Geo API key (optional)"},{"location":"03_stratum1_proxies/#add-repository-master-public-key","text":"We also need to have the public master key of each repository we want to mirror to be available on our Stratum 1. This can be done by copying the .pub file(s) from /etc/cvmfs/keys on the Stratum 0 server to /etc/cvmfs/keys/organization.tld/ (note the extra level!) on the Stratum 1 server, just like we did on the client.","title":"Add repository master public key"},{"location":"03_stratum1_proxies/#create-replica","text":"Now we make the replica by giving the URL to the repository on the Stratum 0 server (which is always like http://host:port/cvmfs/repository , with the :port part optional in this tutorial since the default HTTP port 80 is used) and the path to the corresponding public master key: sudo cvmfs_server add-replica -o $USER http://<STRATUM0_IP>/cvmfs/repo.organization.tld /etc/cvmfs/keys/organization.tld/ Replace the <STRATUM0_IP> part with the IP address of your Stratum 0 server, and adjust for the name and domain of your CernVM-FS repository! Executing the add-replica command should produce output reporting on the steps being performed, and only take a couple of moments to complete. If no output is produced and the command seems to be hanging, make sure that port 80 on your Stratum 0 server is accessible via the IP address you are using!","title":"Create replica"},{"location":"03_stratum1_proxies/#bypassing-the-geo-api-license-key","text":"If you prefer not to create a MaxMind account and Geo API license key for the sake of this tutorial, you can bypass the \" CVMFS_GEO_LICENSE_KEY not set \" error message produced by cvmfs_server add-replica by setting the server variable CVMFS_GEO_DB_FILE to NONE before running the command: # only do this if you do not want to provide a Geo API key (not recommended!) echo 'CVMFS_GEO_DB_FILE=NONE' | sudo tee -a /etc/cvmfs/server.local","title":"Bypassing the Geo API license key"},{"location":"03_stratum1_proxies/#remove-the-replica","text":"If you ever want to remove the repository replica again, you can use the rmfs subcommand in the same way as on Stratum 0: sudo cvmfs_server rmfs repo.organization.tld","title":"Remove the replica"},{"location":"03_stratum1_proxies/#316-manually-synchronize-the-stratum-1","text":"Now that the Stratum 1 has been registered, we should try to do a first synchronization. You can do this by running the following command: sudo cvmfs_server snapshot repo.organization.tld As there is not much in the repository yet, this should complete within a few seconds. The output should end with something like: Serving revision 2 Fetched 2 new chunks out of 3 processed chunks","title":"3.1.6 Manually synchronize the Stratum 1"},{"location":"03_stratum1_proxies/#317-adding-a-synchronization-cron-job","text":"Whenever you make changes to the repository, the changes have to be synchronized to all Stratum 1 servers. This task can be automated by setting up a cron job that periodically runs cvmfs_server snapshot -a , where -a does the synchronization for all active repositories. This option will give an error if no log rotation has been configured for CernVM-FS, so we first have to create a file /etc/logrotate.d/cvmfs with the following contents: /var/log/cvmfs/*.log { weekly missingok notifempty } Now we can make a cron job /etc/cron.d/cvmfs_stratum1_snapshot for the snapshots: */5 * * * * root output=$(/usr/bin/cvmfs_server snapshot -a -i 2>&1) || echo \"$output\"","title":"3.1.7 Adding a synchronization cron job"},{"location":"03_stratum1_proxies/#32-setting-up-a-proxy","text":"If you have a lot of local machines, e.g. an HPC cluster, that need to access your repositories, you also want another cache layer close to these machines. This can be done by adding one or more Squid proxies between your local machine(s) and the Stratum 1 server(s). It is recommended to have at least two proxies, for reliability and load-balancing reasons.","title":"3.2 Setting up a proxy"},{"location":"03_stratum1_proxies/#321-requirements","text":"Just as with the other components, the Squid proxy server does not need a lot of resources. Just a few cores and few gigabytes of memory should be enough, although extra RAM can be used to cache more content. Similarly the more disk space you allocate for this machine, the larger the cache can be, and the better the performance will be. Note that this system will only store a part of the (deduplicated and compressed) repository, so it does not need as much storage space as the Stratum 0 or Stratum 1 server.","title":"3.2.1 Requirements"},{"location":"03_stratum1_proxies/#322-installation","text":"On the proxy server only Squid needs to be installed: sudo yum install -y squid","title":"3.2.2 Installation"},{"location":"03_stratum1_proxies/#323-configuration","text":"The configuration of a standalone Squid proxy is slightly different from the one that we used for our Stratum 1. You can use the following template to set up your own Squid configuration for your proxy server (in /etc/squid/squid.conf ): # List of local IP addresses (separate IPs and/or CIDR notation) allowed to access your local proxy acl local_nodes src YOUR_CLIENT_IPS # Destination domains that are allowed #acl stratum_ones dstdomain .YOURDOMAIN.ORG #acl stratum_ones dstdom_regex YOUR_REGEX # Squid port http_port 3128 # Deny access to anything which is not part of our stratum_ones ACL. http_access deny !stratum_ones # Only allow access from our local machines http_access allow local_nodes http_access allow localhost # Finally, deny all other access to this proxy http_access deny all minimum_expiry_time 0 maximum_object_size 1024 MB cache_mem 128 MB maximum_object_size_in_memory 128 KB # 5 GB disk cache cache_dir ufs /var/spool/squid 5000 16 256 In this template, there are two things you must change in the Access Control List (ACL) settings: The line starting with acl local_nodes specifies which clients are allowed to use this proxy. You can use CIDR notation . For the sake of this tutorial, you can just replace YOUR_CLIENT_IPS with the IP of your client system. For example: acl local_nodes src 1.2.3.4 Add a line starting with acl stratum_ones to specify an ACL for the allowed destination domains. For the sake of this tutorial, we can just hardcode this to our Stratum 1 server via dst (destination): acl stratum_ones dst <STRATUM1_IP> (where you need to change <STRATUM1_IP> with the IP address of your Stratum 1 server) The stratum_ones ACL you defined is used to specify that the Squid should only cache the Stratum 1 server, via the first http_access deny line. (Although restricting the destination is not strictly required for functionality, this can be beneficial for security to prevent misuse of the proxy server.) More information about Squid ACLs can be found in the Squid documentation . Finally, there are some settings regarding the size of your cache. Make sure that you have enough disk space for the value that you provide.","title":"3.2.3 Configuration"},{"location":"03_stratum1_proxies/#324-verifying-squid-configuration","text":"To verify the correctness of your Squid configuration, you can run: sudo squid -k parse When things look OK (no errors or warnings are printed, exit code zero), you can start and enable Squid.","title":"3.2.4 Verifying Squid configuration"},{"location":"03_stratum1_proxies/#325-starting-and-enabling-squid","text":"To start and enable Squid, run: sudo systemctl start squid sudo systemctl enable squid","title":"3.2.5 Starting and enabling Squid"},{"location":"03_stratum1_proxies/#33-re-configuring-the-client","text":"Now that we have a Stratum 0 server, a Stratum 1 server, and a Squid proxy, we have the (minimal) infrastructure in place for a production-ready CernVM-FS setup. So we can now configure our client properly, and start using the repository. In the previous section we connected our client directly to the Stratum 0. We are going to reuse that configuration, and only need to change two things.","title":"3.3 Re-configuring the client"},{"location":"03_stratum1_proxies/#331-connect-to-the-stratum-1","text":"We used the URL of the Stratum 0 in the file /etc/cvmfs/config.d/repo.organization.tld.conf . We should now change this URL, and point to the Stratum 1 instead: CVMFS_SERVER_URL=\"http://<STRATUM1_IP>/cvmfs/@fqrn@\" Replace the <STRATUM1_IP> part with the IP address of your Stratum 1 server! When you have more Stratum 1 servers inside the organization, you can make it a semicolon-separated list of servers. The Geo API will make sure that your client always connects to the geographically closest Stratum 1 server (if CVMFS_USE_GEOAPI=yes ).","title":"3.3.1 Connect to the Stratum 1"},{"location":"03_stratum1_proxies/#332-use-the-squid-proxy","text":"In order to use the local cache layer of our proxy, we have to instruct the client to send all requests through the proxy. This needs one small change in /etc/cvmfs/default.local , where you will have to replace DIRECT with IP address of your Squid proxy service, plus the (default) port 3128 at which Squid is running: CVMFS_HTTP_PROXY=\"http://<PROXY_IP>:3128\" Replace the <PROXY_IP> part with the IP address of your Squid proxy server! After changing the default.local configuration file, if the repository is currently mounted, do sudo cvmfs_config reload repo.organization.tld to apply the new configuration. If it is not currently mounted, the updated configuration will be automatically used the next time the repository is mounted (you can do ls /cvmfs/repo.organization.tld to cause autofs to mount it). More proxies can be added to CVMFS_HTTP_PROXY by separating them with a pipe or semicolon symbol. See the CernVM-FS documentation for examples.","title":"3.3.2 Use the Squid proxy"},{"location":"03_stratum1_proxies/#333-test-the-new-configuration","text":"Now you can test your new configuration by checking if you can still access the repository. Do sudo cvmfs_config chksetup to again test the configuration, including access to all configured Stratum 1 servers through all configured proxy servers. Furthermore, to confirm you are really using your Squid proxy (and to see which if there are more than one), you can do: cvmfs_config stat -v repo.organization.tld This should show a line that looks like: Connection: http://<STRATUM1_IP>/cvmfs/repo.organization.tld through proxy http://<PROXY_IP>:3128 (online) Make sure that it lists your proxy here (and not DIRECT ), and that it is marked as online . If you run into any issues, the Debugging section on the Advanced topics page may provide some useful information for finding the cause.","title":"3.3.3 Test the new configuration"},{"location":"03_stratum1_proxies/#exercise","text":"1) Set up a Stratum 1 server. Make sure that it includes: a proper Geo API license key (if you do not want to request an account, you can use the described method to bypass this, but again: do not do this in production!); cron job for automatically synchronizing the repository (e.g. once every 10 minutes); properly configured Apache and Squid services; 2) Set up a separate Squid proxy. Though it is recommended to at least have two in production, one is enough for now. 3) Reconfigure the client that you set up in the previous section and make sure that it uses your Stratum 1 and Squid proxy.","title":"Exercise"},{"location":"04_publishing/","text":"4. Publishing \u00b6 The previous sections were mostly about setting up the CernVM-FS infrastructure. Now that all the components for hosting and accessing your own CernVM-FS repository are in place, it is time to really start using it. In this section we will give some more details about adding files to your repository, which is referred to as publishing . 4.1 Transactions \u00b6 As we already showed in the first hands-on part of this tutorial , the easiest way to add files to your repository is by starting a transaction on your Stratum 0 server and then publishing the changes. By default, your repository directory under /cvmfs is read-only, but a transaction makes the directory writable for the user that is owner of the repository. This is done by creating a union filesystem in the background with a writable overlay. To start a transaction, run: cvmfs_server transaction repo.organization.tld While the transaction is \"open\", you can make changes to your repository. Once you are done with making changes, be sure to change your working directory to somewhere outside of the repository (otherwise you will get an error), and publish your changes using: cvmfs_server publish repo.organization.tld You can always abort a transaction, which will undo all the non-published modifications: cvmfs_server abort repo.organization.tld After publishing or aborting a transaction, your repository will again be read-only. 4.1.1 Ingesting tarballs \u00b6 When you need to compile software that you want to add to your repository, you may want to do the actual compilation on a different system than your Stratum 0, and copy the resulting installation as a tarball to your Stratum 0. Instead of manually starting a transaction, extracting the tarball and then publishing it, the cvmfs_server command offers a more efficient method for directly publishing the contents of a tarball: cvmfs_server ingest -b some/path repo.organization.tld -t mytarball.tar The -b option expects the relative location ( without leading slash! ) in your repository where the contents of the tarball, specified with -t , should be extracted. So in this case the tarball gets extracted to /cvmfs/repo.organization.tld/some/path . Note that passing ' / ' to the -b option does not work in CernVM-FS versions prior to 2.8.0 (see here ). In case you have a compressed tarball, you can use an appropriate decompression tool and write the output to stdout . This output can then be piped to cvmfs_server command while passing ' - ' to the -t option. For example, for a .tar.gz file: gunzip -c mytarball.tar.gz | cvmfs_server ingest -b some/path -t - 4.2 Tags \u00b6 By default, a newly published version of the repository will automatically get a tag with a timestamp in its name. This allows you to revert back to earlier versions. You can also set your own tag name and/or description upon publication: cvmfs_server publish -a example_tag -m \"Example description\" repo.organization.tld The tag subcommand for cvmfs_server allows you to create ( -a ), remove ( -r ), inspect ( -i ), or list ( -l ) tags of your repository, e.g.: cvmfs_server tag -a \"v1.0\" repo.organization.tld cvmfs_server tag -l repo.organization.tld With the rollback subcommand you can revert back to an earlier version. By default, this will be the previous version, but with -t you can specify a specific tag to revert to: cvmfs_server rollback -t \"v0.5\" repo.organization.tld 4.3 Catalogs \u00b6 All metadata about files in your repository is stored in a file catalog , which is a SQLite database. When a client accesses the repository for the first time, it first needs to retrieve this catalog. Only then it can start fetching the files it actually needs. Clients also need to regularly check for new versions of the repository, and redownload the catalog if it has changed. As this catalog can quickly become quite large when you start adding more and more files (think millions), just having a single one would cause significant overhead. In order to keep them small, you can make use of nested catalogs by having several catalogs for different subtrees of your repository. All metadata for that part of the subtree will not be part of the main catalog anymore, and clients will only download the catalogs for the subtree(s) they are trying to access. The general recommendation is to have more than 1,000 and fewer than 200,000 files and directories per (nested) catalog, and to bundle the files/directories that are often accessed together. For instance, it may make sense to provide a catalog per software installation directory, especially if different software versions or configurations of software are located in their own separate subdirectory (as is common with a software installation tool like EasyBuild ). Making nested catalogs manually can be done in two ways, which we will describe in more detail. Exceeding the limit In case a catalog file does grow larger than the recommended limit of 200,000 entries, you will get a warning when publishing new changes: WARNING: catalog at / has more than 200000 entries (1478305). Large catalogs stress the CernVM-FS transport infrastructure. Please split it into nested catalogs or increase the limit. 4.3.1 .cvmfscatalog files \u00b6 By adding an (empty, hidden) file named .cvmfscatalog into a directory of your repository, each following publish operation will automatically generate a nested catalog for the entire subtree in that directory. You can put these files at as many levels as you like, but do keep the recommendations mentioned above w.r.t. file and directory count in mind. 4.3.2 .cvmfsdirtab file \u00b6 Instead of creating .cvmfscatalog files, you can also add a (hidden) file named .cvmfsdirtab to the root of your repository. In this file you can specify a list of relative directory paths (they all start from the root of your repository) that should get a nested catalog. You can also use wildcards to specify patterns and automatically include future contents, and use exclamation marks to exclude paths from a nested catalog. As an example, assume you have a typical HPC software module tree in your repository with the following structure (relative to the root of the repository): software \u251c\u2500 software/app1 \u2502 \u251c\u2500 software/app1/1.0 \u2502 \u251c\u2500 software/app1/2.0 \u251c\u2500 software/app2 \u2502 \u251c\u2500 software/app2/20201201 \u2502 \u251c\u2500 software/app2/20210125 modules \u251c\u2500 modules/app1 \u2502 \u251c\u2500 modules/app1/1.0.lua \u2502 \u251c\u2500 modules/app1/2.0.lua \u251c\u2500 modules/all/app2 \u2502 \u251c\u2500 modules/app2/20201201.lua \u2502 \u251c\u2500 modules/app2/20210125.lua For this repository the .cvmfsdirtab file may look like: # Nested catalog for each version of each application /software/*/* # One nested catalog for all software directories /software # One nested catalog containing for all module files /modules Note that here the (relative) paths do have to start with a leading slash! After you have added this file to your repository, you should see automatically generated .cvmfscatalog files in all the specified directories (note that you can still place additional ones manually as well). You can also run cvmfs_server list-catalogs to get a full list of all the nested catalogs. One final note: if you use a .cvmfsdirtab file, a tarball ingestion using the cvmfs_server ingest command will currently (in CernVM-FS 2.8.0) not automatically create the nested catalogs . You will need to do another (empty) transaction right after the ingestion to trigger the creation of the nested catalogs. Exercise \u00b6 We have prepared a tarball that contains a collection of dummy installations of (fictional) software applications: cvmfs-tutorial-ingest-example-720k-files.tar.gz . You can download it easily onto your Stratum 0 via curl : curl -OL https://raw.githubusercontent.com/cvmfs-contrib/cvmfs-tutorial-2021/master/cvmfs-tutorial-ingest-example-720k-files.tar.gz Warning This tarball includes over 720,000 files in total, so be careful if/where you unpack it! To give you a head start, here's an overview of the directory structure included in this tarball: amd \u251c\u2500 rome \u2502 \u251c\u2500 modules | \u2502 \u251c\u2500 ... module files for each of the software installations ... \u2502 \u251c\u2500 software \u2502 | \u251c\u2500 arrr | \u2502 | \u251c\u2500 ... multiple versions, medium bin + lib subdir (20 files each) ... \u2502 | \u251c\u2500 FlensorStream | \u2502 | \u251c\u2500 ... multiple versions, small bin + lib subdir (10 files each) ... \u2502 | \u251c\u2500 GROAPPLES | \u2502 | \u251c\u2500 ... multiple versions, tiny bin + lib subdir (2 files each) ... \u2502 | \u251c\u2500 OpenPHOAN | \u2502 | \u251c\u2500 1.2-3 | | \u2502 | \u251c\u2500 bin | | | \u2502 | \u251c\u2500 ... 10 files ... | | \u2502 | \u251c\u2500 examples | | | \u2502 | \u251c\u2500 ... 3 subdirs, each with 80,000 files! ... | | \u2502 | \u251c\u2500 lib | | | \u2502 | \u251c\u2500 ... 10 files ... arm64 \u251c\u2500 thunderx2 \u2502 \u251c\u2500 ... same structure as amd/rome ... intel \u251c\u2500 haswell \u2502 \u251c\u2500 ... same structure as amd/rome ... Insert this tarball to a directory named easybuild in your repository using the ingest subcommand (without actually extracting the tarball manually). Note that you get some warnings about the catalog containing too many entries! Think about where you would create .cvmfscatalog files yourself (but don't do so manually). Fix the warning about the catalog being too big by adding a suitable .cvmfsdirtab file to the root of your repository. Make sure that the warning is gone when you publish this .cvmfsdirtab file. You may see a message about the catalog being defragmented (because lots of entries were cleaned up). Check if the mental exercise you did before adding the .cvmfsdirtab was correct, by inspecting where the .cvmfscatalog files were created. You can do this easily with find : find /cvmfs/repo.organization.tld -name .cvmfscatalog In addition, make sure that no catalogs have more than 200,000 entries. You can check this with: cvmfs_server list-catalogs -e CernVM-FS is less strict about large catalogs for subdirectories: up to 500,000 entries are allowed for non-root catalogs (by default), so not getting a warning when ingesting the .cvmfsdirtab file does not necessarily mean you solved the exercise correctly! (click to show solution for the .cvmfsdirtab - no peeking!) # For each microarchitecture subdirectory (/easybuild/*/*), create a nested catalog for: # the modules dir; /easybuild/*/*/modules # the software dir; /easybuild/*/*/software # each version of each application; /easybuild/*/*/software/*/* # each example subdirectory of each OpenPHOAN installation; /easybuild/*/*/software/OpenPHOAN/*/examples/*","title":"4. Publishing"},{"location":"04_publishing/#4-publishing","text":"The previous sections were mostly about setting up the CernVM-FS infrastructure. Now that all the components for hosting and accessing your own CernVM-FS repository are in place, it is time to really start using it. In this section we will give some more details about adding files to your repository, which is referred to as publishing .","title":"4. Publishing"},{"location":"04_publishing/#41-transactions","text":"As we already showed in the first hands-on part of this tutorial , the easiest way to add files to your repository is by starting a transaction on your Stratum 0 server and then publishing the changes. By default, your repository directory under /cvmfs is read-only, but a transaction makes the directory writable for the user that is owner of the repository. This is done by creating a union filesystem in the background with a writable overlay. To start a transaction, run: cvmfs_server transaction repo.organization.tld While the transaction is \"open\", you can make changes to your repository. Once you are done with making changes, be sure to change your working directory to somewhere outside of the repository (otherwise you will get an error), and publish your changes using: cvmfs_server publish repo.organization.tld You can always abort a transaction, which will undo all the non-published modifications: cvmfs_server abort repo.organization.tld After publishing or aborting a transaction, your repository will again be read-only.","title":"4.1 Transactions"},{"location":"04_publishing/#411-ingesting-tarballs","text":"When you need to compile software that you want to add to your repository, you may want to do the actual compilation on a different system than your Stratum 0, and copy the resulting installation as a tarball to your Stratum 0. Instead of manually starting a transaction, extracting the tarball and then publishing it, the cvmfs_server command offers a more efficient method for directly publishing the contents of a tarball: cvmfs_server ingest -b some/path repo.organization.tld -t mytarball.tar The -b option expects the relative location ( without leading slash! ) in your repository where the contents of the tarball, specified with -t , should be extracted. So in this case the tarball gets extracted to /cvmfs/repo.organization.tld/some/path . Note that passing ' / ' to the -b option does not work in CernVM-FS versions prior to 2.8.0 (see here ). In case you have a compressed tarball, you can use an appropriate decompression tool and write the output to stdout . This output can then be piped to cvmfs_server command while passing ' - ' to the -t option. For example, for a .tar.gz file: gunzip -c mytarball.tar.gz | cvmfs_server ingest -b some/path -t -","title":"4.1.1 Ingesting tarballs"},{"location":"04_publishing/#42-tags","text":"By default, a newly published version of the repository will automatically get a tag with a timestamp in its name. This allows you to revert back to earlier versions. You can also set your own tag name and/or description upon publication: cvmfs_server publish -a example_tag -m \"Example description\" repo.organization.tld The tag subcommand for cvmfs_server allows you to create ( -a ), remove ( -r ), inspect ( -i ), or list ( -l ) tags of your repository, e.g.: cvmfs_server tag -a \"v1.0\" repo.organization.tld cvmfs_server tag -l repo.organization.tld With the rollback subcommand you can revert back to an earlier version. By default, this will be the previous version, but with -t you can specify a specific tag to revert to: cvmfs_server rollback -t \"v0.5\" repo.organization.tld","title":"4.2 Tags"},{"location":"04_publishing/#43-catalogs","text":"All metadata about files in your repository is stored in a file catalog , which is a SQLite database. When a client accesses the repository for the first time, it first needs to retrieve this catalog. Only then it can start fetching the files it actually needs. Clients also need to regularly check for new versions of the repository, and redownload the catalog if it has changed. As this catalog can quickly become quite large when you start adding more and more files (think millions), just having a single one would cause significant overhead. In order to keep them small, you can make use of nested catalogs by having several catalogs for different subtrees of your repository. All metadata for that part of the subtree will not be part of the main catalog anymore, and clients will only download the catalogs for the subtree(s) they are trying to access. The general recommendation is to have more than 1,000 and fewer than 200,000 files and directories per (nested) catalog, and to bundle the files/directories that are often accessed together. For instance, it may make sense to provide a catalog per software installation directory, especially if different software versions or configurations of software are located in their own separate subdirectory (as is common with a software installation tool like EasyBuild ). Making nested catalogs manually can be done in two ways, which we will describe in more detail. Exceeding the limit In case a catalog file does grow larger than the recommended limit of 200,000 entries, you will get a warning when publishing new changes: WARNING: catalog at / has more than 200000 entries (1478305). Large catalogs stress the CernVM-FS transport infrastructure. Please split it into nested catalogs or increase the limit.","title":"4.3 Catalogs"},{"location":"04_publishing/#431-cvmfscatalog-files","text":"By adding an (empty, hidden) file named .cvmfscatalog into a directory of your repository, each following publish operation will automatically generate a nested catalog for the entire subtree in that directory. You can put these files at as many levels as you like, but do keep the recommendations mentioned above w.r.t. file and directory count in mind.","title":"4.3.1 .cvmfscatalog files"},{"location":"04_publishing/#432-cvmfsdirtab-file","text":"Instead of creating .cvmfscatalog files, you can also add a (hidden) file named .cvmfsdirtab to the root of your repository. In this file you can specify a list of relative directory paths (they all start from the root of your repository) that should get a nested catalog. You can also use wildcards to specify patterns and automatically include future contents, and use exclamation marks to exclude paths from a nested catalog. As an example, assume you have a typical HPC software module tree in your repository with the following structure (relative to the root of the repository): software \u251c\u2500 software/app1 \u2502 \u251c\u2500 software/app1/1.0 \u2502 \u251c\u2500 software/app1/2.0 \u251c\u2500 software/app2 \u2502 \u251c\u2500 software/app2/20201201 \u2502 \u251c\u2500 software/app2/20210125 modules \u251c\u2500 modules/app1 \u2502 \u251c\u2500 modules/app1/1.0.lua \u2502 \u251c\u2500 modules/app1/2.0.lua \u251c\u2500 modules/all/app2 \u2502 \u251c\u2500 modules/app2/20201201.lua \u2502 \u251c\u2500 modules/app2/20210125.lua For this repository the .cvmfsdirtab file may look like: # Nested catalog for each version of each application /software/*/* # One nested catalog for all software directories /software # One nested catalog containing for all module files /modules Note that here the (relative) paths do have to start with a leading slash! After you have added this file to your repository, you should see automatically generated .cvmfscatalog files in all the specified directories (note that you can still place additional ones manually as well). You can also run cvmfs_server list-catalogs to get a full list of all the nested catalogs. One final note: if you use a .cvmfsdirtab file, a tarball ingestion using the cvmfs_server ingest command will currently (in CernVM-FS 2.8.0) not automatically create the nested catalogs . You will need to do another (empty) transaction right after the ingestion to trigger the creation of the nested catalogs.","title":"4.3.2 .cvmfsdirtab file"},{"location":"04_publishing/#exercise","text":"We have prepared a tarball that contains a collection of dummy installations of (fictional) software applications: cvmfs-tutorial-ingest-example-720k-files.tar.gz . You can download it easily onto your Stratum 0 via curl : curl -OL https://raw.githubusercontent.com/cvmfs-contrib/cvmfs-tutorial-2021/master/cvmfs-tutorial-ingest-example-720k-files.tar.gz Warning This tarball includes over 720,000 files in total, so be careful if/where you unpack it! To give you a head start, here's an overview of the directory structure included in this tarball: amd \u251c\u2500 rome \u2502 \u251c\u2500 modules | \u2502 \u251c\u2500 ... module files for each of the software installations ... \u2502 \u251c\u2500 software \u2502 | \u251c\u2500 arrr | \u2502 | \u251c\u2500 ... multiple versions, medium bin + lib subdir (20 files each) ... \u2502 | \u251c\u2500 FlensorStream | \u2502 | \u251c\u2500 ... multiple versions, small bin + lib subdir (10 files each) ... \u2502 | \u251c\u2500 GROAPPLES | \u2502 | \u251c\u2500 ... multiple versions, tiny bin + lib subdir (2 files each) ... \u2502 | \u251c\u2500 OpenPHOAN | \u2502 | \u251c\u2500 1.2-3 | | \u2502 | \u251c\u2500 bin | | | \u2502 | \u251c\u2500 ... 10 files ... | | \u2502 | \u251c\u2500 examples | | | \u2502 | \u251c\u2500 ... 3 subdirs, each with 80,000 files! ... | | \u2502 | \u251c\u2500 lib | | | \u2502 | \u251c\u2500 ... 10 files ... arm64 \u251c\u2500 thunderx2 \u2502 \u251c\u2500 ... same structure as amd/rome ... intel \u251c\u2500 haswell \u2502 \u251c\u2500 ... same structure as amd/rome ... Insert this tarball to a directory named easybuild in your repository using the ingest subcommand (without actually extracting the tarball manually). Note that you get some warnings about the catalog containing too many entries! Think about where you would create .cvmfscatalog files yourself (but don't do so manually). Fix the warning about the catalog being too big by adding a suitable .cvmfsdirtab file to the root of your repository. Make sure that the warning is gone when you publish this .cvmfsdirtab file. You may see a message about the catalog being defragmented (because lots of entries were cleaned up). Check if the mental exercise you did before adding the .cvmfsdirtab was correct, by inspecting where the .cvmfscatalog files were created. You can do this easily with find : find /cvmfs/repo.organization.tld -name .cvmfscatalog In addition, make sure that no catalogs have more than 200,000 entries. You can check this with: cvmfs_server list-catalogs -e CernVM-FS is less strict about large catalogs for subdirectories: up to 500,000 entries are allowed for non-root catalogs (by default), so not getting a warning when ingesting the .cvmfsdirtab file does not necessarily mean you solved the exercise correctly! (click to show solution for the .cvmfsdirtab - no peeking!) # For each microarchitecture subdirectory (/easybuild/*/*), create a nested catalog for: # the modules dir; /easybuild/*/*/modules # the software dir; /easybuild/*/*/software # each version of each application; /easybuild/*/*/software/*/* # each example subdirectory of each OpenPHOAN installation; /easybuild/*/*/software/OpenPHOAN/*/examples/*","title":"Exercise"},{"location":"05_advanced/","text":"5. Advanced topics \u00b6 5.1 Automatic deployment of CernVM-FS servers and clients \u00b6 As you may have experienced during this tutorial, it takes quite a bit of manual effort to deploy all the different CernVM-FS components, and you can easily make mistakes. Therefore, we strongly recommend to automate this in a production setup with a tool like Ansible or Puppet . For Ansible, you could take a look at the playbooks of the EESSI project , which use the Ansible role from the Galaxy Project to install and configure both servers and clients. Compute Canada also offers an Ansible role to configure CernVM-FS clients, and a demo release of an Ansible role for Stratum servers . CERN offers its own Puppet module that allows you to install and configure CernVM-FS servers and clients. 5.2 Debugging issues \u00b6 If you are experiencing issues with your CernVM-FS setup, there are various ways to start debugging. Most issues are caused by wrongly configured clients (either a configuration issue, or a wrong public key) and connection or firewall issues. 5.2.1 Debugging with cvmfs_config \u00b6 In order to find the cause of the issue, you should first find out where the issue is being caused. You can start by checking the client configuration: sudo cvmfs_config chksetup This should print OK . To make sure that your configuration is really picked up and set correctly (because of the hierarchical structure of the configuration, it is possible that some parameter gets overwritten by another configuration file), you can dump the effective configuration for your repository: cvmfs_config showconfig repo.organization.tld Make sure that at least CVMFS_HTTP_PROXY and CVMFS_SERVER_URL are set correctly, and that the directory pointed to by CVMFS_KEYS_DIR really contains the (correct) public key file. The probe subcommand can be used for (re)trying to mount the repository, and should print OK : $ cvmfs_config probe repo.organization.tld Probing /cvmfs/repo.organization.tld... OK However, since you are debugging a problem, it probably returns an error... So, let's enable some debugging output by adding the following line to your /etc/cvmfs/default.local : CVMFS_DEBUGLOG=/path/to/cvmfs.log Warning Make sure that the cvmfs user has write permission to the location specified with CVMFS_DEBUGLOG . Otherwise you will not only get no log file, but it will also lead to client failures. Now we unmount the repository and try to probe it again, so that the configuration gets reloaded and the debug log gets created: sudo cvmfs_config umount cvmfs_config probe repo.organization.tld You can now check your debug log file, and look for any error messages near the bottom of the file; they may reveal more details about the issue. 5.2.2 Debugging connection issues \u00b6 If the problem turns out to be some kind of connection issue, you can trace it down further by manually checking the connections from your client to the proxy and/or Stratum 1 server. First, let's rule out that it is some kind of firewall issue by verifying that you can actually connect to the appropriate ports on those servers: telnet <PROXY_IP> 3128 telnet <STRATUM1_IP> 80 If this does work, probably something is wrong with the services running on these machines. Every CernVM-FS repository has a file named .cvmfspublished , and you can try to fetch it manually using curl , both directly from the Stratum 1 and via your proxy: # Without your own proxy, so directly to the Stratum 1: curl --head http://<STRATUM1_IP>/cvmfs/repo.organization.tld/.cvmfspublished # With your caching proxy between the client and Stratum 1: curl --proxy http://<PROXY_IP>:3128 --head http://url-to-your-stratum1/cvmfs/repo.organization.tld/.cvmfspublished These commands should return HTTP/1.1 200 OK . If the first command returns something else, you should inspect your CernVM-FS, Apache, and Squid configuration (and log) files on the Stratum 1 server. If the first curl command does work, but the second does not, there is something wrong with your Squid proxy; make sure that it is running, configured, and able to access your Stratum 1 server. 5.2.3 Checking the logs of CernVM-FS services \u00b6 Besides the client log file that we already explained, there are some other log files that you can inspect on the different servers. On the Stratum 0, the main log files are the Apache access and error files, which you can find (on CentOS) in /var/log/httpd . The Stratum 1 has several services, and, hence, several log files that can be of interest: just like on the Stratum 0, there are the Apache log files. Besides those, also Squid has access and cache log files, which can be found in /var/log/squid . The cvmfs_server snapshot commands will log to /var/log/cvmfs/snapshots.log . Finally, the only relevant service on the proxy server is Squid itself, so /var/log/squid is again the place to find the log files. 5.3 Garbage collection \u00b6 As mentioned in the section about publishing , the default configuration of a Stratum 0 enables automatic tagging, which automatically assigns a timestamped tag to each published transaction. However, by default, these automatically generated tags will not be removed automatically. As a result, files that you remove in later transactions will still take up space in your repository... 5.3.1 Setting the lifetime of automatically generated tags \u00b6 Instead of removing tags manually, you can automatically mark these automatically generated tags for removal after a certain period by setting the following variable in the file /etc/cvmfs/repositories.d/repo.organization.tld/server.conf on your Stratum 0: CVMFS_AUTO_TAG_TIMESPAN=\"30 days ago\" This should be a string that can be parsed by the date command, and defines the lifetime of the tags. 5.3.2 Cleaning up tags marked for removal \u00b6 In order to actually clean up unreferenced data, garbage collection has to be enabled for the repository by adding CVMFS_GARBAGE_COLLECTION=true in the server.conf configuration file on Stratum 0. The garbage collector of the CernVM-FS server can then be run using: sudo cvmfs_server gc repo.organization.tld The gc subcommand has several options; a useful way to run it, especially if you want to do this with a cron job, is: sudo cvmfs_server gc -a -l -f The -a option will automatically run the garbage collection for all your repositories that have garbage collection enabled and log to /var/log/cvmfs/gc.log ; the -l option will make the command print which objects are actually removed; and the -f option will not prompt for confirmation. Note that you cannot run the garbage collection while a publish operation is ongoing. 5.4 Gateway and Publishers \u00b6 Only being able to modify your repository on the Stratum 0 server can be a bit limiting, especially when multiple people have to maintain the repository. A very recent feature of CernVM-FS allows you to set up so-called publisher machines , which are separate systems that are allowed to modify the repository. It also allows for setting up simple ACLs to let a system only access specific subtrees of the repository. In order to use this feature you also need a gateway machine that has the repository storage mounted. The easiest way to set it up is by having a single system that serves as both the Stratum 0 and the gateway. This is the setup that we will explain here. Do note that this is a fairly new feature and is not used a lot by production sites yet. Therefore, use it at your own risk! 5.4.1 Gateway \u00b6 Requirements The gateway system has the same requirements as a standard Stratum 0 server, except that it also needs an additional port for the gateway service. This port is configurable, but by default port 4929 is used. Installation Perform the installation steps for the Stratum 0, which can be found in an earlier section . Additionally, install the cvmfs-gateway package: sudo yum install -y cvmfs-gateway Then create the repository just like we did on Stratum 0 : sudo cvmfs_server mkfs -o $USER repo.organization.tld Configuration The gateway requires you to set up a configuration file /etc/cvmfs/gateway/repo.json . This is a JSON file containing the name of the repository, the keys that can be used by publishers to get access to the repository, and the (sub)path that these publishers are allowed to publish to. The cvmfs-gateway package will make an example file for you, which you can edit or overwrite. It should look like this: { \"version\" : 2 , \"repos\" : [ { \"domain\" : \"repo.organization.tld\" , \"keys\" : [ { \"id\" : \"keyid1\" , \"path\" : \"/\" }, { \"id\" : \"keyid2\" , \"path\" : \"/restricted/to/subdir\" } ] } ], \"keys\" : [ { \"type\" : \"plain_text\" , \"id\" : \"keyid1\" , \"secret\" : \"SOME_SECRET\" }, { \"type\" : \"plain_text\" , \"id\" : \"keyid2\" , \"secret\" : \"SOME_OTHER_SECRET\" }, ] } You can choose the key IDs and secrets yourself; the secret has to be given to the owner of the corresponding publisher machine. Finally, there is a second configuration file /etc/cvmfs/gateway/user.json . This is where you can, for instance, change the port of the gateway service and the maximum length of an acquired lease. Assuming you do not have to change the port, you can leave it as it is. Starting the service To start the gateway service, use: systemctl start cvmfs-gateway Note that once this service is running you should not open transactions on this Stratum 0 server anymore, or you may corrupt the repository. If you do want to open a transaction, stop the gateway service first! 5.4.2 Publisher \u00b6 Requirements There a no special requirements for a publisher system with respect to resources. Installation The publisher only needs to have the cvmfs-server package installed: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs-server Configuration The publisher machine only needs three files with keys: the repository's public master key: repo.organization.tld.pub ; the repository's public key encoded as X509 certificate: repo.organization.tld.crt ; the gateway API key stored in a file named repo.organization.tld.gw . The first two files can be taken from /etc/cvmfs/keys on your Stratum 0 server. The latter can be created manually and should just contain the secret that was used in the gateway configuration. All these files should be placed in some (temporary) directory on the publisher system. Creating the repository We can now create the repository available for writing on our publisher machine by running: export S0_IP = '<STRATUM0_IP>' sudo cvmfs_server mkfs -w http:// $S0_IP /cvmfs/repo.organization.tld \\ -u gw,/srv/cvmfs/repo.organization.tld/data/txn,http:// $S0_IP :4929/api/v1 \\ -k /path/to/keys/dir -o $USER repo.organization.tld Replace <STRATUM0_IP> with the IP address (or hostname) of your gateway / Stratum 0 server (and change 4929 in case you changed the gateway port), and /path/to/keys/dir by the path where you stored the keys in the previous step. Start publishing! You should now be able to make changes to the repository by starting a transaction: cvmfs_server transaction repo.organization.tld making some changes to the repository at /cvmfs/repo.organization.tld , and then publishing the changes: cvmfs_server publish repo.organization.tld 5.5 Using a configuration repository \u00b6 In the first hands-on part of this tutorial we have manually configured our CernVM-FS client. Although that was not very complicated, we did have to make sure that different things were in the right place and properly named in order to successfully mount the repository. We had to copy the public key of the repository under /etc/cvmfs/key/<domain> , and create a configuration file in /etc/cvmfs/config.d/<reponame>.<domain>.conf that specifies the location of the key as well as the IP(s) of (eventually) the Stratum 1 servers that are available for this repository. Next to the manual aspect, there is also a maintenance issue here: if the list of Stratum 1 servers changes, for example if additional servers are added to the network, we have know/remember to update our configuration file. CernVM-FS provides an easy way to prevent these issues, by using a so-called configuration repository . This is a standard CernVM-FS repository which is mounted under /cvmfs , and contains an etc/cvmfs subdirectory with the same structure as the regular /etc/cvmfs . It provides the public keys and configuration of different CernVM-FS repositories, and it is updated automatically when changes are made to it. So there is no more need for manually maintaining or updating for the provided software repositories. One limitation in CernVM-FS is that you can only use one configuration repository at a time. If you want to mount additional software repositories for which the public key and configuration is not included in the configuration repository you are using, you have to statically configure those repositories, and maintain those configurations yourself somehow, either manually or by making sure you update the package that provides the configuration. cvmfs-contrib \u00b6 Several CernVM-FS configuration repositories, which collect the public keys and configuration for a couple of major organizations, are available via the cvmfs-contrib GitHub organisation ; see the website and cvmfs-contrib/config-repo GitHub repository. Easy-to-install packages for different CernVM-FS configuration repositories are available via both a yum and apt repository. EESSI \u00b6 The EESSI project also provides easy-to-install packages for its CernVM-FS configuration repository, which are available through the EESSI/filesystem-layer GitHub repository. For example, to install the EESSI CernVM-FS configuration repository on CentOS 7 or 8: sudo yum install -y https://github.com/EESSI/filesystem-layer/releases/download/v0.2.3/cvmfs-config-eessi-0.2.3-1.noarch.rpm After installing this package, you will have the CernVM-FS configuration repository for EESSI available: $ ls /cvmfs/cvmfs-config.eessi-hpc.org/etc/cvmfs contact default.conf domain.d keys And as a result, you can also access the EESSI pilot software repository at /cvmfs/pilot.eessi-hpc.org !","title":"5. Advanced topics"},{"location":"05_advanced/#5-advanced-topics","text":"","title":"5. Advanced topics"},{"location":"05_advanced/#51-automatic-deployment-of-cernvm-fs-servers-and-clients","text":"As you may have experienced during this tutorial, it takes quite a bit of manual effort to deploy all the different CernVM-FS components, and you can easily make mistakes. Therefore, we strongly recommend to automate this in a production setup with a tool like Ansible or Puppet . For Ansible, you could take a look at the playbooks of the EESSI project , which use the Ansible role from the Galaxy Project to install and configure both servers and clients. Compute Canada also offers an Ansible role to configure CernVM-FS clients, and a demo release of an Ansible role for Stratum servers . CERN offers its own Puppet module that allows you to install and configure CernVM-FS servers and clients.","title":"5.1 Automatic deployment of CernVM-FS servers and clients"},{"location":"05_advanced/#52-debugging-issues","text":"If you are experiencing issues with your CernVM-FS setup, there are various ways to start debugging. Most issues are caused by wrongly configured clients (either a configuration issue, or a wrong public key) and connection or firewall issues.","title":"5.2 Debugging issues"},{"location":"05_advanced/#521-debugging-with-cvmfs_config","text":"In order to find the cause of the issue, you should first find out where the issue is being caused. You can start by checking the client configuration: sudo cvmfs_config chksetup This should print OK . To make sure that your configuration is really picked up and set correctly (because of the hierarchical structure of the configuration, it is possible that some parameter gets overwritten by another configuration file), you can dump the effective configuration for your repository: cvmfs_config showconfig repo.organization.tld Make sure that at least CVMFS_HTTP_PROXY and CVMFS_SERVER_URL are set correctly, and that the directory pointed to by CVMFS_KEYS_DIR really contains the (correct) public key file. The probe subcommand can be used for (re)trying to mount the repository, and should print OK : $ cvmfs_config probe repo.organization.tld Probing /cvmfs/repo.organization.tld... OK However, since you are debugging a problem, it probably returns an error... So, let's enable some debugging output by adding the following line to your /etc/cvmfs/default.local : CVMFS_DEBUGLOG=/path/to/cvmfs.log Warning Make sure that the cvmfs user has write permission to the location specified with CVMFS_DEBUGLOG . Otherwise you will not only get no log file, but it will also lead to client failures. Now we unmount the repository and try to probe it again, so that the configuration gets reloaded and the debug log gets created: sudo cvmfs_config umount cvmfs_config probe repo.organization.tld You can now check your debug log file, and look for any error messages near the bottom of the file; they may reveal more details about the issue.","title":"5.2.1 Debugging with cvmfs_config"},{"location":"05_advanced/#522-debugging-connection-issues","text":"If the problem turns out to be some kind of connection issue, you can trace it down further by manually checking the connections from your client to the proxy and/or Stratum 1 server. First, let's rule out that it is some kind of firewall issue by verifying that you can actually connect to the appropriate ports on those servers: telnet <PROXY_IP> 3128 telnet <STRATUM1_IP> 80 If this does work, probably something is wrong with the services running on these machines. Every CernVM-FS repository has a file named .cvmfspublished , and you can try to fetch it manually using curl , both directly from the Stratum 1 and via your proxy: # Without your own proxy, so directly to the Stratum 1: curl --head http://<STRATUM1_IP>/cvmfs/repo.organization.tld/.cvmfspublished # With your caching proxy between the client and Stratum 1: curl --proxy http://<PROXY_IP>:3128 --head http://url-to-your-stratum1/cvmfs/repo.organization.tld/.cvmfspublished These commands should return HTTP/1.1 200 OK . If the first command returns something else, you should inspect your CernVM-FS, Apache, and Squid configuration (and log) files on the Stratum 1 server. If the first curl command does work, but the second does not, there is something wrong with your Squid proxy; make sure that it is running, configured, and able to access your Stratum 1 server.","title":"5.2.2 Debugging connection issues"},{"location":"05_advanced/#523-checking-the-logs-of-cernvm-fs-services","text":"Besides the client log file that we already explained, there are some other log files that you can inspect on the different servers. On the Stratum 0, the main log files are the Apache access and error files, which you can find (on CentOS) in /var/log/httpd . The Stratum 1 has several services, and, hence, several log files that can be of interest: just like on the Stratum 0, there are the Apache log files. Besides those, also Squid has access and cache log files, which can be found in /var/log/squid . The cvmfs_server snapshot commands will log to /var/log/cvmfs/snapshots.log . Finally, the only relevant service on the proxy server is Squid itself, so /var/log/squid is again the place to find the log files.","title":"5.2.3 Checking the logs of CernVM-FS services"},{"location":"05_advanced/#53-garbage-collection","text":"As mentioned in the section about publishing , the default configuration of a Stratum 0 enables automatic tagging, which automatically assigns a timestamped tag to each published transaction. However, by default, these automatically generated tags will not be removed automatically. As a result, files that you remove in later transactions will still take up space in your repository...","title":"5.3 Garbage collection"},{"location":"05_advanced/#531-setting-the-lifetime-of-automatically-generated-tags","text":"Instead of removing tags manually, you can automatically mark these automatically generated tags for removal after a certain period by setting the following variable in the file /etc/cvmfs/repositories.d/repo.organization.tld/server.conf on your Stratum 0: CVMFS_AUTO_TAG_TIMESPAN=\"30 days ago\" This should be a string that can be parsed by the date command, and defines the lifetime of the tags.","title":"5.3.1 Setting the lifetime of automatically generated tags"},{"location":"05_advanced/#532-cleaning-up-tags-marked-for-removal","text":"In order to actually clean up unreferenced data, garbage collection has to be enabled for the repository by adding CVMFS_GARBAGE_COLLECTION=true in the server.conf configuration file on Stratum 0. The garbage collector of the CernVM-FS server can then be run using: sudo cvmfs_server gc repo.organization.tld The gc subcommand has several options; a useful way to run it, especially if you want to do this with a cron job, is: sudo cvmfs_server gc -a -l -f The -a option will automatically run the garbage collection for all your repositories that have garbage collection enabled and log to /var/log/cvmfs/gc.log ; the -l option will make the command print which objects are actually removed; and the -f option will not prompt for confirmation. Note that you cannot run the garbage collection while a publish operation is ongoing.","title":"5.3.2 Cleaning up tags marked for removal"},{"location":"05_advanced/#54-gateway-and-publishers","text":"Only being able to modify your repository on the Stratum 0 server can be a bit limiting, especially when multiple people have to maintain the repository. A very recent feature of CernVM-FS allows you to set up so-called publisher machines , which are separate systems that are allowed to modify the repository. It also allows for setting up simple ACLs to let a system only access specific subtrees of the repository. In order to use this feature you also need a gateway machine that has the repository storage mounted. The easiest way to set it up is by having a single system that serves as both the Stratum 0 and the gateway. This is the setup that we will explain here. Do note that this is a fairly new feature and is not used a lot by production sites yet. Therefore, use it at your own risk!","title":"5.4 Gateway and Publishers"},{"location":"05_advanced/#541-gateway","text":"Requirements The gateway system has the same requirements as a standard Stratum 0 server, except that it also needs an additional port for the gateway service. This port is configurable, but by default port 4929 is used. Installation Perform the installation steps for the Stratum 0, which can be found in an earlier section . Additionally, install the cvmfs-gateway package: sudo yum install -y cvmfs-gateway Then create the repository just like we did on Stratum 0 : sudo cvmfs_server mkfs -o $USER repo.organization.tld Configuration The gateway requires you to set up a configuration file /etc/cvmfs/gateway/repo.json . This is a JSON file containing the name of the repository, the keys that can be used by publishers to get access to the repository, and the (sub)path that these publishers are allowed to publish to. The cvmfs-gateway package will make an example file for you, which you can edit or overwrite. It should look like this: { \"version\" : 2 , \"repos\" : [ { \"domain\" : \"repo.organization.tld\" , \"keys\" : [ { \"id\" : \"keyid1\" , \"path\" : \"/\" }, { \"id\" : \"keyid2\" , \"path\" : \"/restricted/to/subdir\" } ] } ], \"keys\" : [ { \"type\" : \"plain_text\" , \"id\" : \"keyid1\" , \"secret\" : \"SOME_SECRET\" }, { \"type\" : \"plain_text\" , \"id\" : \"keyid2\" , \"secret\" : \"SOME_OTHER_SECRET\" }, ] } You can choose the key IDs and secrets yourself; the secret has to be given to the owner of the corresponding publisher machine. Finally, there is a second configuration file /etc/cvmfs/gateway/user.json . This is where you can, for instance, change the port of the gateway service and the maximum length of an acquired lease. Assuming you do not have to change the port, you can leave it as it is. Starting the service To start the gateway service, use: systemctl start cvmfs-gateway Note that once this service is running you should not open transactions on this Stratum 0 server anymore, or you may corrupt the repository. If you do want to open a transaction, stop the gateway service first!","title":"5.4.1 Gateway"},{"location":"05_advanced/#542-publisher","text":"Requirements There a no special requirements for a publisher system with respect to resources. Installation The publisher only needs to have the cvmfs-server package installed: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs-server Configuration The publisher machine only needs three files with keys: the repository's public master key: repo.organization.tld.pub ; the repository's public key encoded as X509 certificate: repo.organization.tld.crt ; the gateway API key stored in a file named repo.organization.tld.gw . The first two files can be taken from /etc/cvmfs/keys on your Stratum 0 server. The latter can be created manually and should just contain the secret that was used in the gateway configuration. All these files should be placed in some (temporary) directory on the publisher system. Creating the repository We can now create the repository available for writing on our publisher machine by running: export S0_IP = '<STRATUM0_IP>' sudo cvmfs_server mkfs -w http:// $S0_IP /cvmfs/repo.organization.tld \\ -u gw,/srv/cvmfs/repo.organization.tld/data/txn,http:// $S0_IP :4929/api/v1 \\ -k /path/to/keys/dir -o $USER repo.organization.tld Replace <STRATUM0_IP> with the IP address (or hostname) of your gateway / Stratum 0 server (and change 4929 in case you changed the gateway port), and /path/to/keys/dir by the path where you stored the keys in the previous step. Start publishing! You should now be able to make changes to the repository by starting a transaction: cvmfs_server transaction repo.organization.tld making some changes to the repository at /cvmfs/repo.organization.tld , and then publishing the changes: cvmfs_server publish repo.organization.tld","title":"5.4.2 Publisher"},{"location":"05_advanced/#55-using-a-configuration-repository","text":"In the first hands-on part of this tutorial we have manually configured our CernVM-FS client. Although that was not very complicated, we did have to make sure that different things were in the right place and properly named in order to successfully mount the repository. We had to copy the public key of the repository under /etc/cvmfs/key/<domain> , and create a configuration file in /etc/cvmfs/config.d/<reponame>.<domain>.conf that specifies the location of the key as well as the IP(s) of (eventually) the Stratum 1 servers that are available for this repository. Next to the manual aspect, there is also a maintenance issue here: if the list of Stratum 1 servers changes, for example if additional servers are added to the network, we have know/remember to update our configuration file. CernVM-FS provides an easy way to prevent these issues, by using a so-called configuration repository . This is a standard CernVM-FS repository which is mounted under /cvmfs , and contains an etc/cvmfs subdirectory with the same structure as the regular /etc/cvmfs . It provides the public keys and configuration of different CernVM-FS repositories, and it is updated automatically when changes are made to it. So there is no more need for manually maintaining or updating for the provided software repositories. One limitation in CernVM-FS is that you can only use one configuration repository at a time. If you want to mount additional software repositories for which the public key and configuration is not included in the configuration repository you are using, you have to statically configure those repositories, and maintain those configurations yourself somehow, either manually or by making sure you update the package that provides the configuration.","title":"5.5 Using a configuration repository"},{"location":"05_advanced/#cvmfs-contrib","text":"Several CernVM-FS configuration repositories, which collect the public keys and configuration for a couple of major organizations, are available via the cvmfs-contrib GitHub organisation ; see the website and cvmfs-contrib/config-repo GitHub repository. Easy-to-install packages for different CernVM-FS configuration repositories are available via both a yum and apt repository.","title":"cvmfs-contrib"},{"location":"05_advanced/#eessi","text":"The EESSI project also provides easy-to-install packages for its CernVM-FS configuration repository, which are available through the EESSI/filesystem-layer GitHub repository. For example, to install the EESSI CernVM-FS configuration repository on CentOS 7 or 8: sudo yum install -y https://github.com/EESSI/filesystem-layer/releases/download/v0.2.3/cvmfs-config-eessi-0.2.3-1.noarch.rpm After installing this package, you will have the CernVM-FS configuration repository for EESSI available: $ ls /cvmfs/cvmfs-config.eessi-hpc.org/etc/cvmfs contact default.conf domain.d keys And as a result, you can also access the EESSI pilot software repository at /cvmfs/pilot.eessi-hpc.org !","title":"EESSI"}]}