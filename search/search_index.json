{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the CernVM-FS tutorial! \u00b6 Scope \u00b6 This is an introductory tutorial to the CernVM File System (CernVM-FS). In this tutorial you will learn what CernVM-FS is and how to use it, both from an administrator and end user point of view, through guided examples and hands-on exercises. We focus on the core concepts and basic usage of CernVM-FS, and provide pointers to more information regarding more advanced aspects. Intended audience \u00b6 This tutorial is intended for people who are new to CernVM-FS: no specific prior knowledge or experience with it is required. We expect it to be most valuable to people who are interested in either setting up and managing a CernVM-FS repository themselves, or using one or more existing CernVM-FS repositories. Prerequisites \u00b6 Being familiar with a Linux shell environment and having a basic notion of Linux filesystems and system administration is recommended but not strictly required; we hope that the guided examples are sufficient to follow along comfortably. You should have access to a couple of Linux instances on which you have administrative rights ( sudo access). The required resources are minimal: 1 or 2 cores, a couple of GBs of RAM, and about 10GB of disk space per instance is more than sufficient. We recommend using Linux virtual machines that were created specifically for this tutorial, on which only a base Linux distribution was installed as operating system, and which you are comfortable to discard afterwards. This tutorial was prepared on CentOS 7 ( x86_64 ), but it should be relatively straightforward to translate the instructions to other Linux distributions like Ubuntu, or another CPU architecture like Arm 64-bit ( aarch64 ). Practical information \u00b6 This tutorial is being organised during the 4th EasyBuild User Meeting (Jan 25-29 2021) . More information about the CernVM-FS tutorial sessions is available at https://easybuild.io/eum/#cvmfs-tutorial . Registration for this tutorial is required in order to get access to the provided cloud resources! Dedicated cloud resources in Microsoft Azure (sponsored by Microsoft) will be available only to registered tutorial attendees for working on the hands-on exercises during the the week of Jan 25-29 2021. Nothing in this tutorial, other than the section of using the provided Azure cloud resources , is specific to Azure. You can use your own resources if you prefer doing so. Tutorial contents \u00b6 0. Azure cloud resources 1. Introduction to CernVM-FS 2. Stratum 0 and client (*) 3. Stratum 1 and proxies (*) 4. Publishing (*) 5. Advanced topics (sections indicated with (*) involve hands-on exercises) Contributors \u00b6 Jakob Blomer (CERN, Switzerland) Bob Dr\u00f6ge (University of Groningen, The Netherlands) Kenneth Hoste (HPC-UGent, Belgium) Ryan Taylor (ComputeCanada) Additional resources \u00b6 CernVM-FS website: https://cernvm.cern.ch/fs CernVM-FS documentation: https://cvmfs.readthedocs.io CernVM-FS @ GitHub: https://github.com/cvmfs CernVM-FS tutorial @ GitHub: https://github.com/cvmfs-contrib/cvmfs-tutorial-2021","title":"Home"},{"location":"#welcome-to-the-cernvm-fs-tutorial","text":"","title":"Welcome to the CernVM-FS tutorial!"},{"location":"#scope","text":"This is an introductory tutorial to the CernVM File System (CernVM-FS). In this tutorial you will learn what CernVM-FS is and how to use it, both from an administrator and end user point of view, through guided examples and hands-on exercises. We focus on the core concepts and basic usage of CernVM-FS, and provide pointers to more information regarding more advanced aspects.","title":"Scope"},{"location":"#intended-audience","text":"This tutorial is intended for people who are new to CernVM-FS: no specific prior knowledge or experience with it is required. We expect it to be most valuable to people who are interested in either setting up and managing a CernVM-FS repository themselves, or using one or more existing CernVM-FS repositories.","title":"Intended audience"},{"location":"#prerequisites","text":"Being familiar with a Linux shell environment and having a basic notion of Linux filesystems and system administration is recommended but not strictly required; we hope that the guided examples are sufficient to follow along comfortably. You should have access to a couple of Linux instances on which you have administrative rights ( sudo access). The required resources are minimal: 1 or 2 cores, a couple of GBs of RAM, and about 10GB of disk space per instance is more than sufficient. We recommend using Linux virtual machines that were created specifically for this tutorial, on which only a base Linux distribution was installed as operating system, and which you are comfortable to discard afterwards. This tutorial was prepared on CentOS 7 ( x86_64 ), but it should be relatively straightforward to translate the instructions to other Linux distributions like Ubuntu, or another CPU architecture like Arm 64-bit ( aarch64 ).","title":"Prerequisites"},{"location":"#practical-information","text":"This tutorial is being organised during the 4th EasyBuild User Meeting (Jan 25-29 2021) . More information about the CernVM-FS tutorial sessions is available at https://easybuild.io/eum/#cvmfs-tutorial . Registration for this tutorial is required in order to get access to the provided cloud resources! Dedicated cloud resources in Microsoft Azure (sponsored by Microsoft) will be available only to registered tutorial attendees for working on the hands-on exercises during the the week of Jan 25-29 2021. Nothing in this tutorial, other than the section of using the provided Azure cloud resources , is specific to Azure. You can use your own resources if you prefer doing so.","title":"Practical information"},{"location":"#tutorial-contents","text":"0. Azure cloud resources 1. Introduction to CernVM-FS 2. Stratum 0 and client (*) 3. Stratum 1 and proxies (*) 4. Publishing (*) 5. Advanced topics (sections indicated with (*) involve hands-on exercises)","title":"Tutorial contents"},{"location":"#contributors","text":"Jakob Blomer (CERN, Switzerland) Bob Dr\u00f6ge (University of Groningen, The Netherlands) Kenneth Hoste (HPC-UGent, Belgium) Ryan Taylor (ComputeCanada)","title":"Contributors"},{"location":"#additional-resources","text":"CernVM-FS website: https://cernvm.cern.ch/fs CernVM-FS documentation: https://cvmfs.readthedocs.io CernVM-FS @ GitHub: https://github.com/cvmfs CernVM-FS tutorial @ GitHub: https://github.com/cvmfs-contrib/cvmfs-tutorial-2021","title":"Additional resources"},{"location":"00_azure_cloud_resources/","text":"0. Azure cloud resources \u00b6 details on accessing Azure VMs go here...","title":"0. Azure cloud resources"},{"location":"00_azure_cloud_resources/#0-azure-cloud-resources","text":"details on accessing Azure VMs go here...","title":"0. Azure cloud resources"},{"location":"01_introduction/","text":"1. Introduction to CernVM-FS \u00b6 1.1 What is CernVM-FS? \u00b6 Let's get started with explaining in detail what CernVM-FS is... CernVM-FS in a nutshell The CernVM File System (CernVM-FS) provides a scalable and reliable software distribution service, which is implemented as a read-only POSIX filesystem in user space (a FUSE module). Files and directories are hosted on standard web servers and mounted in the universal namespace /cvmfs . That's a mouthful, so let's break it down a bit... 1.1.1 Read-only filesystem over HTTP \u00b6 CernVM-FS is a network filesystem , which you can mount in Linux or macOS via FUSE (Filesystem in Userspace) and on Windows in a WSL2 virtualized Linux environment. In some ways it is similar to other network filesystems like NFS or AFS , but there are various aspects to it that are quite different. The files and directories that are made available via CernVM-FS are always located in a subdirectory of /cvmfs , and are provisioned via a network of servers that can basically be viewed as web servers since only outgoing HTTP connections are used. This makes it easy to use CernVM-FS in environments that are protected by a strict firewall. CernVM-FS is a read-only filesystem for those who access it; only those who administer it are able to add or change its contents. Internally, CernVM-FS uses content-addressable storage and Merkle trees in order to maintain file data and meta-data, but the filesystem it exposes is a standard POSIX filesystem . 1.1.2 Software distribution system \u00b6 The primary use case of CernVM-FS is to easily distribute software around the world, which is reflected in various ways in the features implemented by CernVM-FS. It's worth highlighting that with software we actually mean software installations , that is the files that collectively form a usable instance of an application, tool, or library. This is in contrast with software packages (for example, RPMs), which are essentially bundles of files wrapped together for easy distribution, and which need to be installed in order to provide a working instance of the provided software. Software installations have specific characteristics, such as often involving lots of small files which are being opened and read as a whole regularly, frequent searching for files in multiple directories, hierarchical structuring, etc. CernVM-FS is heavily tuned to cater to this use case, with aggressive caching and reduction of latency, for example via automatic file de-duplication and compression. 1.1.3 Scalable and reliable \u00b6 CernVM-FS was designed to be scalable and reliable , with known deployments involving hundreds of millions of files and many tens of thousands of clients. It was originally created to fulfill the software distribution needs of the experiments at the Large Hadron Collider (LHC) at CERN. The network of (web) servers that make a CernVM-FS instance accessible is constructed such that it is robust against problems like network disconnects and hardware failures, and so it can be extended and tweaked on demand for optimal performance. More details are available in the CernVM-FS documentation . 1.2 Terminology \u00b6 Before we get our hands dirty, let's cover some of the terminology used by the CernVM-FS project. The figure included below shows the different components of the CernVM-FS network: the central Stratum 0 server which hosts the filesystem; the Stratum 1 replica servers, and the associated proxies ; the client accessing the filesystem provided via CernVM-FS. 1.2.1 Clients \u00b6 A client in the context of CernVM-FS is any system that mounts the filesystem. This includes laptops or personal workstations who need access to the provided software installations, but also High-Performance Computing (HPC) clusters, virtual machines running in a cloud environment, etc. Clients only have read-only access to the files included in a CernVM-FS repository, and are automatically notified when the contents of the filesystem has changed. The filesystem that is mounted on a client (under /cvmfs ) is a virtual filesystem, in the sense that data is only (down)loaded when it is actually accessed (and cached aggressively to ensure good performance). Mounting a CernVM-FS repository on a client will be covered in the first hands-on part of this tutorial . Extensive documentation on configuring a client is available in the CernVM-FS documentation . 1.2.2 Stratum 0 + repository \u00b6 A CernVM-FS repository is an instance of a CernVM-FS filesystem. A repository is hosted on one Stratum 0 server, which is the single authoritative source of content for the repository. Multiple repositories can be hosted on the same Stratum 0 server. The data in a repository is stored using a content-addressable storage (CAS) scheme. All files written to a CernVM-FS repository must be converted into data chunks in the CAS store during the process of publishing , which involves creating catalogs which represent directory structure and metadata, and splitting files into chunks, compressing them, calculating content hashes, etc. Publishing is done on a dedicated release manager machine or publisher system which interfaces with the Stratum 0 server. Read-write access to a CernVM-FS repository is only available on a Stratum 0 server or publisher (the publisher and Stratum 0 can be the same system). Write access is provided via a union filesystem , which overlays a writable scratch area and the read-only mount of the CernVM-FS repository. Publishing is an atomic operation: adding or changing files in a repository is done in a transaction that records and collectively commits a set of file system changes, preventing partial or incomplete updates of the repository and ensuring that changes are either applied completely, or not at all. In the first hands-on part of this tutorial we will guide you through the process of creating a CernVM-FS repository, which is also covered in detail in the CernVM-FS documentation . The 3rd hands-on part of this tutorial will focus on the publishing procedure to update the contents of a CernVM-FS repository. 1.2.3 Stratum 1 replica servers \u00b6 A Stratum 1 replica server is a standard web server that provides a read-only mirror of a CernVM-FS repository served by a Stratum 0. The main purpose of a Stratum 1 is to improve reliability and capacity of the CernVM-FS network, by distributing load across multiple servers and allowing clients to fail over if one is unavailable, and to relieve the Stratum 0 from serving client requests. Although clients can access a CernVM-FS repository via the Stratum 0, it is advisable to block external client access to the Stratum 0 with a firewall, and instead rely on the Stratum 1 replica servers to provide client access. There usually are multiple Stratum 1 servers in a CernVM-FS network, which are typically distributed across geographic regions. A repository may be replicated to arbitrarily many Stratum 1 servers, but for reasons related to caching efficiency of HTTP proxies, it is best to use only a modest number of Stratum 1 servers (in the 5-10 range), not an excessive amount. While it depends on the specific context and circumstances under consideration, a reasonable rule of thumb would be approximately one Stratum 1 per continent for a deployment that is global in scope, and one Stratum 1 per geographic region of a country for a deployment that is national in scope. Stratum 1 servers enable clients to determine which Stratum 1 is geographically closest to connect to, via the Geo API which uses a GeoIP database that translates IP addresses of clients to an estimated longitude and latitude. Setting up a Stratum 1 replica server will be covered in the second hands-on part of this tutorial , and is also covered in detail in the CernVM-FS documentation . 1.2.4 Squid proxies \u00b6 To further extend the scalability and hierarchical caching model of CernVM-FS, another layer is used between end clients and Stratum 1 servers: forward caching HTTP proxies which reduce load on Stratum 1 servers and help reduce latency for clients. Squid cache is commonly used for this. A Squid proxy caches content that has been accessed recently, and helps to reduce bandwidth and improve response times. It is particularly important to have caching proxies at large systems like HPC clusters where many worker nodes are accessing the CernVM-FS repository, and it is recommended to set up multiple Squid proxies for redundancy and capacity. The second hands-on part of this tutorial will also cover setting up a Squid proxy. More details are available in the CernVM-FS documentation .","title":"1. Introduction to CernVM-FS"},{"location":"01_introduction/#1-introduction-to-cernvm-fs","text":"","title":"1. Introduction to CernVM-FS"},{"location":"01_introduction/#11-what-is-cernvm-fs","text":"Let's get started with explaining in detail what CernVM-FS is... CernVM-FS in a nutshell The CernVM File System (CernVM-FS) provides a scalable and reliable software distribution service, which is implemented as a read-only POSIX filesystem in user space (a FUSE module). Files and directories are hosted on standard web servers and mounted in the universal namespace /cvmfs . That's a mouthful, so let's break it down a bit...","title":"1.1 What is CernVM-FS?"},{"location":"01_introduction/#111-read-only-filesystem-over-http","text":"CernVM-FS is a network filesystem , which you can mount in Linux or macOS via FUSE (Filesystem in Userspace) and on Windows in a WSL2 virtualized Linux environment. In some ways it is similar to other network filesystems like NFS or AFS , but there are various aspects to it that are quite different. The files and directories that are made available via CernVM-FS are always located in a subdirectory of /cvmfs , and are provisioned via a network of servers that can basically be viewed as web servers since only outgoing HTTP connections are used. This makes it easy to use CernVM-FS in environments that are protected by a strict firewall. CernVM-FS is a read-only filesystem for those who access it; only those who administer it are able to add or change its contents. Internally, CernVM-FS uses content-addressable storage and Merkle trees in order to maintain file data and meta-data, but the filesystem it exposes is a standard POSIX filesystem .","title":"1.1.1 Read-only filesystem over HTTP"},{"location":"01_introduction/#112-software-distribution-system","text":"The primary use case of CernVM-FS is to easily distribute software around the world, which is reflected in various ways in the features implemented by CernVM-FS. It's worth highlighting that with software we actually mean software installations , that is the files that collectively form a usable instance of an application, tool, or library. This is in contrast with software packages (for example, RPMs), which are essentially bundles of files wrapped together for easy distribution, and which need to be installed in order to provide a working instance of the provided software. Software installations have specific characteristics, such as often involving lots of small files which are being opened and read as a whole regularly, frequent searching for files in multiple directories, hierarchical structuring, etc. CernVM-FS is heavily tuned to cater to this use case, with aggressive caching and reduction of latency, for example via automatic file de-duplication and compression.","title":"1.1.2 Software distribution system"},{"location":"01_introduction/#113-scalable-and-reliable","text":"CernVM-FS was designed to be scalable and reliable , with known deployments involving hundreds of millions of files and many tens of thousands of clients. It was originally created to fulfill the software distribution needs of the experiments at the Large Hadron Collider (LHC) at CERN. The network of (web) servers that make a CernVM-FS instance accessible is constructed such that it is robust against problems like network disconnects and hardware failures, and so it can be extended and tweaked on demand for optimal performance. More details are available in the CernVM-FS documentation .","title":"1.1.3 Scalable and reliable"},{"location":"01_introduction/#12-terminology","text":"Before we get our hands dirty, let's cover some of the terminology used by the CernVM-FS project. The figure included below shows the different components of the CernVM-FS network: the central Stratum 0 server which hosts the filesystem; the Stratum 1 replica servers, and the associated proxies ; the client accessing the filesystem provided via CernVM-FS.","title":"1.2 Terminology"},{"location":"01_introduction/#121-clients","text":"A client in the context of CernVM-FS is any system that mounts the filesystem. This includes laptops or personal workstations who need access to the provided software installations, but also High-Performance Computing (HPC) clusters, virtual machines running in a cloud environment, etc. Clients only have read-only access to the files included in a CernVM-FS repository, and are automatically notified when the contents of the filesystem has changed. The filesystem that is mounted on a client (under /cvmfs ) is a virtual filesystem, in the sense that data is only (down)loaded when it is actually accessed (and cached aggressively to ensure good performance). Mounting a CernVM-FS repository on a client will be covered in the first hands-on part of this tutorial . Extensive documentation on configuring a client is available in the CernVM-FS documentation .","title":"1.2.1 Clients"},{"location":"01_introduction/#122-stratum-0-repository","text":"A CernVM-FS repository is an instance of a CernVM-FS filesystem. A repository is hosted on one Stratum 0 server, which is the single authoritative source of content for the repository. Multiple repositories can be hosted on the same Stratum 0 server. The data in a repository is stored using a content-addressable storage (CAS) scheme. All files written to a CernVM-FS repository must be converted into data chunks in the CAS store during the process of publishing , which involves creating catalogs which represent directory structure and metadata, and splitting files into chunks, compressing them, calculating content hashes, etc. Publishing is done on a dedicated release manager machine or publisher system which interfaces with the Stratum 0 server. Read-write access to a CernVM-FS repository is only available on a Stratum 0 server or publisher (the publisher and Stratum 0 can be the same system). Write access is provided via a union filesystem , which overlays a writable scratch area and the read-only mount of the CernVM-FS repository. Publishing is an atomic operation: adding or changing files in a repository is done in a transaction that records and collectively commits a set of file system changes, preventing partial or incomplete updates of the repository and ensuring that changes are either applied completely, or not at all. In the first hands-on part of this tutorial we will guide you through the process of creating a CernVM-FS repository, which is also covered in detail in the CernVM-FS documentation . The 3rd hands-on part of this tutorial will focus on the publishing procedure to update the contents of a CernVM-FS repository.","title":"1.2.2 Stratum 0 + repository"},{"location":"01_introduction/#123-stratum-1-replica-servers","text":"A Stratum 1 replica server is a standard web server that provides a read-only mirror of a CernVM-FS repository served by a Stratum 0. The main purpose of a Stratum 1 is to improve reliability and capacity of the CernVM-FS network, by distributing load across multiple servers and allowing clients to fail over if one is unavailable, and to relieve the Stratum 0 from serving client requests. Although clients can access a CernVM-FS repository via the Stratum 0, it is advisable to block external client access to the Stratum 0 with a firewall, and instead rely on the Stratum 1 replica servers to provide client access. There usually are multiple Stratum 1 servers in a CernVM-FS network, which are typically distributed across geographic regions. A repository may be replicated to arbitrarily many Stratum 1 servers, but for reasons related to caching efficiency of HTTP proxies, it is best to use only a modest number of Stratum 1 servers (in the 5-10 range), not an excessive amount. While it depends on the specific context and circumstances under consideration, a reasonable rule of thumb would be approximately one Stratum 1 per continent for a deployment that is global in scope, and one Stratum 1 per geographic region of a country for a deployment that is national in scope. Stratum 1 servers enable clients to determine which Stratum 1 is geographically closest to connect to, via the Geo API which uses a GeoIP database that translates IP addresses of clients to an estimated longitude and latitude. Setting up a Stratum 1 replica server will be covered in the second hands-on part of this tutorial , and is also covered in detail in the CernVM-FS documentation .","title":"1.2.3 Stratum 1 replica servers"},{"location":"01_introduction/#124-squid-proxies","text":"To further extend the scalability and hierarchical caching model of CernVM-FS, another layer is used between end clients and Stratum 1 servers: forward caching HTTP proxies which reduce load on Stratum 1 servers and help reduce latency for clients. Squid cache is commonly used for this. A Squid proxy caches content that has been accessed recently, and helps to reduce bandwidth and improve response times. It is particularly important to have caching proxies at large systems like HPC clusters where many worker nodes are accessing the CernVM-FS repository, and it is recommended to set up multiple Squid proxies for redundancy and capacity. The second hands-on part of this tutorial will also cover setting up a Squid proxy. More details are available in the CernVM-FS documentation .","title":"1.2.4 Squid proxies"},{"location":"02_stratum0_client/","text":"2. Stratum 0 and client \u00b6 In order to get started with CernVM-FS, the first thing you need is a Stratum 0 server. The Stratum 0 is the central server that hosts your repositories and makes it available to other systems. There can be only one Stratum 0 server for each CernVM-FS repository , and from a security perspective it is recommended to restrict the access to this system. We will look more into that later. For now, we are going to set up a Stratum 0, create a repository, and access it by connecting from a client machine directly to the Stratum 0 server. Warning Directly connecting to the Stratum 0 is not recommended , but it is a good way to get started. In the next hands-on part of this tutorial we will remedy this by setting up a Stratum 1 replica server and squid proxy, and accessing the repository that way instead. 2.1 Setting up the Stratum 0 \u00b6 2.1.1 Requirements \u00b6 Resources \u00b6 Due to the scalable design of CernVM-FS, the Stratum 0 server does not need a lot of resources in terms of CPU cores and memory; just a few cores and a couple gigabytes of memory is sufficient. Besides this, you do need sufficient storage space to store the contents of your repository. CernVM-FS uses /var/spool/cvmfs as scratch space while adding new files to the repository, and /srv/cvmfs as central repository storage location. To change these locations, you can create either of the paths as a symbolic link to a different directory. Operating system \u00b6 Several (popular) Linux distributions are supported by CernVM-FS, see the Getting Started page of the CernVM-FS documentation for a full list. In this tutorial we will use CentOS 8 on x86_64 , but it should be relatively straightforward to use another OS or CPU architecture instead. CernVM-FS supports for hosting the repository contents in S3 compatible storage, but for this tutorial we will focus on storing the files locally on the Stratum 0 server. For this we need an Apache (web)server on the host, and port 80 must be open. 2.1.2 Installing CernVM-FS \u00b6 Installing CernVM-FS is simple and only requires some packages to be installed. You can easily do this by adding the CernVM-FS repository and install the packages through your package manager: # sudo yum install -y epel-release # only needed on CentOS 7 sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs cvmfs-server Alternatively, you can download the packages from the CernVM-FS downloads page and install them package manually. Note that you need both the client and server package installed on the Stratum 0 server. 2.1.3 Starting Apache webserver \u00b6 Since the Stratum 0 is serving contents via HTTP, the Apache service needs to be running before we can make a repository. The Apache ( httpd ) package should have been installed already, as it is a dependency of the cvmfs-server package, so the service can now be enabled (so that it always starts after a reboot) and started using: sudo systemctl enable httpd sudo systemctl start httpd 2.1.4 Creating the repository \u00b6 Now that all required packages have been installed, we can create a CernVM-FS repository. In the simplest way, this can be done by running the following command, which will make the current user the owner of the repository: sudo cvmfs_server mkfs -o $USER repo.organization.tld The full repository name (here repo.organization.tld ) resembles a DNS name, but the organization.tld domain does not actually need to exist. It is recommended to give all the repositories belonging to the same project or organization the same .organization.tld domain. This makes the client configuration much easier, also in case new repositories will be added later on. Warning Please come up with a proper name for your repository, including a domain you will remember throughout this tutorial. Do not use repo.organization.tld . For example, use repo.<your_first_name>.org , where you replace <your_first_name> with (you guessed it) your first name. Feel free to use something else than your first name, of course. 2.1.5 Repository keys \u00b6 For each repository that you create, a set of keys will be generated in /etc/cvmfs/keys : *.crt : the repository\u2019s public key (encoded as an X509 certificate); *.key : the repository's private key; *.masterkey : the repository's private master key; *.pub : repository\u2019s public master key (RSA). The public master key ( repo.organization.tld.pub ) is the one that is needed by clients in order to access the repository, so we will need this later on. The private master key ( repo.organization.tld.masterkey ) is used to sign a whitelist of known publisher certificates, and should not be shared with others. This whitelist is (by default) valid for 30 days, so the signing has to be done regularly (for example via a cron job). Although you can use a different master key per repository, it is recommended to use the same master key for all repositories under a single domain, so that clients only need a single public master key to access all repositories under this domain. For more information, see the CernVM-FS documentation: https://cvmfs.readthedocs.io/en/stable/cpt-repo.html#master-keys . 2.1.6 Adding files to the repository \u00b6 A new repository automatically gets a file named new_repository in its root ( /cvmfs/repo.organization.tld ). You can add more files by starting and publishing a transaction, which will be explained in more detail in the 3rd hands-on part of this tutorial . For now it is enough to just run the following commands to add a simple hello.sh script to your repository. First, start the transaction via cvmfs_server transaction <name_of_repo> : # Change this to your repository/domain name! MY_REPO_NAME = repo.organization.tld cvmfs_server transaction ${ MY_REPO_NAME } Next, add the file to the repository (in /cvmfs/${MY_REPO_NAME} ). If you made the current user the owner of the repository, you can do this without sudo since you have write permissions to your repository: echo '#!/bin/bash' > /cvmfs/ ${ MY_REPO_NAME } /hello.sh echo 'echo hello' >> /cvmfs/ ${ MY_REPO_NAME } /hello.sh chmod a+x /cvmfs/ ${ MY_REPO_NAME } /hello.sh Complete the transaction by publishing the changes using: cvmfs_server publish ${ MY_REPO_NAME } 2.1.7 Cron job for resigning the whitelist \u00b6 Each CernVM-FS repository has a whitelist containing fingerprints of certificates that are allowed to sign the repository. This whitelist has an expiration time of (by default) 30 days, so you regularly have to resign the whitelist. There are several ways to do this, see for instance the page about master keys in the CernVM-FS documentation. If you keep the master key on our Stratum 0 sever, you can set up a simple cronjob for resigning the whitelist. For instance, make a file /etc/cron.d/cvmfs_resign with the following content to do this every Monday at 11:00: 0 11 * * 1 root /usr/bin/cvmfs_server resign repo.organization.tld For the sake of this tutorial this is not really necessary of course, but it's an important aspect to be aware of. 2.1.8 Removing a repository \u00b6 An existing repository can be removed easily by running: sudo cvmfs_server rmfs repo.organization.tld Obviously you should only do this when you actually want to get rid of the repository... 2.2 Setting up a client \u00b6 Accessing a CernVM-FS repository on a client system involves three steps: installing the CernVM-FS client package; adding some configuration files for the repository you want to connect to; running the CernVM-FS setup procedure that will mount the repository. The client is going to pull in files from the repository over an HTTP connection. CernVM-FS maintains a local cache on the client, so you need sufficient space for storing it. You can define the maximum size of your cache in the client configuration. The larger your cache is, the less often you have to pull in files again, and the faster your applications will start. Typical client cache sizes range from 4GB to 50GB. Note that you can add more cache layers by adding a proxy nearby your client, which will be covered in the 2nd hands-on part of this tutorial . Note Make sure you use a different system (or virtual machine) for the client! It doesn't make much sense to install both the Stratum 0 server and the CernVM-FS client configuration on the same system... 2.2.1 Installing the client package \u00b6 The installation is the same as for the Stratum 0, except that you only need the cvmfs package (we don't need to CernVM-FS server component on the client): # sudo yum install -y epel-release # only needed on CentOS 7 sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs 2.2.2 Configuring the client \u00b6 Most organizations hosting CernVM-FS repositories offer a client package that you can install to do most of the configuration. For the sake of this tutorial, we are going to do this manually for our repository. All required configuration files will have to be stored under /etc/cvmfs . We will discuss them one by one, where we use repo.organization.tld as repository name, and hence organization.tld as domain. Don't forget to rename or change these files according to the repository name and domain you are using! Public key of repository \u00b6 /etc/cvmfs/keys/organization.tld/repo.organization.tld.pub This file contains the public key of the repository you want to access. You can copy this file from your Stratum 0 server, where it is stored under /etc/cvmfs/keys/ . Main repository configuration \u00b6 /etc/cvmfs/config.d/repo.organization.tld.conf This file contains the main configuration for the repository you want to access, which should minimally contain the URL(s) of the Stratum 1 servers and the location of the keys. We do not have a Stratum 1 server yet (we will set that up in the next hands-on part of this tutorial ), so we are going to connect directly to our Stratum 0 server instead. You should not do this in production! A typical, minimal configuration should look as follows: CVMFS_SERVER_URL=\"http://<STRATUM0_IP>/cvmfs/@fqrn@\" CVMFS_KEYS_DIR=\"/etc/cvmfs/keys/organization.tld\" Replace the <STRATUM0_IP> part with the IP address of your Stratum 0 server! Note that the CVMFS_SERVER_URL should include the part /cvmfs/@fqrn@ exactly like that; the last part ( @fqrn@ ) will be replaced automatically by the full name of your repository. Local client configuration \u00b6 /etc/cvmfs/default.local This file can be used for setting or overriding client configuration settings that are specific to your system. The CVMFS_HTTP_PROXY parameter is required : it should point to your local proxy that serves as a cache between your client(s) and the Stratum 1 server(s). Since we do not have a proxy yet, we are setting this to DIRECT , meaning that we connect directly to the Stratum 1 server (actually, Stratum 0 at this point): CVMFS_HTTP_PROXY=DIRECT You can also use this file to specify a maximum size (in megabytes) for the cache. For example, to use a local cache of maximum 5GB: CVMFS_QUOTA_LIMIT=5000 2.2.3 Mounting the repositories \u00b6 When your client configuration is complete, you can run the following command as root to mount the repository: sudo cvmfs_config setup This should not return any output or error messages. after). If you do run into a problem, check out the debugging section on the Advanced topics page . 2.2.4 Inspecting the repository \u00b6 Finally, we can try to access our repository on the client system. Note that CernVM-FS uses autofs , which means that you may not see the repository when you just run \" ls /cvmfs \". Your repository will only be actually mounted when you access it, and may be unmounted after not using it for a while. So, the following should work and show the contents of your repository: ls /cvmfs/repo.organization.tld Exercise \u00b6 Time to get your hands dirty! Try this yourself: Set up your own CernVM-FS repository on Stratum 0 server in a virtual machine. Create a repository with a suitable name (for example, exercise.<your_first_name>.org ). Add a simple bash script to the repository, which, for instance, prints Hello world! . Install and configure the CernVM-FS client on another virtual machine. Access your repository directly via Stratum 0 for now. Try to access your repository, and run your bash script on the client.","title":"2. Stratum 0 and client"},{"location":"02_stratum0_client/#2-stratum-0-and-client","text":"In order to get started with CernVM-FS, the first thing you need is a Stratum 0 server. The Stratum 0 is the central server that hosts your repositories and makes it available to other systems. There can be only one Stratum 0 server for each CernVM-FS repository , and from a security perspective it is recommended to restrict the access to this system. We will look more into that later. For now, we are going to set up a Stratum 0, create a repository, and access it by connecting from a client machine directly to the Stratum 0 server. Warning Directly connecting to the Stratum 0 is not recommended , but it is a good way to get started. In the next hands-on part of this tutorial we will remedy this by setting up a Stratum 1 replica server and squid proxy, and accessing the repository that way instead.","title":"2. Stratum 0 and client"},{"location":"02_stratum0_client/#21-setting-up-the-stratum-0","text":"","title":"2.1 Setting up the Stratum 0"},{"location":"02_stratum0_client/#211-requirements","text":"","title":"2.1.1 Requirements"},{"location":"02_stratum0_client/#resources","text":"Due to the scalable design of CernVM-FS, the Stratum 0 server does not need a lot of resources in terms of CPU cores and memory; just a few cores and a couple gigabytes of memory is sufficient. Besides this, you do need sufficient storage space to store the contents of your repository. CernVM-FS uses /var/spool/cvmfs as scratch space while adding new files to the repository, and /srv/cvmfs as central repository storage location. To change these locations, you can create either of the paths as a symbolic link to a different directory.","title":"Resources"},{"location":"02_stratum0_client/#operating-system","text":"Several (popular) Linux distributions are supported by CernVM-FS, see the Getting Started page of the CernVM-FS documentation for a full list. In this tutorial we will use CentOS 8 on x86_64 , but it should be relatively straightforward to use another OS or CPU architecture instead. CernVM-FS supports for hosting the repository contents in S3 compatible storage, but for this tutorial we will focus on storing the files locally on the Stratum 0 server. For this we need an Apache (web)server on the host, and port 80 must be open.","title":"Operating system"},{"location":"02_stratum0_client/#212-installing-cernvm-fs","text":"Installing CernVM-FS is simple and only requires some packages to be installed. You can easily do this by adding the CernVM-FS repository and install the packages through your package manager: # sudo yum install -y epel-release # only needed on CentOS 7 sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs cvmfs-server Alternatively, you can download the packages from the CernVM-FS downloads page and install them package manually. Note that you need both the client and server package installed on the Stratum 0 server.","title":"2.1.2 Installing CernVM-FS"},{"location":"02_stratum0_client/#213-starting-apache-webserver","text":"Since the Stratum 0 is serving contents via HTTP, the Apache service needs to be running before we can make a repository. The Apache ( httpd ) package should have been installed already, as it is a dependency of the cvmfs-server package, so the service can now be enabled (so that it always starts after a reboot) and started using: sudo systemctl enable httpd sudo systemctl start httpd","title":"2.1.3 Starting Apache webserver"},{"location":"02_stratum0_client/#214-creating-the-repository","text":"Now that all required packages have been installed, we can create a CernVM-FS repository. In the simplest way, this can be done by running the following command, which will make the current user the owner of the repository: sudo cvmfs_server mkfs -o $USER repo.organization.tld The full repository name (here repo.organization.tld ) resembles a DNS name, but the organization.tld domain does not actually need to exist. It is recommended to give all the repositories belonging to the same project or organization the same .organization.tld domain. This makes the client configuration much easier, also in case new repositories will be added later on. Warning Please come up with a proper name for your repository, including a domain you will remember throughout this tutorial. Do not use repo.organization.tld . For example, use repo.<your_first_name>.org , where you replace <your_first_name> with (you guessed it) your first name. Feel free to use something else than your first name, of course.","title":"2.1.4 Creating the repository"},{"location":"02_stratum0_client/#215-repository-keys","text":"For each repository that you create, a set of keys will be generated in /etc/cvmfs/keys : *.crt : the repository\u2019s public key (encoded as an X509 certificate); *.key : the repository's private key; *.masterkey : the repository's private master key; *.pub : repository\u2019s public master key (RSA). The public master key ( repo.organization.tld.pub ) is the one that is needed by clients in order to access the repository, so we will need this later on. The private master key ( repo.organization.tld.masterkey ) is used to sign a whitelist of known publisher certificates, and should not be shared with others. This whitelist is (by default) valid for 30 days, so the signing has to be done regularly (for example via a cron job). Although you can use a different master key per repository, it is recommended to use the same master key for all repositories under a single domain, so that clients only need a single public master key to access all repositories under this domain. For more information, see the CernVM-FS documentation: https://cvmfs.readthedocs.io/en/stable/cpt-repo.html#master-keys .","title":"2.1.5 Repository keys"},{"location":"02_stratum0_client/#216-adding-files-to-the-repository","text":"A new repository automatically gets a file named new_repository in its root ( /cvmfs/repo.organization.tld ). You can add more files by starting and publishing a transaction, which will be explained in more detail in the 3rd hands-on part of this tutorial . For now it is enough to just run the following commands to add a simple hello.sh script to your repository. First, start the transaction via cvmfs_server transaction <name_of_repo> : # Change this to your repository/domain name! MY_REPO_NAME = repo.organization.tld cvmfs_server transaction ${ MY_REPO_NAME } Next, add the file to the repository (in /cvmfs/${MY_REPO_NAME} ). If you made the current user the owner of the repository, you can do this without sudo since you have write permissions to your repository: echo '#!/bin/bash' > /cvmfs/ ${ MY_REPO_NAME } /hello.sh echo 'echo hello' >> /cvmfs/ ${ MY_REPO_NAME } /hello.sh chmod a+x /cvmfs/ ${ MY_REPO_NAME } /hello.sh Complete the transaction by publishing the changes using: cvmfs_server publish ${ MY_REPO_NAME }","title":"2.1.6 Adding files to the repository"},{"location":"02_stratum0_client/#217-cron-job-for-resigning-the-whitelist","text":"Each CernVM-FS repository has a whitelist containing fingerprints of certificates that are allowed to sign the repository. This whitelist has an expiration time of (by default) 30 days, so you regularly have to resign the whitelist. There are several ways to do this, see for instance the page about master keys in the CernVM-FS documentation. If you keep the master key on our Stratum 0 sever, you can set up a simple cronjob for resigning the whitelist. For instance, make a file /etc/cron.d/cvmfs_resign with the following content to do this every Monday at 11:00: 0 11 * * 1 root /usr/bin/cvmfs_server resign repo.organization.tld For the sake of this tutorial this is not really necessary of course, but it's an important aspect to be aware of.","title":"2.1.7 Cron job for resigning the whitelist"},{"location":"02_stratum0_client/#218-removing-a-repository","text":"An existing repository can be removed easily by running: sudo cvmfs_server rmfs repo.organization.tld Obviously you should only do this when you actually want to get rid of the repository...","title":"2.1.8 Removing a repository"},{"location":"02_stratum0_client/#22-setting-up-a-client","text":"Accessing a CernVM-FS repository on a client system involves three steps: installing the CernVM-FS client package; adding some configuration files for the repository you want to connect to; running the CernVM-FS setup procedure that will mount the repository. The client is going to pull in files from the repository over an HTTP connection. CernVM-FS maintains a local cache on the client, so you need sufficient space for storing it. You can define the maximum size of your cache in the client configuration. The larger your cache is, the less often you have to pull in files again, and the faster your applications will start. Typical client cache sizes range from 4GB to 50GB. Note that you can add more cache layers by adding a proxy nearby your client, which will be covered in the 2nd hands-on part of this tutorial . Note Make sure you use a different system (or virtual machine) for the client! It doesn't make much sense to install both the Stratum 0 server and the CernVM-FS client configuration on the same system...","title":"2.2 Setting up a client"},{"location":"02_stratum0_client/#221-installing-the-client-package","text":"The installation is the same as for the Stratum 0, except that you only need the cvmfs package (we don't need to CernVM-FS server component on the client): # sudo yum install -y epel-release # only needed on CentOS 7 sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs","title":"2.2.1 Installing the client package"},{"location":"02_stratum0_client/#222-configuring-the-client","text":"Most organizations hosting CernVM-FS repositories offer a client package that you can install to do most of the configuration. For the sake of this tutorial, we are going to do this manually for our repository. All required configuration files will have to be stored under /etc/cvmfs . We will discuss them one by one, where we use repo.organization.tld as repository name, and hence organization.tld as domain. Don't forget to rename or change these files according to the repository name and domain you are using!","title":"2.2.2 Configuring the client"},{"location":"02_stratum0_client/#public-key-of-repository","text":"/etc/cvmfs/keys/organization.tld/repo.organization.tld.pub This file contains the public key of the repository you want to access. You can copy this file from your Stratum 0 server, where it is stored under /etc/cvmfs/keys/ .","title":"Public key of repository"},{"location":"02_stratum0_client/#main-repository-configuration","text":"/etc/cvmfs/config.d/repo.organization.tld.conf This file contains the main configuration for the repository you want to access, which should minimally contain the URL(s) of the Stratum 1 servers and the location of the keys. We do not have a Stratum 1 server yet (we will set that up in the next hands-on part of this tutorial ), so we are going to connect directly to our Stratum 0 server instead. You should not do this in production! A typical, minimal configuration should look as follows: CVMFS_SERVER_URL=\"http://<STRATUM0_IP>/cvmfs/@fqrn@\" CVMFS_KEYS_DIR=\"/etc/cvmfs/keys/organization.tld\" Replace the <STRATUM0_IP> part with the IP address of your Stratum 0 server! Note that the CVMFS_SERVER_URL should include the part /cvmfs/@fqrn@ exactly like that; the last part ( @fqrn@ ) will be replaced automatically by the full name of your repository.","title":"Main repository configuration"},{"location":"02_stratum0_client/#local-client-configuration","text":"/etc/cvmfs/default.local This file can be used for setting or overriding client configuration settings that are specific to your system. The CVMFS_HTTP_PROXY parameter is required : it should point to your local proxy that serves as a cache between your client(s) and the Stratum 1 server(s). Since we do not have a proxy yet, we are setting this to DIRECT , meaning that we connect directly to the Stratum 1 server (actually, Stratum 0 at this point): CVMFS_HTTP_PROXY=DIRECT You can also use this file to specify a maximum size (in megabytes) for the cache. For example, to use a local cache of maximum 5GB: CVMFS_QUOTA_LIMIT=5000","title":"Local client configuration"},{"location":"02_stratum0_client/#223-mounting-the-repositories","text":"When your client configuration is complete, you can run the following command as root to mount the repository: sudo cvmfs_config setup This should not return any output or error messages. after). If you do run into a problem, check out the debugging section on the Advanced topics page .","title":"2.2.3 Mounting the repositories"},{"location":"02_stratum0_client/#224-inspecting-the-repository","text":"Finally, we can try to access our repository on the client system. Note that CernVM-FS uses autofs , which means that you may not see the repository when you just run \" ls /cvmfs \". Your repository will only be actually mounted when you access it, and may be unmounted after not using it for a while. So, the following should work and show the contents of your repository: ls /cvmfs/repo.organization.tld","title":"2.2.4 Inspecting the repository"},{"location":"02_stratum0_client/#exercise","text":"Time to get your hands dirty! Try this yourself: Set up your own CernVM-FS repository on Stratum 0 server in a virtual machine. Create a repository with a suitable name (for example, exercise.<your_first_name>.org ). Add a simple bash script to the repository, which, for instance, prints Hello world! . Install and configure the CernVM-FS client on another virtual machine. Access your repository directly via Stratum 0 for now. Try to access your repository, and run your bash script on the client.","title":"Exercise"},{"location":"03_stratum1_proxies/","text":"3. Stratum 1 and proxies \u00b6 In the previous section we have set up a Stratum 0 server and a client that directly connects to the Stratum 0. Although this worked fine this is not recommended , since this setup is neither scalable, nor very reliable, nor secure: it is a single point of failure, too open in terms of connectivity, and it will have to serve all clients on its own. Therefore, we will show how all these points can be addressed by adding a Stratum 1 server and a caching proxy server . Quick reminder: a Stratum 1 is a replica server that keeps mirrors of the repositories served by a Stratum 0. It is a web server that periodically synchronizes the contents of the repositories. In contrast to the central Stratum 0 server you can have multiple Stratum 1 servers, and it is recommended to have them geographically distributed, so that clients always have a nearby Stratum 1. How many Stratum 1 servers you need mostly depends on the number of clients and how they are distributed geographically, but often a few is already sufficient. Scalability and performance can be improved with proxies, which we will discuss later in this section. 3.1 Setting up the Stratum 1 server \u00b6 3.1.1 Requirements \u00b6 A Stratum 1 server has similar requirements as a Stratum 0 in terms of resources . In addition to port 80 (for the Apache web server), port 8000 also has to be accessible for a Stratum 1 (for the Squid proxy frontend). Furthermore, you need a (free) license key for Maxmind's Geo API , which you can obtain by signing up for an account . This is used by CernVM-FS to allow clients to determine which Stratum 1 server is geographically located closest. 3.1.2 Installation \u00b6 For the Stratum 1 you need to install the following packages: # sudo yum install -y epel-release # only needed on CentOS 7 sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs-server squid # sudo yum install -y mod_wsgi # on CentOS 7 sudo yum install -y python3-mod_wsgi # on CenOS 8 This is: cvmfs-server : the CermVM-FS server package (just like for Stratum 0); mod_wsgi : an Apache module that provides a WSGI compliant interface for hosting Python based web applications within Apache, which is required by CernVM-FS to query the Geo API (more on that later); squid : the Squid proxy package; 3.1.3 Configuring Apache and Squid proxy \u00b6 On the Stratum 1, we will be running Apache with a Squid frontend (reverse proxy). The Apache web server will be listening internally on port 8080, while the Squid proxy needs to listen (externally) on port 80 and 8000, which are the default Stratum 1 ports. Apache configuration \u00b6 First, we modify the Apache webserver configuration, by editing /etc/httpd/conf/httpd.conf and change the default: Listen 80 to: Listen 127.0.0.1:8080 Squid configuration \u00b6 Next, we replace the default contents of /etc/squid/squid.conf with the following: http_port 80 accel http_port 8000 accel http_access allow all cache_peer 127.0.0.1 parent 8080 0 no-query originserver acl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/ cache deny !CVMFSAPI cache_mem 128 MB To clarify: http_port specifies on which ports Squid will listen for HTTP requests; http_access specifies access restrictions for HTTP traffic (none, in this case); cache_peer specifies that the Apache web server is listening on port 8080; acl specifies the access list for CernVM-FS: only paths under /cvmfs/*/api/ are relevant; cache specifies which paths should be cached by the Squid proxy (only paths that match the regular expression on the line above); cache_mem specifies the amount of memory that Squid is allowed to use; For more information, see the Squid documentation: http://www.squid-cache.org/Doc/config/ . Start & enable services \u00b6 Finally, we start and enable both the Apache and Squid services: sudo systemctl start httpd sudo systemctl start squid sudo systemctl enable httpd sudo systemctl enable squid 3.1.4 DNS cache \u00b6 As a Stratum 1 server does a lot of DNS lookups, it is recommended to have a local DNS caching server on that same system. We will not discuss this topic any further here, but you can use dnsmasq , bind , or systemd-resolved . See for instance this tutorial for setting up systemd-resolved . 3.1.5 Creating the Stratum 1 replica \u00b6 With all the required components in place, we can now really set up our Stratum 1 replica server. Create and add Geo API key (optional) \u00b6 We first add our Geo API key to the CernVM-FS server settings, by creating it and then running these commands: echo 'CVMFS_GEO_LICENSE_KEY=YOUR_KEY' | sudo tee -a /etc/cvmfs/server.local sudo chmod 600 /etc/cvmfs/server.local Replace YOUR_KEY with your Geo API license key; see https://www.maxmind.com/en/accounts/YOUR_ACCOUNT_ID/license-key ! Note that this is not strictly required for the sake of this tutorial, but it's highly recommended. Add repository master public key \u00b6 We also need to have the public master key of each repository we want to mirror to be available on our Stratum 1. This can be done by copying the .pub file(s) from /etc/cvmfs/keys on the Stratum 0 server to /etc/cvmfs/keys/organization.tld/ (note the extra level!) on the Stratum 1 server, just like we did on the client. Create replica \u00b6 Now we make the replica by giving the URL to the repository on the Stratum 0 server (which is always like http://host:port/cvmfs/repository , with the :port part optional) and the path to the corresponding public master key: sudo cvmfs_server add-replica -o $USER http://<STRATUM0_IP>/cvmfs/repo.organization.tld /etc/cvmfs/keys/organization.tld/ Replace the <STRATUM0_IP> part with the IP address of your Stratum 0 server, and adjust for the name and domain of your CernVM-FS repository! Executing the add-replica command should produce output reporting on the steps being performed, and only take a couple of moments to complete. If no output is produced and the command seems to be hanging, make sure that port 80 on your Stratum 0 server is accessible via the IP address you are using! Bypassing the Geo API license key \u00b6 If you prefer not to create a MaxMind account and Geo API license key for the sake of this tutorial, you can bypass the \" CVMFS_GEO_LICENSE_KEY not set \" error message produced by cvmfs_server add-replica by setting the server variable CVMFS_GEO_DB_FILE to NONE before running the command: # only do this if you do not want to provide a Geo API key (not recommended!) echo 'CVMFS_GEO_DB_FILE=NONE' | sudo tee -a /etc/cvmfs/server.local Remove the replica \u00b6 If you ever want to remove the repository replica again, you can use the rmfs subcommand in the same way as on Stratum 0: sudo cvmfs_server rmfs repo.organization.tld 3.1.6 Manually synchronize the Stratum 1 \u00b6 Now that the Stratum 1 has been registered, we should try to do a first synchronization. You can do this by running the following command: sudo cvmfs_server snapshot repo.organization.tld As there is not much in the repository yet, this should complete within a few seconds. The output should end with something like: Serving revision 2 Fetched 2 new chunks out of 3 processed chunks 3.1.7 Adding a synchronization cron job \u00b6 Whenever you make changes to the repository, the changes have to be synchronized to all Stratum 1 servers. This task can be automated by setting up a cron job that periodically runs cvmfs_server snapshot -a , where -a does the synchronization for all active repositories. This option will give an error if no log rotation has been configured for CernVM-FS, so we first have to create a file /etc/logrotate.d/cvmfs with the following contents: /var/log/cvmfs/*.log { weekly missingok notifempty } Now we can make a cron job /etc/cron.d/cvmfs_stratum1_snapshot for the snapshots: */5 * * * * root output=$(/usr/bin/cvmfs_server snapshot -a -i 2>&1) || echo \"$output\" 3.2 Setting up a proxy \u00b6 If you have a lot of local machines, e.g. an HPC cluster, that need to access your repositories, you also want another cache layer close to these machines. This can be done by adding one or more Squid proxies between your local machine(s) and the Stratum 1 server(s). It is recommended to have at least two proxies, for reliability and load-balancing reasons. 3.2.1 Requirements \u00b6 Just as with the other components, the Squid proxy server does not need a lot of resources. Just a few cores and few gigabytes of memory should be enough. The more disk space you allocate for this machine, the larger the cache can be, and the better the performance will be. Note that this system will only store a part of the (deduplicated and compressed) repository, so it does not need as much storage space as Stratum 0 or Stratum 1 server. 3.2.2 Installation \u00b6 On the proxy server only Squid needs to be installed: sudo yum install -y squid 3.2.3 Configuration \u00b6 The configuration of a standalone Squid proxy is slightly different from the one that we used for our Stratum 1. You can use the following template to set up your own Squid configuration for your proxy server (in /etc/squid/squid.conf ): # List of local IP addresses (separate IPs and/or CIDR notation) allowed to access your local proxy acl local_nodes src YOUR_CLIENT_IPS # Destination domains that are allowed #acl stratum_ones dstdomain .YOURDOMAIN.ORG #acl stratum_ones dstdom_regex YOUR_REGEX # Squid port http_port 3128 # Deny access to anything which is not part of our stratum_ones ACL. http_access deny !stratum_ones # Only allow access from our local machines http_access allow local_nodes http_access allow localhost # Finally, deny all other access to this proxy http_access deny all minimum_expiry_time 0 maximum_object_size 1024 MB cache_mem 128 MB maximum_object_size_in_memory 128 KB # 5 GB disk cache cache_dir ufs /var/spool/squid 5000 16 256 In this template, there are a two things you must change in the Access Control List (ACL) settings: The line starting with acl local_nodes specifies which clients are allowed to use this proxy. You can use CIDR notation . For the sake of this tutorial, you can just replace YOUR_CLIENT_IPS with the IP of your client system. For example: acl local_nodes src 1.2.3.4 You will need to add a line starting with acl stratum_ones to specify an ACL for the allowed destination domains. For the sake of this tutorial, we can just \"hardcode\" this to our Stratum 1 server via dst (destination): acl stratum_ones dst <STRATUM1_IP> (where you need to change <STRATUM1_IP> with the IP address of your Stratum 1 server) The stratum_ones ACL you defined is used to specify that the Squid should only cache the Stratum 1 server, via the first http_access deny line. More information about Squid ACLs can be found in the Squid documentation . Finally, there are some settings regarding the size of your cache. Make sure that you have enough disk space for the value that you provide. 3.2.4 Verifying Squid configuration \u00b6 To verify the correctness of your Squid configuration, you can run: sudo squid -k parse When things look OK (no errors or warnings are printed, exit code zero), you can start and enable Squid. 3.2.5 Starting and enabling Squid \u00b6 To start and enable Squid, run: sudo systemctl start squid sudo systemctl enable squid 3.3 Re-configuring the client \u00b6 Now that we have a Stratum 0 server, a Stratum 1 server, and a Squid proxy, we have the (minimal) infrastructure in place for a production-ready CernVM-FS setup. So we can now configure our client properly, and start using the repository. In the previous section we connected our client directly to the Stratum 0. We are going to reuse that configuration, and only need to change two things. 3.3.1 Connect to the Stratum 1 \u00b6 We used the URL of the Stratum 0 in the file /etc/cvmfs/config.d/repo.organization.tld.conf . We should now change this URL, and point to the Stratum 1 instead: CVMFS_SERVER_URL=\"http://<STRATUM1_IP>/cvmfs/@fqrn@\" Replace the <STRATUM1_IP> part with the IP address of your Stratum 1 server! When you have more Stratum 1 servers inside the organization, you can make it a semicolon-separated list of servers. The Geo API will make sure that your client always connects to the geographically closest Stratum 1 server. 3.3.2 Use the Squid proxy \u00b6 In order to use the local cache layer of our proxy, we have to instruct the client to send all requests through the proxy. This needs one small change in /etc/cvmfs/default.local , where you will have to replace DIRECT with IP address of your Squid proxy service, plus the (default) port 3128 at which Squid is running: CVMFS_HTTP_PROXY=\"http://<PROXY_IP>:3128\" Replace the <PROXY_IP> part with the IP address of your Squid proxy server! After changing the local configuration file, make sure to reload the CernVM-FS client configuration: sudo cvmfs_config reload repo.organization.tld More proxies can be added to that list by separating them with a pipe symbol. For more (complex) examples, see the CernVM-FS documentation . 3.3.3 Test the new configuration \u00b6 Now you can test your new configuration by checking if you can still access the repository. Furthermore, you may want to check if you are really using your Squid proxy. You can do this by running: cvmfs_config stat -v repo.organization.tld This should show a line that looks like: Connection: http://<STRATUM1_IP>/cvmfs/repo.organization.tld through proxy http://<PROXY_IP>:3128 (online) Make sure that it lists your proxy here (and not DIRECT ), and that it is marked as online . Exercise \u00b6 1) Set up a Stratum 1 server. Make sure that it includes: - a proper Geo API license key (if you do not want to request an account, you can use the described method to bypass this, but again: do not do this in production!); - cron job for automatically synchronizing the database; - properly configured Apache and Squid services; 2) Set up a separate Squid proxy. Though it is recommended to at least have two in production, one is enough for now. 3) Reconfigure the client that you set up in the previous section and make sure that it uses your Stratum 1 and Squid proxy.","title":"3. Stratum 1 and proxies"},{"location":"03_stratum1_proxies/#3-stratum-1-and-proxies","text":"In the previous section we have set up a Stratum 0 server and a client that directly connects to the Stratum 0. Although this worked fine this is not recommended , since this setup is neither scalable, nor very reliable, nor secure: it is a single point of failure, too open in terms of connectivity, and it will have to serve all clients on its own. Therefore, we will show how all these points can be addressed by adding a Stratum 1 server and a caching proxy server . Quick reminder: a Stratum 1 is a replica server that keeps mirrors of the repositories served by a Stratum 0. It is a web server that periodically synchronizes the contents of the repositories. In contrast to the central Stratum 0 server you can have multiple Stratum 1 servers, and it is recommended to have them geographically distributed, so that clients always have a nearby Stratum 1. How many Stratum 1 servers you need mostly depends on the number of clients and how they are distributed geographically, but often a few is already sufficient. Scalability and performance can be improved with proxies, which we will discuss later in this section.","title":"3. Stratum 1 and proxies"},{"location":"03_stratum1_proxies/#31-setting-up-the-stratum-1-server","text":"","title":"3.1 Setting up the Stratum 1 server"},{"location":"03_stratum1_proxies/#311-requirements","text":"A Stratum 1 server has similar requirements as a Stratum 0 in terms of resources . In addition to port 80 (for the Apache web server), port 8000 also has to be accessible for a Stratum 1 (for the Squid proxy frontend). Furthermore, you need a (free) license key for Maxmind's Geo API , which you can obtain by signing up for an account . This is used by CernVM-FS to allow clients to determine which Stratum 1 server is geographically located closest.","title":"3.1.1 Requirements"},{"location":"03_stratum1_proxies/#312-installation","text":"For the Stratum 1 you need to install the following packages: # sudo yum install -y epel-release # only needed on CentOS 7 sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs-server squid # sudo yum install -y mod_wsgi # on CentOS 7 sudo yum install -y python3-mod_wsgi # on CenOS 8 This is: cvmfs-server : the CermVM-FS server package (just like for Stratum 0); mod_wsgi : an Apache module that provides a WSGI compliant interface for hosting Python based web applications within Apache, which is required by CernVM-FS to query the Geo API (more on that later); squid : the Squid proxy package;","title":"3.1.2 Installation"},{"location":"03_stratum1_proxies/#313-configuring-apache-and-squid-proxy","text":"On the Stratum 1, we will be running Apache with a Squid frontend (reverse proxy). The Apache web server will be listening internally on port 8080, while the Squid proxy needs to listen (externally) on port 80 and 8000, which are the default Stratum 1 ports.","title":"3.1.3 Configuring Apache and Squid proxy"},{"location":"03_stratum1_proxies/#apache-configuration","text":"First, we modify the Apache webserver configuration, by editing /etc/httpd/conf/httpd.conf and change the default: Listen 80 to: Listen 127.0.0.1:8080","title":"Apache configuration"},{"location":"03_stratum1_proxies/#squid-configuration","text":"Next, we replace the default contents of /etc/squid/squid.conf with the following: http_port 80 accel http_port 8000 accel http_access allow all cache_peer 127.0.0.1 parent 8080 0 no-query originserver acl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/ cache deny !CVMFSAPI cache_mem 128 MB To clarify: http_port specifies on which ports Squid will listen for HTTP requests; http_access specifies access restrictions for HTTP traffic (none, in this case); cache_peer specifies that the Apache web server is listening on port 8080; acl specifies the access list for CernVM-FS: only paths under /cvmfs/*/api/ are relevant; cache specifies which paths should be cached by the Squid proxy (only paths that match the regular expression on the line above); cache_mem specifies the amount of memory that Squid is allowed to use; For more information, see the Squid documentation: http://www.squid-cache.org/Doc/config/ .","title":"Squid configuration"},{"location":"03_stratum1_proxies/#start-enable-services","text":"Finally, we start and enable both the Apache and Squid services: sudo systemctl start httpd sudo systemctl start squid sudo systemctl enable httpd sudo systemctl enable squid","title":"Start &amp; enable services"},{"location":"03_stratum1_proxies/#314-dns-cache","text":"As a Stratum 1 server does a lot of DNS lookups, it is recommended to have a local DNS caching server on that same system. We will not discuss this topic any further here, but you can use dnsmasq , bind , or systemd-resolved . See for instance this tutorial for setting up systemd-resolved .","title":"3.1.4 DNS cache"},{"location":"03_stratum1_proxies/#315-creating-the-stratum-1-replica","text":"With all the required components in place, we can now really set up our Stratum 1 replica server.","title":"3.1.5 Creating the Stratum 1 replica"},{"location":"03_stratum1_proxies/#create-and-add-geo-api-key-optional","text":"We first add our Geo API key to the CernVM-FS server settings, by creating it and then running these commands: echo 'CVMFS_GEO_LICENSE_KEY=YOUR_KEY' | sudo tee -a /etc/cvmfs/server.local sudo chmod 600 /etc/cvmfs/server.local Replace YOUR_KEY with your Geo API license key; see https://www.maxmind.com/en/accounts/YOUR_ACCOUNT_ID/license-key ! Note that this is not strictly required for the sake of this tutorial, but it's highly recommended.","title":"Create and add Geo API key (optional)"},{"location":"03_stratum1_proxies/#add-repository-master-public-key","text":"We also need to have the public master key of each repository we want to mirror to be available on our Stratum 1. This can be done by copying the .pub file(s) from /etc/cvmfs/keys on the Stratum 0 server to /etc/cvmfs/keys/organization.tld/ (note the extra level!) on the Stratum 1 server, just like we did on the client.","title":"Add repository master public key"},{"location":"03_stratum1_proxies/#create-replica","text":"Now we make the replica by giving the URL to the repository on the Stratum 0 server (which is always like http://host:port/cvmfs/repository , with the :port part optional) and the path to the corresponding public master key: sudo cvmfs_server add-replica -o $USER http://<STRATUM0_IP>/cvmfs/repo.organization.tld /etc/cvmfs/keys/organization.tld/ Replace the <STRATUM0_IP> part with the IP address of your Stratum 0 server, and adjust for the name and domain of your CernVM-FS repository! Executing the add-replica command should produce output reporting on the steps being performed, and only take a couple of moments to complete. If no output is produced and the command seems to be hanging, make sure that port 80 on your Stratum 0 server is accessible via the IP address you are using!","title":"Create replica"},{"location":"03_stratum1_proxies/#bypassing-the-geo-api-license-key","text":"If you prefer not to create a MaxMind account and Geo API license key for the sake of this tutorial, you can bypass the \" CVMFS_GEO_LICENSE_KEY not set \" error message produced by cvmfs_server add-replica by setting the server variable CVMFS_GEO_DB_FILE to NONE before running the command: # only do this if you do not want to provide a Geo API key (not recommended!) echo 'CVMFS_GEO_DB_FILE=NONE' | sudo tee -a /etc/cvmfs/server.local","title":"Bypassing the Geo API license key"},{"location":"03_stratum1_proxies/#remove-the-replica","text":"If you ever want to remove the repository replica again, you can use the rmfs subcommand in the same way as on Stratum 0: sudo cvmfs_server rmfs repo.organization.tld","title":"Remove the replica"},{"location":"03_stratum1_proxies/#316-manually-synchronize-the-stratum-1","text":"Now that the Stratum 1 has been registered, we should try to do a first synchronization. You can do this by running the following command: sudo cvmfs_server snapshot repo.organization.tld As there is not much in the repository yet, this should complete within a few seconds. The output should end with something like: Serving revision 2 Fetched 2 new chunks out of 3 processed chunks","title":"3.1.6 Manually synchronize the Stratum 1"},{"location":"03_stratum1_proxies/#317-adding-a-synchronization-cron-job","text":"Whenever you make changes to the repository, the changes have to be synchronized to all Stratum 1 servers. This task can be automated by setting up a cron job that periodically runs cvmfs_server snapshot -a , where -a does the synchronization for all active repositories. This option will give an error if no log rotation has been configured for CernVM-FS, so we first have to create a file /etc/logrotate.d/cvmfs with the following contents: /var/log/cvmfs/*.log { weekly missingok notifempty } Now we can make a cron job /etc/cron.d/cvmfs_stratum1_snapshot for the snapshots: */5 * * * * root output=$(/usr/bin/cvmfs_server snapshot -a -i 2>&1) || echo \"$output\"","title":"3.1.7 Adding a synchronization cron job"},{"location":"03_stratum1_proxies/#32-setting-up-a-proxy","text":"If you have a lot of local machines, e.g. an HPC cluster, that need to access your repositories, you also want another cache layer close to these machines. This can be done by adding one or more Squid proxies between your local machine(s) and the Stratum 1 server(s). It is recommended to have at least two proxies, for reliability and load-balancing reasons.","title":"3.2 Setting up a proxy"},{"location":"03_stratum1_proxies/#321-requirements","text":"Just as with the other components, the Squid proxy server does not need a lot of resources. Just a few cores and few gigabytes of memory should be enough. The more disk space you allocate for this machine, the larger the cache can be, and the better the performance will be. Note that this system will only store a part of the (deduplicated and compressed) repository, so it does not need as much storage space as Stratum 0 or Stratum 1 server.","title":"3.2.1 Requirements"},{"location":"03_stratum1_proxies/#322-installation","text":"On the proxy server only Squid needs to be installed: sudo yum install -y squid","title":"3.2.2 Installation"},{"location":"03_stratum1_proxies/#323-configuration","text":"The configuration of a standalone Squid proxy is slightly different from the one that we used for our Stratum 1. You can use the following template to set up your own Squid configuration for your proxy server (in /etc/squid/squid.conf ): # List of local IP addresses (separate IPs and/or CIDR notation) allowed to access your local proxy acl local_nodes src YOUR_CLIENT_IPS # Destination domains that are allowed #acl stratum_ones dstdomain .YOURDOMAIN.ORG #acl stratum_ones dstdom_regex YOUR_REGEX # Squid port http_port 3128 # Deny access to anything which is not part of our stratum_ones ACL. http_access deny !stratum_ones # Only allow access from our local machines http_access allow local_nodes http_access allow localhost # Finally, deny all other access to this proxy http_access deny all minimum_expiry_time 0 maximum_object_size 1024 MB cache_mem 128 MB maximum_object_size_in_memory 128 KB # 5 GB disk cache cache_dir ufs /var/spool/squid 5000 16 256 In this template, there are a two things you must change in the Access Control List (ACL) settings: The line starting with acl local_nodes specifies which clients are allowed to use this proxy. You can use CIDR notation . For the sake of this tutorial, you can just replace YOUR_CLIENT_IPS with the IP of your client system. For example: acl local_nodes src 1.2.3.4 You will need to add a line starting with acl stratum_ones to specify an ACL for the allowed destination domains. For the sake of this tutorial, we can just \"hardcode\" this to our Stratum 1 server via dst (destination): acl stratum_ones dst <STRATUM1_IP> (where you need to change <STRATUM1_IP> with the IP address of your Stratum 1 server) The stratum_ones ACL you defined is used to specify that the Squid should only cache the Stratum 1 server, via the first http_access deny line. More information about Squid ACLs can be found in the Squid documentation . Finally, there are some settings regarding the size of your cache. Make sure that you have enough disk space for the value that you provide.","title":"3.2.3 Configuration"},{"location":"03_stratum1_proxies/#324-verifying-squid-configuration","text":"To verify the correctness of your Squid configuration, you can run: sudo squid -k parse When things look OK (no errors or warnings are printed, exit code zero), you can start and enable Squid.","title":"3.2.4 Verifying Squid configuration"},{"location":"03_stratum1_proxies/#325-starting-and-enabling-squid","text":"To start and enable Squid, run: sudo systemctl start squid sudo systemctl enable squid","title":"3.2.5 Starting and enabling Squid"},{"location":"03_stratum1_proxies/#33-re-configuring-the-client","text":"Now that we have a Stratum 0 server, a Stratum 1 server, and a Squid proxy, we have the (minimal) infrastructure in place for a production-ready CernVM-FS setup. So we can now configure our client properly, and start using the repository. In the previous section we connected our client directly to the Stratum 0. We are going to reuse that configuration, and only need to change two things.","title":"3.3 Re-configuring the client"},{"location":"03_stratum1_proxies/#331-connect-to-the-stratum-1","text":"We used the URL of the Stratum 0 in the file /etc/cvmfs/config.d/repo.organization.tld.conf . We should now change this URL, and point to the Stratum 1 instead: CVMFS_SERVER_URL=\"http://<STRATUM1_IP>/cvmfs/@fqrn@\" Replace the <STRATUM1_IP> part with the IP address of your Stratum 1 server! When you have more Stratum 1 servers inside the organization, you can make it a semicolon-separated list of servers. The Geo API will make sure that your client always connects to the geographically closest Stratum 1 server.","title":"3.3.1 Connect to the Stratum 1"},{"location":"03_stratum1_proxies/#332-use-the-squid-proxy","text":"In order to use the local cache layer of our proxy, we have to instruct the client to send all requests through the proxy. This needs one small change in /etc/cvmfs/default.local , where you will have to replace DIRECT with IP address of your Squid proxy service, plus the (default) port 3128 at which Squid is running: CVMFS_HTTP_PROXY=\"http://<PROXY_IP>:3128\" Replace the <PROXY_IP> part with the IP address of your Squid proxy server! After changing the local configuration file, make sure to reload the CernVM-FS client configuration: sudo cvmfs_config reload repo.organization.tld More proxies can be added to that list by separating them with a pipe symbol. For more (complex) examples, see the CernVM-FS documentation .","title":"3.3.2 Use the Squid proxy"},{"location":"03_stratum1_proxies/#333-test-the-new-configuration","text":"Now you can test your new configuration by checking if you can still access the repository. Furthermore, you may want to check if you are really using your Squid proxy. You can do this by running: cvmfs_config stat -v repo.organization.tld This should show a line that looks like: Connection: http://<STRATUM1_IP>/cvmfs/repo.organization.tld through proxy http://<PROXY_IP>:3128 (online) Make sure that it lists your proxy here (and not DIRECT ), and that it is marked as online .","title":"3.3.3 Test the new configuration"},{"location":"03_stratum1_proxies/#exercise","text":"1) Set up a Stratum 1 server. Make sure that it includes: - a proper Geo API license key (if you do not want to request an account, you can use the described method to bypass this, but again: do not do this in production!); - cron job for automatically synchronizing the database; - properly configured Apache and Squid services; 2) Set up a separate Squid proxy. Though it is recommended to at least have two in production, one is enough for now. 3) Reconfigure the client that you set up in the previous section and make sure that it uses your Stratum 1 and Squid proxy.","title":"Exercise"},{"location":"04_publishing/","text":"Publishing \u00b6 The previous sections were mostly about setting up the infrastructure. Now that all the components for hosting and accessing your own CernVM-FS repositories are in place, it is time to really start using it. In this section we will give some more details about publishing files. Transactions \u00b6 As we already showed in a previous section, the easiest way to add files to your repository is by opening and publishing a transaction on your Stratum 0 server. By default, your repository directory under /cvmfs is read-only, but by a transaction makes the directory writable for the user that is owner of the repository. cvmfs_server transaction repo.organization.tld Once you are done with making changes, be sure to change your working directory to somewhere outside of the repository (otherwise you will get an error), and publish your changes using: cvmfs_server publish repo.organization.tld And you can always abort a transaction, which will undo all the non-published modifications: cvmfs_server abort repo.organization.tld Ingesting tarballs \u00b6 When you need to compile software that you want to add to your repository, you may want to do the actual compilation on a different machine than your Stratum 0 and copy the resulting installation as a tarball to your Stratum 0. Instead of manually extracting the tarball and doing a transaction, the cvmfs_server command offers a more efficient method for directly publishing the contents of a tarball: cvmfs_server ingest -b /some/path repo.organization.tld -t mytarball.tar The -b expects the relative location in your repository where the contents of the tarball, specified with -t , will be extracted. So, in this case, the tarball gets extracted to /cvmfs/repo.organization.tld/some/path . Note that passing / to -b does not work at the moment, but will be supported in a future release of CernVM-FS. In case you have a compressed tarball, you can use an appropriate decompression tool and write the output to stdout . This output can then be piped to cvmfs_server by setting -t - , e.g. for a .tar.gz file: gunzip -c mytarball.tar.gz | cvmfs_server ingest -b /some/path -t - Tags \u00b6 By default, a newly published version of the repository will automatically get a tag with a timestamp in its name. This allows you to revert back to earlier versions. You can also set your own tag name and/or description upon publication: cvmfs_server publish [-a tag name] [-m tag description] repo.organization.tld The tag subcommand for cvmfs_server allows you to create ( -a ), remove ( -r ), inspect ( -i ), or list ( -l ) tags of your repository, e.g.: cvmfs_server tag -a \"v1.0\" repo.organization.tld cvmfs_server tag -l repo.organization.tld With the rollback subcommand you can revert back to an earlier version. By default, this will be the previous version, but with -t you can specify a specific tag to revert to: cvmfs_server rollback -t \"v0.5\" repo.organization.tld Catalogs \u00b6 All metadata about files in your repository is stored in a file catalog, which is a SQLite database. When a client accesses the repository for the first time, it first needs to retrieve this catalog. Only then it can start fetching the files it actually needs. Clients also need to regularly check for new versions of the repository, and redownload it if it has changed. As this catalog can quickly become quite large when you start adding more and more files, just having a single one would cause significant overhead. In order to keep them small, you can make use of nested catalogs by having several catalogs for different subtrees of your repository. All metadata for that part of the subtree will not be part of the main catalog anymore, and clients will only download the catalogs for the subtree(s) they are trying to access. The general recommendation is to have more than 1000 and fewer than 200,000 files/directories per (nested) catalog, and to bundle the files/directories that are often accessed together. For instance, it may make sense to make a catalog per installation directory of a specific version of some software in your repository. Making nested catalogs manually can be done in two ways, which we will describe in more detail. .cvmfscatalog files \u00b6 By adding an (empty) file named .cvmfscatalog into a directory of your repository, each following publish operation will automatically generate a nested catalog for the entire subtree below that directory. You can put these files at as many levels as you like, but do keep the aforementioned recommendations in mind. .cvmfsdirtab \u00b6 Instead of creating the .cvmfscatalog files manually, you can also add a file named .cvmfsdirtab to the root of your repository. In this file you can specify a list of relative directory paths (they all start from the root of your repository) that should get a nested catalog. You can also use wildcards to specify patterns and automatically include future contents, and use exclamation marks to exclude paths from a nested catalog. As an example, assume you have a typical HPC software module tree in your repository with the following structure: / \u251c\u2500 /software \u2502 \u251c\u2500 /software/app1 \u2502 \u2502 \u251c\u2500 /software/app1/1.0 \u2502 \u2502 \u251c\u2500 /software/app1/2.0 \u2502 \u251c\u2500 /software/app2 \u2502 \u2502 \u251c\u2500 /software/app2/20201201 \u2502 \u2502 \u251c\u2500 /software/app2/20210125 \u251c\u2500 /modules \u2502 \u251c\u2500 /modules/all \u2502 \u2502 \u251c\u2500 /modules/all/app1 \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app1/1.0.lua \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app1/2.0.lua \u2502 \u2502 \u251c\u2500 /modules/all/app2 \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app2/20201201.lua \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app2/20210125.lua For this repository the .cvmfsdirtab file may look like: # Nested catalog for each version of each application /software/*/* # One nested catalog for all software directories /software # Nested catalog containing for all modulefiles /modules After you have added this file to your repository, you should see automatically generated .cvmfscatalog files in all the specified directories (note that you can still place additional ones manually as well). You can also run cvmfs_server list-catalogs to get a full list of all the nested catalogs. One final note: if you use a .cvmfsdirtab file, a tarball ingestion using the cvmfs_server ingest command will not automatically create the nested catalogs. You will need to do another (empty) transaction right after the ingestion to trigger the creation of the nested catalogs. Homework \u00b6 We prepared a tarball that contains a tree with dummy software installations. You can find the tarball at: TODO: INSERT DETAILS Insert this tarball to a directory named software in your repository using the ingest subcommand (i.e. without actually extracting the tarball); Note that you get some warnings about the catalog containing too many entries; Fix the catalog issue by adding a .cvmfsdirtab file to the root of your repo, which automatically makes a catalog for each software installation directory; Make sure that the warning is gone when you publish this .cvmfsdirtab file. Instead, you may see a message about the catalog being defragmented (because lots of entries were cleaned up).","title":"4. Publishing"},{"location":"04_publishing/#publishing","text":"The previous sections were mostly about setting up the infrastructure. Now that all the components for hosting and accessing your own CernVM-FS repositories are in place, it is time to really start using it. In this section we will give some more details about publishing files.","title":"Publishing"},{"location":"04_publishing/#transactions","text":"As we already showed in a previous section, the easiest way to add files to your repository is by opening and publishing a transaction on your Stratum 0 server. By default, your repository directory under /cvmfs is read-only, but by a transaction makes the directory writable for the user that is owner of the repository. cvmfs_server transaction repo.organization.tld Once you are done with making changes, be sure to change your working directory to somewhere outside of the repository (otherwise you will get an error), and publish your changes using: cvmfs_server publish repo.organization.tld And you can always abort a transaction, which will undo all the non-published modifications: cvmfs_server abort repo.organization.tld","title":"Transactions"},{"location":"04_publishing/#ingesting-tarballs","text":"When you need to compile software that you want to add to your repository, you may want to do the actual compilation on a different machine than your Stratum 0 and copy the resulting installation as a tarball to your Stratum 0. Instead of manually extracting the tarball and doing a transaction, the cvmfs_server command offers a more efficient method for directly publishing the contents of a tarball: cvmfs_server ingest -b /some/path repo.organization.tld -t mytarball.tar The -b expects the relative location in your repository where the contents of the tarball, specified with -t , will be extracted. So, in this case, the tarball gets extracted to /cvmfs/repo.organization.tld/some/path . Note that passing / to -b does not work at the moment, but will be supported in a future release of CernVM-FS. In case you have a compressed tarball, you can use an appropriate decompression tool and write the output to stdout . This output can then be piped to cvmfs_server by setting -t - , e.g. for a .tar.gz file: gunzip -c mytarball.tar.gz | cvmfs_server ingest -b /some/path -t -","title":"Ingesting tarballs"},{"location":"04_publishing/#tags","text":"By default, a newly published version of the repository will automatically get a tag with a timestamp in its name. This allows you to revert back to earlier versions. You can also set your own tag name and/or description upon publication: cvmfs_server publish [-a tag name] [-m tag description] repo.organization.tld The tag subcommand for cvmfs_server allows you to create ( -a ), remove ( -r ), inspect ( -i ), or list ( -l ) tags of your repository, e.g.: cvmfs_server tag -a \"v1.0\" repo.organization.tld cvmfs_server tag -l repo.organization.tld With the rollback subcommand you can revert back to an earlier version. By default, this will be the previous version, but with -t you can specify a specific tag to revert to: cvmfs_server rollback -t \"v0.5\" repo.organization.tld","title":"Tags"},{"location":"04_publishing/#catalogs","text":"All metadata about files in your repository is stored in a file catalog, which is a SQLite database. When a client accesses the repository for the first time, it first needs to retrieve this catalog. Only then it can start fetching the files it actually needs. Clients also need to regularly check for new versions of the repository, and redownload it if it has changed. As this catalog can quickly become quite large when you start adding more and more files, just having a single one would cause significant overhead. In order to keep them small, you can make use of nested catalogs by having several catalogs for different subtrees of your repository. All metadata for that part of the subtree will not be part of the main catalog anymore, and clients will only download the catalogs for the subtree(s) they are trying to access. The general recommendation is to have more than 1000 and fewer than 200,000 files/directories per (nested) catalog, and to bundle the files/directories that are often accessed together. For instance, it may make sense to make a catalog per installation directory of a specific version of some software in your repository. Making nested catalogs manually can be done in two ways, which we will describe in more detail.","title":"Catalogs"},{"location":"04_publishing/#cvmfscatalog-files","text":"By adding an (empty) file named .cvmfscatalog into a directory of your repository, each following publish operation will automatically generate a nested catalog for the entire subtree below that directory. You can put these files at as many levels as you like, but do keep the aforementioned recommendations in mind.","title":".cvmfscatalog files"},{"location":"04_publishing/#cvmfsdirtab","text":"Instead of creating the .cvmfscatalog files manually, you can also add a file named .cvmfsdirtab to the root of your repository. In this file you can specify a list of relative directory paths (they all start from the root of your repository) that should get a nested catalog. You can also use wildcards to specify patterns and automatically include future contents, and use exclamation marks to exclude paths from a nested catalog. As an example, assume you have a typical HPC software module tree in your repository with the following structure: / \u251c\u2500 /software \u2502 \u251c\u2500 /software/app1 \u2502 \u2502 \u251c\u2500 /software/app1/1.0 \u2502 \u2502 \u251c\u2500 /software/app1/2.0 \u2502 \u251c\u2500 /software/app2 \u2502 \u2502 \u251c\u2500 /software/app2/20201201 \u2502 \u2502 \u251c\u2500 /software/app2/20210125 \u251c\u2500 /modules \u2502 \u251c\u2500 /modules/all \u2502 \u2502 \u251c\u2500 /modules/all/app1 \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app1/1.0.lua \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app1/2.0.lua \u2502 \u2502 \u251c\u2500 /modules/all/app2 \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app2/20201201.lua \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app2/20210125.lua For this repository the .cvmfsdirtab file may look like: # Nested catalog for each version of each application /software/*/* # One nested catalog for all software directories /software # Nested catalog containing for all modulefiles /modules After you have added this file to your repository, you should see automatically generated .cvmfscatalog files in all the specified directories (note that you can still place additional ones manually as well). You can also run cvmfs_server list-catalogs to get a full list of all the nested catalogs. One final note: if you use a .cvmfsdirtab file, a tarball ingestion using the cvmfs_server ingest command will not automatically create the nested catalogs. You will need to do another (empty) transaction right after the ingestion to trigger the creation of the nested catalogs.","title":".cvmfsdirtab"},{"location":"04_publishing/#homework","text":"We prepared a tarball that contains a tree with dummy software installations. You can find the tarball at: TODO: INSERT DETAILS Insert this tarball to a directory named software in your repository using the ingest subcommand (i.e. without actually extracting the tarball); Note that you get some warnings about the catalog containing too many entries; Fix the catalog issue by adding a .cvmfsdirtab file to the root of your repo, which automatically makes a catalog for each software installation directory; Make sure that the warning is gone when you publish this .cvmfsdirtab file. Instead, you may see a message about the catalog being defragmented (because lots of entries were cleaned up).","title":"Homework"},{"location":"05_advanced/","text":"Advanced topics \u00b6 gateway-publisher (with warnings) installing these takes time prepared: 1 gateway + 2 publishers demo only (no hands-on) troubleshooting exploding containers + use via Singularity demo Automating the deployment of CernVM-FS servers and clients \u00b6 As you may have experienced during this workshop, it takes quite a bit of manual effort to deploy all the different CernVM-FS components, and you can easily make mistakes here. Therefore, we strongly recommend to automate this in a production setup with a tool like Ansible or Puppet . For Ansible, you could take a look at the playbooks of the EESSI project , which use the Ansible role from the Galaxy Project to install and configure both servers and clients. Compute Canada also offers an Ansible role to configure CernVM-FS clients. For Puppet, Cern offers its own module that allows you to install and configure CernVM-FS servers and clients. Debugging issues \u00b6 If you are experiencing issues with your CernVM-FS setup, there are various ways to start debugging. Most issues are caused by wrongly configured clients (either a configuration issue, or a wrong public key) and connection or firewall issues. In order to find the cause of the issue, we should first find out where the issue is being caused. You can start by checking the client configuration for syntax errors: sudo cvmfs_config chksetup This should return OK . To make sure that your configuration is really picked up and set correctly (because of the hierarchical structure of the configuration, it is easily possible that some parameter gets overwritten by another configuration file), you can dump the effective configuration for your repository using: cvmfs_config showconfig repo.organization.tld Make sure that at least CVMFS_HTTP_PROXY and CVMFS_SERVER_URL are set correctly, and that the directory pointed to by CVMFS_KEYS_DIR really contains the (correct) public key file. The probe subcommand can be used for (re)trying to mount the repository, and should normally return OK : cvmfs_config probe repo.organization.tld Probing /cvmfs/repo.organization.tld... OK But, since you were debugging an issue, it probably returns an error. So, let's enable some debugging output by adding the following line to your /etc/cvmfs/default.local : CVMFS_DEBUGLOG=/path/to/cvmfs.log Now we unmount the repository, re-run the setup step, and try to probe it again: sudo cvmfs_config umount sudo cvmfs_config setup cvmfs_config probe repo.organization.tld You can now check your debug log file, and look for any error messages near the bottom of the file; they may reveal more details about the issue. If it turns out to be some kind of connection issue, you can trace it down further by manually checking the connections to your proxy and/or Stratum 1 server. First, let's rule out that it is some kind of firewall issue by verifying that you can actually connect to the appropriate ports on those servers: telnet url-to-your-proxy 3128 telnet url-to-your-stratum1 80 If this does work, probably something is wrong with the services running on these machines. Every CernVM-FS repository has a file named .cvmfspublished , and you can try to fetch it manually using curl , both directly from the Stratum 1 and via your proxy: # Without your own proxy, i.e. directly go to the Stratum 1: curl --head http://url-to-your-stratum1/cvmfs/repo.organization.tld/.cvmfspublished # With your caching proxy between the client and Stratum 1: curl --proxy http://url-to-your-proxy:3128 --head http://url-to-your-stratum1/cvmfs/repo.organization.tld/.cvmfspublished These commands should return HTTP/1.1 200 OK . If the first command returns something else, you should inspect your CernVM-FS, Apache, and Squid configuration (and log) files on the Stratum 1 server. If the first curl command does work, but the second does not, there is something wrong with your Squid proxy; make sure that it is running, configured, and able to access your Stratum 1 server. Garbage collection \u00b6 As mentioned in the section about publishing , the default configuration of a Stratum 0 enables automatic tagging, which automatically assigns a timestamped tag to each published transaction. However, by default, these automatically generated tags will not be removed automatically. This means, for instance, that files that you remove in later transactions will still take up space in your repository. Setting the lifetime of automatically generated tags \u00b6 Instead of removing tags manually, you can automatically mark these automatically generated tags for removal after a certain period by setting the following variable in the file /etc/cvmfs/repositories.d/repo.organization.tld/server.conf on your Stratum 0: CVMFS_AUTO_TAG_TIMESPAN=\"30 days ago\" This should be a string that can be parsed by the date command, and defines the lifetime of the tags. Cleaning up tags marked for removal \u00b6 In order to actually clean up unreferenced data, garbage collection has to be enabled for this repository by setting CVMFS_GARBAGE_COLLECTION=true in the aforementioned server configuration file. The garbage collector of the CernVM-FS server can then be run using: sudo cvmfs_server gc repo.organization.tld The gc subcommand has several options; a useful way to run it is, especially if you want to do this with a cron job: sudo cvmfs_server gc -a -l -f The -a option will automatically run the garbage collection for all your repositories that have garbage collection enabled and log to /var/log/cvmfs/gc.log , the -l will print which objects are actually removed, while -f will not prompt for confirmation. Note that you cannot run the garbage collection while a publish operation is ongoing. Gateway and Publishers \u00b6 Only being able to modify your repository on the Stratum 0 server can be a bit of a limitation, especially when multiple people have to maintain the repository. A quite new feature in CernVM-FS allows you to set up so-called publisher machines, which are separate machines that are allowed to modify the repository. It also allows for setting up simple ACLs to give certain machines only access to subtrees of the repository. In order to use this feature you also need a gateway machine that has the repository storage mounted; the easiest way to set it up is by having a single machine that serves as both the Stratum 0 and the gateway. This is the setup that we will explain here. Do note that this is a fairly new feature and is not used a lot by production sites yet. Therefore, use it at your own risk! Gateway \u00b6 Requirements \u00b6 This machine has the same requirements as a standard Stratum 0 server, except that it also needs an additional port for the gateway service. This port is configurable, but by default port 4929 is used. Installation \u00b6 Perform the installation steps for the Stratum 0, which can be found in an earlier section. Additionally, install the cvmfs-gateway package: sudo yum install -y cvmfs-gateway Now make your repository using: sudo cvmfs_server mkfs -o $USER repo.organization.tld Configuration \u00b6 The gateway requires you to set up a configuration file /etc/cvmfs/gateway/repo.json . This is a JSON file containing the name of the repository, the keys that can be used by publishers to get access to the repository, and the (sub)path that these publishers are allowed to publish to. The cvmfs-gateway package will make an example file for you, which you can edit or overwrite. It should look like this: { \"version\": 2, \"repos\" : [ { \"domain\" : \"repo.organization.tld\", \"keys\" : [ { \"id\": \"keyid1\", \"path\": \"/\" }, { \"id\": \"keyid2\", \"path\": \"/restricted/to/subdir\" } ] } ], \"keys\" : [ { \"type\" : \"plain_text\", \"id\" : \"keyid1\", \"secret\" : \"SOME_SECRET\" }, { \"type\" : \"plain_text\", \"id\" : \"keyid2\", \"secret\" : \"SOME_OTHER_SECRET\" }, ] } You can choose the key IDs and secrets yourself; the secret has to be given to the owner of the corresponding publisher machine. Finally, there is a second configuration file /etc/cvmfs/gateway/user.json . This is where you can, for instance, change the port of the gateway service and the maximum length of an acquired lease. Assuming you do not have to change the port, you can leave it as it is for now. Starting the service \u00b6 We can now start the gateway service using: systemctl start cvmfs-gateway Do note that, once this service is running, you should not open transactions on this machine anymore, or you may corrupt the repository. If you do want to open a transaction, stop the gateway service first. Publisher \u00b6 Requirements \u00b6 This machine has no special requirements. Installation \u00b6 The publisher only needs the cvmfs-server package to be installed: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs-server Configuration \u00b6 The publisher machine only needs three files with keys: the repository's public key: repo.organization.tld.pub ; the repository's public key encoded as X509 certificate: repo.organization.tld.crt ; the gateway API key stored in a file named repo.organization.tld.gw . The first two files can be taken from /etc/cvmfs/keys on your Stratum 0 server. The latter can be created manually and should just contain the secret that you used in the gateway configuration. Place all these files in some (temporary) directory on your publisher machine. Make the repository \u00b6 We can now make the repository available for writing on our publisher machine by running: sudo cvmfs_server mkfs -w http://YOUR_STRATUM0_GATEWAY/cvmfs/repo.organization.tld \\ -u gw,/srv/cvmfs/repo.organization.tld/data/txn,http://YOUR_STRATUM0_GATEWAY:4929/api/v1 \\ -k /path/to/keys/dir -o `whoami` repo.organization.tld Replace both occurrences of YOUR_STRATUM0_GATEWAY by the IP address or hostname of your gateway / Stratum 0 server (and change 4929 in case you changed the gateway port), and /path/to/keys/dir by the path where you stored the keys in the previous step. Start publishing! \u00b6 You should now be able to make changes to the repository by opening a transaction: cvmfs_server transaction repo.organization.tld # MAKE CHANGES TO /cvmfs/repo.organization.tld cvmfs_server publish repo.organization.tld","title":"5. Advanced topics"},{"location":"05_advanced/#advanced-topics","text":"gateway-publisher (with warnings) installing these takes time prepared: 1 gateway + 2 publishers demo only (no hands-on) troubleshooting exploding containers + use via Singularity demo","title":"Advanced topics"},{"location":"05_advanced/#automating-the-deployment-of-cernvm-fs-servers-and-clients","text":"As you may have experienced during this workshop, it takes quite a bit of manual effort to deploy all the different CernVM-FS components, and you can easily make mistakes here. Therefore, we strongly recommend to automate this in a production setup with a tool like Ansible or Puppet . For Ansible, you could take a look at the playbooks of the EESSI project , which use the Ansible role from the Galaxy Project to install and configure both servers and clients. Compute Canada also offers an Ansible role to configure CernVM-FS clients. For Puppet, Cern offers its own module that allows you to install and configure CernVM-FS servers and clients.","title":"Automating the deployment of CernVM-FS servers and clients"},{"location":"05_advanced/#debugging-issues","text":"If you are experiencing issues with your CernVM-FS setup, there are various ways to start debugging. Most issues are caused by wrongly configured clients (either a configuration issue, or a wrong public key) and connection or firewall issues. In order to find the cause of the issue, we should first find out where the issue is being caused. You can start by checking the client configuration for syntax errors: sudo cvmfs_config chksetup This should return OK . To make sure that your configuration is really picked up and set correctly (because of the hierarchical structure of the configuration, it is easily possible that some parameter gets overwritten by another configuration file), you can dump the effective configuration for your repository using: cvmfs_config showconfig repo.organization.tld Make sure that at least CVMFS_HTTP_PROXY and CVMFS_SERVER_URL are set correctly, and that the directory pointed to by CVMFS_KEYS_DIR really contains the (correct) public key file. The probe subcommand can be used for (re)trying to mount the repository, and should normally return OK : cvmfs_config probe repo.organization.tld Probing /cvmfs/repo.organization.tld... OK But, since you were debugging an issue, it probably returns an error. So, let's enable some debugging output by adding the following line to your /etc/cvmfs/default.local : CVMFS_DEBUGLOG=/path/to/cvmfs.log Now we unmount the repository, re-run the setup step, and try to probe it again: sudo cvmfs_config umount sudo cvmfs_config setup cvmfs_config probe repo.organization.tld You can now check your debug log file, and look for any error messages near the bottom of the file; they may reveal more details about the issue. If it turns out to be some kind of connection issue, you can trace it down further by manually checking the connections to your proxy and/or Stratum 1 server. First, let's rule out that it is some kind of firewall issue by verifying that you can actually connect to the appropriate ports on those servers: telnet url-to-your-proxy 3128 telnet url-to-your-stratum1 80 If this does work, probably something is wrong with the services running on these machines. Every CernVM-FS repository has a file named .cvmfspublished , and you can try to fetch it manually using curl , both directly from the Stratum 1 and via your proxy: # Without your own proxy, i.e. directly go to the Stratum 1: curl --head http://url-to-your-stratum1/cvmfs/repo.organization.tld/.cvmfspublished # With your caching proxy between the client and Stratum 1: curl --proxy http://url-to-your-proxy:3128 --head http://url-to-your-stratum1/cvmfs/repo.organization.tld/.cvmfspublished These commands should return HTTP/1.1 200 OK . If the first command returns something else, you should inspect your CernVM-FS, Apache, and Squid configuration (and log) files on the Stratum 1 server. If the first curl command does work, but the second does not, there is something wrong with your Squid proxy; make sure that it is running, configured, and able to access your Stratum 1 server.","title":"Debugging issues"},{"location":"05_advanced/#garbage-collection","text":"As mentioned in the section about publishing , the default configuration of a Stratum 0 enables automatic tagging, which automatically assigns a timestamped tag to each published transaction. However, by default, these automatically generated tags will not be removed automatically. This means, for instance, that files that you remove in later transactions will still take up space in your repository.","title":"Garbage collection"},{"location":"05_advanced/#setting-the-lifetime-of-automatically-generated-tags","text":"Instead of removing tags manually, you can automatically mark these automatically generated tags for removal after a certain period by setting the following variable in the file /etc/cvmfs/repositories.d/repo.organization.tld/server.conf on your Stratum 0: CVMFS_AUTO_TAG_TIMESPAN=\"30 days ago\" This should be a string that can be parsed by the date command, and defines the lifetime of the tags.","title":"Setting the lifetime of automatically generated tags"},{"location":"05_advanced/#cleaning-up-tags-marked-for-removal","text":"In order to actually clean up unreferenced data, garbage collection has to be enabled for this repository by setting CVMFS_GARBAGE_COLLECTION=true in the aforementioned server configuration file. The garbage collector of the CernVM-FS server can then be run using: sudo cvmfs_server gc repo.organization.tld The gc subcommand has several options; a useful way to run it is, especially if you want to do this with a cron job: sudo cvmfs_server gc -a -l -f The -a option will automatically run the garbage collection for all your repositories that have garbage collection enabled and log to /var/log/cvmfs/gc.log , the -l will print which objects are actually removed, while -f will not prompt for confirmation. Note that you cannot run the garbage collection while a publish operation is ongoing.","title":"Cleaning up tags marked for removal"},{"location":"05_advanced/#gateway-and-publishers","text":"Only being able to modify your repository on the Stratum 0 server can be a bit of a limitation, especially when multiple people have to maintain the repository. A quite new feature in CernVM-FS allows you to set up so-called publisher machines, which are separate machines that are allowed to modify the repository. It also allows for setting up simple ACLs to give certain machines only access to subtrees of the repository. In order to use this feature you also need a gateway machine that has the repository storage mounted; the easiest way to set it up is by having a single machine that serves as both the Stratum 0 and the gateway. This is the setup that we will explain here. Do note that this is a fairly new feature and is not used a lot by production sites yet. Therefore, use it at your own risk!","title":"Gateway and Publishers"},{"location":"05_advanced/#gateway","text":"","title":"Gateway"},{"location":"05_advanced/#requirements","text":"This machine has the same requirements as a standard Stratum 0 server, except that it also needs an additional port for the gateway service. This port is configurable, but by default port 4929 is used.","title":"Requirements"},{"location":"05_advanced/#installation","text":"Perform the installation steps for the Stratum 0, which can be found in an earlier section. Additionally, install the cvmfs-gateway package: sudo yum install -y cvmfs-gateway Now make your repository using: sudo cvmfs_server mkfs -o $USER repo.organization.tld","title":"Installation"},{"location":"05_advanced/#configuration","text":"The gateway requires you to set up a configuration file /etc/cvmfs/gateway/repo.json . This is a JSON file containing the name of the repository, the keys that can be used by publishers to get access to the repository, and the (sub)path that these publishers are allowed to publish to. The cvmfs-gateway package will make an example file for you, which you can edit or overwrite. It should look like this: { \"version\": 2, \"repos\" : [ { \"domain\" : \"repo.organization.tld\", \"keys\" : [ { \"id\": \"keyid1\", \"path\": \"/\" }, { \"id\": \"keyid2\", \"path\": \"/restricted/to/subdir\" } ] } ], \"keys\" : [ { \"type\" : \"plain_text\", \"id\" : \"keyid1\", \"secret\" : \"SOME_SECRET\" }, { \"type\" : \"plain_text\", \"id\" : \"keyid2\", \"secret\" : \"SOME_OTHER_SECRET\" }, ] } You can choose the key IDs and secrets yourself; the secret has to be given to the owner of the corresponding publisher machine. Finally, there is a second configuration file /etc/cvmfs/gateway/user.json . This is where you can, for instance, change the port of the gateway service and the maximum length of an acquired lease. Assuming you do not have to change the port, you can leave it as it is for now.","title":"Configuration"},{"location":"05_advanced/#starting-the-service","text":"We can now start the gateway service using: systemctl start cvmfs-gateway Do note that, once this service is running, you should not open transactions on this machine anymore, or you may corrupt the repository. If you do want to open a transaction, stop the gateway service first.","title":"Starting the service"},{"location":"05_advanced/#publisher","text":"","title":"Publisher"},{"location":"05_advanced/#requirements_1","text":"This machine has no special requirements.","title":"Requirements"},{"location":"05_advanced/#installation_1","text":"The publisher only needs the cvmfs-server package to be installed: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs-server","title":"Installation"},{"location":"05_advanced/#configuration_1","text":"The publisher machine only needs three files with keys: the repository's public key: repo.organization.tld.pub ; the repository's public key encoded as X509 certificate: repo.organization.tld.crt ; the gateway API key stored in a file named repo.organization.tld.gw . The first two files can be taken from /etc/cvmfs/keys on your Stratum 0 server. The latter can be created manually and should just contain the secret that you used in the gateway configuration. Place all these files in some (temporary) directory on your publisher machine.","title":"Configuration"},{"location":"05_advanced/#make-the-repository","text":"We can now make the repository available for writing on our publisher machine by running: sudo cvmfs_server mkfs -w http://YOUR_STRATUM0_GATEWAY/cvmfs/repo.organization.tld \\ -u gw,/srv/cvmfs/repo.organization.tld/data/txn,http://YOUR_STRATUM0_GATEWAY:4929/api/v1 \\ -k /path/to/keys/dir -o `whoami` repo.organization.tld Replace both occurrences of YOUR_STRATUM0_GATEWAY by the IP address or hostname of your gateway / Stratum 0 server (and change 4929 in case you changed the gateway port), and /path/to/keys/dir by the path where you stored the keys in the previous step.","title":"Make the repository"},{"location":"05_advanced/#start-publishing","text":"You should now be able to make changes to the repository by opening a transaction: cvmfs_server transaction repo.organization.tld # MAKE CHANGES TO /cvmfs/repo.organization.tld cvmfs_server publish repo.organization.tld","title":"Start publishing!"}]}